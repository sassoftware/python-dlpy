{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and Train Keras LeNet Model, Convert to SAS Deep Learning LeNet Model with DLPy\n",
    "\n",
    "This example trains a LeNet model in Keras, and then imports the resulting trained model into a SAS Deep Learning model using SAS DLPy.  The test image data is scored by both Keras and DLPy models, and the model results are compared. The results show that the imported and converted SAS Deep Learning model duplicates the results that were produced by the original Keras model. This demonstrates the consistency between the open-source Keras model and the imported and converted SAS Deep Learning model.  \n",
    "\n",
    "For best success before attempting this example in your own environment, consider reading Adrian Rosebrook's tutorial blog post on LeNet CNN models: https://www.pyimagesearch.com/2016/08/01/lenet-convolutional-neural-network-in-python. The blog provides a nice summary of LeNet, and this example follows the blog's LeNet model definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Note: Client and Server Definitions\n",
    "\n",
    "SAS Viya and Deep Learning literature and technical documentation often refers to client and server entities.  In this scenario, the client is the computer that runs the Jupyter notebook with the example code.  The server is the computer that is running the SAS Viya server.  These two computers might (or might not) use the same operating system, and might (or might not) have access to a common file system.  \n",
    "\n",
    "This example assumes that the client and server do not use the same operating system, but that they do have access to a common file system.  If the client and server in your environment do not have access to a common file system, you will need to copy or transfer files between client and server twice during this example.  The example in this notebook uses comments in cells to indicate when a given file should be moved from the client to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define variables to contain path specifications to  \n",
    "# the client and server machine image root directories\n",
    "\n",
    "client_image_root = \"/client/path-to/image-root/directory\" # this is the CLIENT_IMAGE_ROOT directory\n",
    "\n",
    "server_image_root = \"/server/path-to/image-root/directory\" # this is the SERVER_IMAGE_ROOT directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python Function Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we import and intitialize Python function packages that will be used during the example. The Keras function packages are used to build the LeNet model in Keras before the trained model is imported as a SAS Viya Deep Learning model using SAS DLPy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from keras.optimizers import RMSprop\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the Keras MNIST Image Classification Data\n",
    "\n",
    "This step of the example extracts the MNIST data using Keras utilities. For more information on the MNIST data, see https://keras.io/datasets/#mnist-database-of-handwritten-digits. Convenience functions provided by Keras load the data and assign labels to the training and test data.  There should be 60000 training images and 10000 test images.  Each image should be 28 pixels by 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images = 60000\n",
      "Number of test images = 10000\n",
      "Image size = 28 x 28\n"
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# convert class vectors to binary class matrices - this effectively gives a classification\n",
    "# label for each image\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(\"Number of training images = \" + str(x_train.shape[0]))\n",
    "print(\"Number of test images = \" + str(x_test.shape[0]))\n",
    "print(\"Image size = \" + str(x_train.shape[1]) + \" x \" + str(x_train.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Images Before Loading into SAS CAS \n",
    "\n",
    "\n",
    "The test images for the example are stored in Python numpy arrays. There is not a direct way to convert a numpy array to a SAS CAS table, so the example first stores the test images in an intermediate PNG format. Afterwards, the example performs image processing actions and then loads the stored PNG images into a SAS CAS table.  Note that the action that loads images expects the following directory structure:\n",
    "\n",
    "<br>\n",
    "CLIENT_IMAGE_ROOT/label1\n",
    "<br>\n",
    "CLIENT_IMAGE_ROOT/label2\n",
    "<br>\n",
    "...\n",
    "<br>\n",
    "CLIENT_IMAGE_ROOT/labelN\n",
    "<br>\n",
    "\n",
    "In this instance, CLIENT_IMAGE_ROOT specifies the name of the root image directory on the client, and \"label1\", \"label2\", ..., \"labelN\" \n",
    "represents the N image classes.  All the images that belong to image class \"label1\" should reside in the CLIENT_IMAGE_ROOT/label1 directory, all the images that belong to image class \"label2\" should reside in the CLIENT_IMAGE_ROOT/label2 directory, and so on.\n",
    "\n",
    "The MNIST image data contains 10 object classes. Each class is assigned a label \"0\", \"1\", ..., \"9\".  The example code assigns the test images to 10 separate directories as follows:\n",
    "\n",
    "<br>\n",
    "CLIENT_IMAGE_ROOT/0\n",
    "<br>\n",
    "CLIENT_IMAGE_ROOT/1\n",
    "<br>\n",
    "...\n",
    "<br>\n",
    "CLIENT_IMAGE_ROOT/9\n",
    "<br>\n",
    "\n",
    "Note: This example assumes that if the CLIENT_IMAGE_ROOT directory already exists, then all image files have been previously unpacked and are ready for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image directory exists, assuming image files prepared for loading into CAS table\n"
     ]
    }
   ],
   "source": [
    "# check if working directory exists\n",
    "if os.path.isdir(client_image_root):\n",
    "    print(\"Image directory exists, assuming image files are prepared for loading into CAS table.\")\n",
    "else:\n",
    "    os.mkdir(client_image_root)\n",
    "    \n",
    "    # create subdirectories for each image class\n",
    "    for ii in range(10):\n",
    "        new_path = os.path.join(client_image_root,str(ii))\n",
    "        if not os.path.isdir(new_path):\n",
    "            os.mkdir(new_path)\n",
    "\n",
    "    # distribute images to proper directories\n",
    "    im_index = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    for ii in range(x_test.shape[0]):\n",
    "        im = Image.fromarray(x_test[ii,])\n",
    "        idx = np.where(y_test[ii])[0][0]\n",
    "        file_name = 'im'+str(idx)+'_file'+str(im_index[idx])+'.png'\n",
    "        new_path = os.path.join(client_image_root,str(idx))\n",
    "        im.save(os.path.join(new_path,file_name))\n",
    "        im_index[idx] = im_index[idx] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your client/server do not share a common file system, please copy the image files and subdirectories from CLIENT_IMAGE_ROOT to \n",
    "# SERVER_IMAGE_ROOT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "The following code converts the image data to floating point and scales it such that all pixel values fall within the range 0.0 to 1.0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the 60,000 train images and the 10,000 test images.\n",
    "x_train = x_train.reshape(60000, 28, 28, 1)\n",
    "x_test = x_test.reshape(10000, 28, 28, 1)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Keras LeNet Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 20)        520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 50)        25050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               1225500   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 1,256,080\n",
      "Trainable params: 1,256,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the Architecture for a Keras LeNet \n",
    "# Image Classification Model\n",
    "\n",
    "# training parameters\n",
    "batch_size = 128\n",
    "epochs = 1\n",
    "\n",
    "# model architecture and activation functions\n",
    "model = Sequential()\n",
    "model.add(Conv2D(20, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(28,28,1), padding=\"same\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "model.add(Conv2D(50, kernel_size=(5, 5), strides=(1, 1), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# print a model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide Trained LeNet Model Parameters\n",
    "\n",
    "After you define the Keras LeNet model architecture, you must either train the model from scratch, or use an exisiting trained model parameter file in HDF5 (.h5) format. You can make changes in the example code below according to your choice.\n",
    "\n",
    "If you have <b>not</b> run this example code before, you should use the code block that sets the `do_train` parameter to True. This setting trains the model from scratch, and then saves the weighted results in a generated .h5 file named `lenet.h5`. \n",
    "\n",
    "If you are running subsequent instances of this example (perhaps experimenting with parameter changes) it is not necessary to train the model from scratch each time. Instead, you can use the generated `lenet.h5` file that was created after training the model from scratch. To do this, you should comment out the `do_train = True` code block, and instead use the `do_train = False` code block below. The location of the trained `lenet.h5` parameter file should be specified in `CLIENT_IMAGE_ROOT/lenet.h5`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 35s 584us/step - loss: 0.1499 - accuracy: 0.9542 - val_loss: 0.0371 - val_accuracy: 0.9875\n",
      "Test loss: 0.037071275619626975\n",
      "Test accuracy: 0.987500011920929\n"
     ]
    }
   ],
   "source": [
    "# Compile the model using categorical cross entropy as the loss \n",
    "# function and RMS propagation as the iterative training optimizer.\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy']\n",
    "              )\n",
    "\n",
    "# If training for the first time, set the `do_train` parameter to True.\n",
    "# This will train the model from scratch, and save the pre-trained weighted \n",
    "# model parameters in a file named \"Lenet.h5\"\n",
    "\n",
    "# If you have already trained the model from scratch, you can use the \n",
    "# saved \"Lenet.h5\" file to load the model using pre-trained weights, \n",
    "# instead of spending processor time to train from scratch.\n",
    "\n",
    "# For first time training, use the following code:\n",
    "\n",
    "do_train = True\n",
    "if do_train:\n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test)\n",
    "                       )\n",
    "    \n",
    "    model.save(os.path.join(client_image_root,\"lenet.h5\"))\n",
    "\n",
    "\n",
    "# If you have already trained the model from scratch, commment out the \n",
    "# previous code block, and un-comment the following code block. The    \n",
    "# following code sets the `do_train` parameter to False, and uses the  \n",
    "# generated model weights (Lenet.h5) file for the model:               \n",
    "\n",
    "\n",
    "# do_train = False\n",
    "# if do_train:\n",
    "#    history = model.fit(x_train, y_train,\n",
    "#    batch_size=batch_size,\n",
    "#    epochs=epochs,\n",
    "#    verbose=1,\n",
    "#    validation_data=(x_test, y_test))\n",
    "    \n",
    "#    model.save(os.path.join(client_image_root,\"lenet.h5\"))\n",
    "\n",
    "# else:\n",
    "\n",
    "# Load the previously trained parameter file \"lenet.h5\" instead\n",
    "# of training the LeNet model from scratch.\n",
    "\n",
    "#    model.load_weights(os.path.join(client_image_root,\"lenet.h5\"))\n",
    "\n",
    "# At this point you have a trained model network.\n",
    "# Now use the trained model (whether trained by scratch or using \n",
    "# the pre-trained model parameter file) to score the test Data.\n",
    "    \n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to the SAS CAS Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Added action set 'deepLearn'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; actionset</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>deepLearn</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00283s</span> &#183; <span class=\"cas-user\">user 0.00129s</span> &#183; <span class=\"cas-sys\">sys 0.00151s</span> &#183; <span class=\"cas-memory\">mem 0.205MB</span></small></p>"
      ],
      "text/plain": [
       "[actionset]\n",
       "\n",
       " 'deepLearn'\n",
       "\n",
       "+ Elapsed: 0.00283s, user: 0.00129s, sys: 0.00151s, mem: 0.205mb"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure matplotlib utility to display output in \n",
    "# Jupyter notebook cell\n",
    "%matplotlib inline\n",
    "\n",
    "# SAS Scripting Wrapper for Analytic Transfer (SWAT)\n",
    "# is a module needed to connect to a CAS server.\n",
    "import swat\n",
    "from swat import * \n",
    "\n",
    "host_name='your-qualified.server-name.com'\n",
    "\n",
    "\n",
    "# port_number='12345' # use a unique 5-digit port ID\n",
    "port_number=12345\n",
    "\n",
    "s = swat.CAS('your-qualified.server-name.com', 12345)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Convert the Keras Model to a SAS Deep Learning Model\n",
    "\n",
    "Two operations take place when the SAS DLPy `from_keras_model()` function is called. The first operation automatically translates the Keras model into a SAS Deep Learning model.  You can perform this process manually, but it is not recommended because it is error-prone and time-consuming.  \n",
    "\n",
    "The second operation rearranges and reformats the HDF5 parameter file that was generated by Keras.  Rearranging and reformatting the parameter file enables the SAS Deep Learning action set to correctly parse and read the trained parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: could not find layer conv2d_1_input, in model. Translated model may be inaccurate.\n",
      "NOTE: Model table is attached successfully!\n",
      "NOTE: Model is named to \"lenet\" according to the model name in the table.\n",
      "NOTE: the model weights has been stored in the following file:\n",
      "C:\\Users\\chrobi\\python-dlpy-1\\examples\\keras_model_conversion\\lenet_weights.kerasmodel.h5\n"
     ]
    }
   ],
   "source": [
    "from dlpy import Model\n",
    "# Convert the Keras model to a SAS Deep Learning model\n",
    "# Convert Keras model parameters to a format \n",
    "# that can be read by SAS DLPy and the SAS \n",
    "# Deep Learning tools.\n",
    "\n",
    "model_name = 'lenet'\n",
    "model1,use_gpu = Model.from_keras_model(conn=s, \n",
    "                                        keras_model=model, \n",
    "                                        output_model_table=model_name,\n",
    "                                        include_weights=True, \n",
    "                                        scale=1.0/255.0,\n",
    "                                        input_weights_file=os.path.join(client_image_root,\"lenet.h5\")\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: You should not be concerned about the warning in the example output. In this case, it is expected, because the Keras model input layer did not have a name. The warning simply informs users that a name has been created for the Viya model input layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the Reformatted HDF5 File\n",
    "\n",
    "This is for convenience, in the event that your client and server share a common file system.  If there is no common file system, then you must perform a copy operation to transfer the HDF5 file from one file system to another.  Remember that deep learning HDF5 files can be quite large, so the data transfer could take a significant amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\\\\\sashq\\\\root\\\\u\\\\chrobi\\\\Keras_LeNet\\\\lenet_weights.kerasmodel.h5'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from shutil import copyfile\n",
    "copyfile('lenet_weights.kerasmodel.h5', os.path.join(client_image_root,\"lenet_weights.kerasmodel.h5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate the SAS Deep Learning Model with Trained Parameters\n",
    "\n",
    "Up to this point, the SAS Deep Learning model contains parameters that were initialized using the default settings that were supplied when the model was created in the preceding step. Now we will import the trained parameters from the Keras model and install them into the SAS Viya model. The weighted parameter file is called `lenet_weights.kerasmodel.h5` and it should be downloaded and stored in a subdirectory to the server path specfication you stored earlier in the variable `server_image_root`.\n",
    "\n",
    "It is also a good idea at this time to apply labels to the output of the classification layer. The layers should be consistent with the labels that we will shortly apply to the test images. The label file is called `mnist_labels.csv` and it can be downloaded from the [SAS DLPy example folder for Keras examples](https://github.com/sassoftware/python-dlpy/tree/master/examples/keras_model_conversion) on GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table NEW_LABEL_TABLE_AINUGF in caslib CASUSER(chrobi).\n",
      "NOTE: The table NEW_LABEL_TABLE_AINUGF has been created in caslib CASUSER(chrobi) from binary data uploaded to Cloud Analytic Services.\n",
      "NOTE: no dataspec(s) provided - creating image classification model.\n",
      "NOTE: Model weights attached successfully!\n"
     ]
    }
   ],
   "source": [
    "model1.load_weights(path=server_image_root+\"/lenet_weights.kerasmodel.h5\",\n",
    "                    labels=True,\n",
    "                    label_file_name=os.path.join(os.getcwd(),\"mnist_labels.csv\"),\n",
    "                    label_length=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the SAS Deep Learning Model Architecture\n",
    "\n",
    "Please compare the SAS Deep Learning model architecture and parameter count in this summary to the model architecture and parameter count from the Keras model above. The model layers and model layer definitions may differ somewhat between the Keras and SAS Deep Learning models, but the total number of trained parameters in the two models should be identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Id</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Type</th>\n",
       "      <th>Kernel Size</th>\n",
       "      <th>Stride</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Output Size</th>\n",
       "      <th>Number of Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>conv2d_1_input</td>\n",
       "      <td>input</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>(28, 28, 1)</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>conv2d_1</td>\n",
       "      <td>convo</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>(1.0, 1.0)</td>\n",
       "      <td>Rectifier</td>\n",
       "      <td>(28, 28, 20)</td>\n",
       "      <td>(500, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>max_pooling2d_1</td>\n",
       "      <td>pool</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2.0, 2.0)</td>\n",
       "      <td>Max</td>\n",
       "      <td>(14, 14, 20)</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>conv2d_2</td>\n",
       "      <td>convo</td>\n",
       "      <td>(5, 5)</td>\n",
       "      <td>(1.0, 1.0)</td>\n",
       "      <td>Rectifier</td>\n",
       "      <td>(14, 14, 50)</td>\n",
       "      <td>(25000, 50)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>max_pooling2d_2</td>\n",
       "      <td>pool</td>\n",
       "      <td>(2, 2)</td>\n",
       "      <td>(2.0, 2.0)</td>\n",
       "      <td>Max</td>\n",
       "      <td>(7, 7, 50)</td>\n",
       "      <td>(0, 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>dense_1</td>\n",
       "      <td>fc</td>\n",
       "      <td>(2450, 500)</td>\n",
       "      <td></td>\n",
       "      <td>Rectifier</td>\n",
       "      <td>500</td>\n",
       "      <td>(1225000, 500)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>dense_2</td>\n",
       "      <td>output</td>\n",
       "      <td>(500, 10)</td>\n",
       "      <td></td>\n",
       "      <td>Softmax</td>\n",
       "      <td>10</td>\n",
       "      <td>(5000, 10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1256080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Layer Id            Layer    Type  Kernel Size      Stride Activation  \\\n",
       "0        0   conv2d_1_input   input                                None   \n",
       "1        1         conv2d_1   convo       (5, 5)  (1.0, 1.0)  Rectifier   \n",
       "2        2  max_pooling2d_1    pool       (2, 2)  (2.0, 2.0)        Max   \n",
       "3        3         conv2d_2   convo       (5, 5)  (1.0, 1.0)  Rectifier   \n",
       "4        4  max_pooling2d_2    pool       (2, 2)  (2.0, 2.0)        Max   \n",
       "5        5          dense_1      fc  (2450, 500)              Rectifier   \n",
       "6        6          dense_2  output    (500, 10)                Softmax   \n",
       "7                                                                         \n",
       "\n",
       "    Output Size Number of Parameters  \n",
       "0   (28, 28, 1)               (0, 0)  \n",
       "1  (28, 28, 20)            (500, 20)  \n",
       "2  (14, 14, 20)               (0, 0)  \n",
       "3  (14, 14, 50)          (25000, 50)  \n",
       "4    (7, 7, 50)               (0, 0)  \n",
       "5           500       (1225000, 500)  \n",
       "6            10           (5000, 10)  \n",
       "7                            1256080  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model1.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the MNIST Image Classification Test Data\n",
    "\n",
    "You will need to provide a path to your environment's server path in order to load the MNIST test data images. This example uses the SERVER_IMAGE_ROOT server path variable that you specified earlier in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import SAS DLPy simple image processing functions\n",
    "from dlpy.images import ImageTable\n",
    "\n",
    "# load the MNIST image classification model test data images\n",
    "my_images = ImageTable.load_files(s, path=server_image_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Appearance of Test Data Images\n",
    "\n",
    "After loading the MNIST model test data images into the SAS Viya model in DLPy, display a matrix of 8 random images arranged in 4 columns. Verify that the test images resemble various hand-written Arabic numerals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x864 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "my_images.show(nimages=8,ncol=4, randomize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score the Test Images with the SAS Deep Learning Model\n",
    "\n",
    "Now use the SAS DLPy `predict()` function to score the test images using the Deep Learning classification model `model1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Descr         Value\n",
      "0  Number of Observations Read         10000\n",
      "1  Number of Observations Used         10000\n",
      "2  Misclassification Error (%)          1.28\n",
      "3                   Loss Error      0.037126\n"
     ]
    }
   ],
   "source": [
    "# score images\n",
    "viya_score = model1.predict(my_images, buffer_size=batch_size, n_threads=1)\n",
    "print(viya_score['ScoreInfo'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the Results\n",
    "\n",
    "Compare the classification results generated by the Keras model and the imported SAS Viya model.\n",
    "These results should be identical (or nearly so).  Note that the Keras model reports classification rate, while the SAS Deep Learning model reports misclassification rate. To compare classification rates, subtract the Deep Learning misclassification rate from 100. \n",
    "\n",
    "The code below calculates the classification rate in terms of the percent of images correctly classified.  The output shows that the original Keras image classification model and the imported SAS Deep Learning image classification model deliver identical classification performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viya classification rate = 98.72%\n",
      "Keras classification rate = 98.75%\n"
     ]
    }
   ],
   "source": [
    "print('Viya classification rate = ' + \"{:2.2f}\".format(100.0-float(viya_score['ScoreInfo']['Value'][2])) + '%')\n",
    "print('Keras classification rate = ' + \"{:2.2f}\".format(100.0*score[1]) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
