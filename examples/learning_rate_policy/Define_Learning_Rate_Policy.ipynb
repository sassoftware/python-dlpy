{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SAS DLPy to Define SAS Viya Model Learning Rate Policies\n",
    "\n",
    "When creating and training deep learning neural network models, one of the most important hyperparameters to configure for good performance is the learning rate.  This example notebook demonstrates how you can use SAS DLPy to specify different predefined learning rate policies for your SAS Viya deep learning models. This notebook also shows you how to specify your own customized learning rate policy using the SAS Viya FCMP function utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "- [Important Note: Client and Server Definitions](#ClientServer)\n",
    "- [Prepare Resources and Configure Computing Environment for Modeling](#getReady)\n",
    "    - [Download the Image Data](#downloadData)\n",
    "    - [Import Required Python and SAS DLPy Modules](#importPythonDLPy)\n",
    "- [Launch SAS CAS Session](#LaunchCAS)\n",
    "- [Use SAS DLPy to Create a Simple SAS Viya ResNet Network](#CreateResNet)\n",
    "- [Load the Image Data into SAS CAS](#LoadImageData)\n",
    "- [Define the Model Learning Rate Hyperparameter](#DefineLR)\n",
    "    - [Specify Step Learning Rate](#StepLR)\n",
    "    - [Specify Cyclic Learning Rate Scheduler](#CyclicLR)\n",
    "    - [Specify Reduce Learning Rate on Plateau](#ReduceLR_Plat)\n",
    "    - [Specify Customized Learning Rate Policy](#CustomLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ClientServer\"></a>\n",
    "\n",
    "### Important Note: Client and Server Definitions\n",
    "SAS Viya literature and technical documentation often refers to client and server entities. In this scenario, the client is the computer that runs the Jupyter notebook with the example code. The server is the computer that is running the Viya server. These two computers might (or might not) use the same operating system, and might (or might not) have access to a common file system.\n",
    "\n",
    "This notebook assumes that the client and server do not use the same operating system, but that they do have access to a common file system. If the client and server in your environment do not have access to a common file system, you will need to copy or transfer files between client and server during this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code defines path variables that contain path specifications to  \n",
    "# the client and server model files and image root directories.\n",
    "\n",
    "# Server Learning Rate root location (your path will be different)\n",
    "server_learning_rate_root = r'/cas/DeepLearn/UserID/Learning_Rate/' \n",
    "\n",
    "# Server Learning Rate Image root location (your path will be different)\n",
    "server_image_root = r'/cas/DeepLearn/UserID/Learning_Rate/Giraffe_Dolphin' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"getReady\"></a>\n",
    "\n",
    "### Prepare Resources and Configure Computing Environment for Modeling\n",
    "\n",
    "Use this section to organize all of the resources that you will need and configure your local computing environment for this notebook example. Performing these tasks in advance means you can run the example without multiple stops to locate and download necessary resources. This approach enables you to focus on how to use SAS DLPy to complete the modeling task in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"downloadData\"></a>\n",
    "\n",
    "#### Download the Image Data\n",
    "\n",
    "All of the examples use a small toy data set that contains dolphin or giraffe images for the classification tasks. You can download the dolphin and giraffe image data from the SAS DLPy GitHub site [here.](https://github.com/sassoftware/python-dlpy/tree/master/dlpy/tests/datasources/giraffe_dolphin_small). Create a folder named `Giraffe_Dolphin` in the server location that you saved as `server_image_root`, and use this folder to contain the `Dolphin` and `Giraffe` image subfolders. The notebook example expects the following folder structures:\n",
    "\n",
    "server_learning_rate_root\n",
    " * Giraffe_Dolphin  (server_image_root)     \n",
    "       * Dolphin\n",
    "           * Dolphin image 1\n",
    "           * Dolphin image 2\n",
    "           * ...  \n",
    "       * Giraffe\n",
    "           * Giraffe image 1\n",
    "           * Giraffe image 2\n",
    "           * ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"importPythonDLPy\"></a>\n",
    "\n",
    "#### Import Required Python and SAS DLPy Modules\n",
    "\n",
    "After copying the input image data to your server, configure your client computing environment for image captioning modeling using Python and SAS DLPy. \n",
    "\n",
    "Import the various Python and SAS DLPy modules that this notebook uses. Begin by importing the SAS Scripting Wrapper for Analytic Transfer (SWAT). SWAT is the Python interface to SAS CAS. You can find more detailed information about starting a SAS CAS session with the SWAT package [here](https://sassoftware.github.io/python-swat/getting-started.html). \n",
    "\n",
    "Import the SAS DLPy modules and functions that are used to create the CNN and RNN models in this notebook. The DLPy `ImageTable` module makes it easier to load images from a folder into a SAS CAS table. The DLPy `applications` module contains parameters for the pre-built VGG-16 CNN model that the notebook uses to perform image feature extraction.\n",
    "\n",
    "The `image_captioning` module contains the functions that are necessary for building the RNN image captioning model and to create the captioned image table that will be used for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python utilities \n",
    "# and SAS DLPy modules\n",
    "\n",
    "import swat\n",
    "import sys\n",
    "import dlpy\n",
    "from dlpy.layers import *\n",
    "from dlpy.model import *\n",
    "from dlpy.images import ImageTable\n",
    "from dlpy.sequential import Sequential\n",
    "from dlpy.lr_scheduler import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"LaunchCAS\"></a>\n",
    "\n",
    "### Launch SAS CAS Session\n",
    "\n",
    "SAS DLPy requires a running SAS CAS server and the SAS Scripting Wrapper for Analytic Transfer (SWAT). The SWAT package is a Python interface to CAS. You can choose the port number you want to use. By default SAS uses '5570' for a portID. \n",
    "\n",
    "Note: For more information about starting a CAS session with the SWAT package, see https://sassoftware.github.io/python-swat/getting-started.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch SAS CAS session\n",
    "host_name='your-server.unx.your-company.com'\n",
    "port_number='5570'\n",
    "\n",
    "sess = swat.CAS(host_name, port_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"CreateResNet\"></a>\n",
    "\n",
    "### Use SAS DLPy to Create a Simple SAS Viya ResNet Network\n",
    "\n",
    "ResNet models are a family of models that were developed as backbones for convolutional neural networks performing computer vision tasks, such as image classification. The SAS DLPy API includes complete ResNet model architectures, but you can also use DLPy to define a simple ResNet model from scratch at the block level.\n",
    "\n",
    "The following code uses SAS DLPy to create a simple ResNet CNN model named `resnet_like_model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define convolution blocks\n",
    "\n",
    "def conv_block(x, filters, size, stride=1, mode='same', act=True):\n",
    "    x = Conv2d(filters, size, size, act='identity', include_bias=False, stride=stride)(x)\n",
    "    x = BN(act='relu' if act else 'identity')(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define residual blocks\n",
    "\n",
    "def res_block(ip, nf=64):\n",
    "    x = conv_block(ip, nf, 3, 2)\n",
    "    x = conv_block(x, nf, 3, 1, act=False)\n",
    "    return Res()([x, ip])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "# Define input layer, shape, and scale\n",
    "inp_resnet= Input(3, 112, 112, scale = 1.0 / 255, name='InputLayer_1')\n",
    "\n",
    "# 2D Convolution and Pooling layers\n",
    "x=conv_block(inp_resnet, 64, 9, 1)\n",
    "for i in range(4): x=res_block(x)\n",
    "x=Conv2d(20, 9, 9, act='tanh')(x)\n",
    "x=Pooling(7, 7)(x)\n",
    "\n",
    "# Output layer\n",
    "output = OutputLayer(n=2)(x)\n",
    "\n",
    "# Name and compile the model \n",
    "resnet_like_model = Model(sess, inputs = inp_resnet, outputs = output)\n",
    "resnet_like_model.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the DLPy `plot_network()` function to display a DAG of the ResNet model `resnet_like_model` you just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: Model_8ZIA1w Pages: 1 -->\r\n",
       "<svg width=\"339pt\" height=\"1781pt\"\r\n",
       " viewBox=\"0.00 0.00 338.50 1781.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 1777)\">\r\n",
       "<title>Model_8ZIA1w</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-1777 334.5,-1777 334.5,4 -4,4\"/>\r\n",
       "<!-- InputLayer_1 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>InputLayer_1</title>\r\n",
       "<polygon fill=\"#3288bd\" fill-opacity=\"0.250980\" stroke=\"#3288bd\" points=\"89.5,-1750.5 89.5,-1772.5 315.5,-1772.5 315.5,-1750.5 89.5,-1750.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"202.5\" y=\"-1757.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">112x112x3 InputLayer_1(input)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>Conv2d_1</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"118,-1680.5 118,-1702.5 287,-1702.5 287,-1680.5 118,-1680.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"202.5\" y=\"-1687.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">9x9 Conv2d_1(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- InputLayer_1&#45;&gt;Conv2d_1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>InputLayer_1&#45;&gt;Conv2d_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M202.5,-1750.47C202.5,-1740.62 202.5,-1725.33 202.5,-1712.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"206,-1712.57 202.5,-1702.57 199,-1712.57 206,-1712.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"235.5\" y=\"-1724\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 112 x 112 x 3 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_1 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>BN_1</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"90.5,-1610.5 90.5,-1632.5 314.5,-1632.5 314.5,-1610.5 90.5,-1610.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"202.5\" y=\"-1617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">112x112x64 BN_1(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_1&#45;&gt;BN_1 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>Conv2d_1&#45;&gt;BN_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M202.5,-1680.47C202.5,-1670.62 202.5,-1655.33 202.5,-1642.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"206,-1642.57 202.5,-1632.57 199,-1642.57 206,-1642.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"238\" y=\"-1654\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 112 x 112 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_2 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>Conv2d_2</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"49,-1540.5 49,-1562.5 218,-1562.5 218,-1540.5 49,-1540.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"133.5\" y=\"-1547.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_2(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_1&#45;&gt;Conv2d_2 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>BN_1&#45;&gt;Conv2d_2</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M190.717,-1610.46C184.57,-1605.11 176.998,-1598.34 170.5,-1592 163.441,-1585.12 155.952,-1577.24 149.572,-1570.34\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"152.015,-1567.82 142.683,-1562.81 146.849,-1572.55 152.015,-1567.82\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"206\" y=\"-1584\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 112 x 112 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Res_1 -->\r\n",
       "<g id=\"node8\" class=\"node\"><title>Res_1</title>\r\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"105,-1260.5 105,-1282.5 302,-1282.5 302,-1260.5 105,-1260.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"203.5\" y=\"-1267.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">56x56x64 Res_1(residual)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_1&#45;&gt;Res_1 -->\r\n",
       "<g id=\"edge8\" class=\"edge\"><title>BN_1&#45;&gt;Res_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M220.71,-1610.48C228.014,-1605.7 235.99,-1599.4 241.5,-1592 252.626,-1577.06 255.5,-1571.13 255.5,-1552.5 255.5,-1552.5 255.5,-1552.5 255.5,-1340.5 255.5,-1319.88 240.221,-1301.63 226.228,-1289.25\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"228.198,-1286.33 218.262,-1282.66 223.737,-1291.73 228.198,-1286.33\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"291\" y=\"-1444\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 112 x 112 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_2 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>BN_2</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"20,-1470.5 20,-1492.5 227,-1492.5 227,-1470.5 20,-1470.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"123.5\" y=\"-1477.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">56x56x64 BN_2(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_2&#45;&gt;BN_2 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>Conv2d_2&#45;&gt;BN_2</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M132.024,-1540.47C130.562,-1530.52 128.282,-1515.01 126.447,-1502.54\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"129.899,-1501.96 124.982,-1492.57 122.974,-1502.98 129.899,-1501.96\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"160\" y=\"-1514\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_3 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>Conv2d_3</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"39,-1400.5 39,-1422.5 208,-1422.5 208,-1400.5 39,-1400.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"123.5\" y=\"-1407.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_3(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_2&#45;&gt;Conv2d_3 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>BN_2&#45;&gt;Conv2d_3</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M123.5,-1470.47C123.5,-1460.62 123.5,-1445.33 123.5,-1432.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"127,-1432.57 123.5,-1422.57 120,-1432.57 127,-1432.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"154\" y=\"-1444\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_3 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>BN_3</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"20,-1330.5 20,-1352.5 227,-1352.5 227,-1330.5 20,-1330.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"123.5\" y=\"-1337.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">56x56x64 BN_3(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_3&#45;&gt;BN_3 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>Conv2d_3&#45;&gt;BN_3</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M123.5,-1400.47C123.5,-1390.62 123.5,-1375.33 123.5,-1362.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"127,-1362.57 123.5,-1352.57 120,-1362.57 127,-1362.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"154\" y=\"-1374\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_3&#45;&gt;Res_1 -->\r\n",
       "<g id=\"edge7\" class=\"edge\"><title>BN_3&#45;&gt;Res_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M135.304,-1330.47C148.066,-1319.62 168.619,-1302.15 183.83,-1289.22\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"186.295,-1291.72 191.647,-1282.57 181.761,-1286.38 186.295,-1291.72\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"199\" y=\"-1304\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_4 -->\r\n",
       "<g id=\"node9\" class=\"node\"><title>Conv2d_4</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"67,-1190.5 67,-1212.5 236,-1212.5 236,-1190.5 67,-1190.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"151.5\" y=\"-1197.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_4(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_1&#45;&gt;Conv2d_4 -->\r\n",
       "<g id=\"edge9\" class=\"edge\"><title>Res_1&#45;&gt;Conv2d_4</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M195.827,-1260.47C187.839,-1250.02 175.155,-1233.43 165.398,-1220.67\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"168.059,-1218.39 159.204,-1212.57 162.499,-1222.64 168.059,-1218.39\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"212\" y=\"-1234\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Res_2 -->\r\n",
       "<g id=\"node13\" class=\"node\"><title>Res_2</title>\r\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"119,-910.5 119,-932.5 316,-932.5 316,-910.5 119,-910.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"217.5\" y=\"-917.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">28x28x64 Res_2(residual)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_1&#45;&gt;Res_2 -->\r\n",
       "<g id=\"edge14\" class=\"edge\"><title>Res_1&#45;&gt;Res_2</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M224.021,-1260.3C243.395,-1249.05 269.5,-1228.99 269.5,-1202.5 269.5,-1202.5 269.5,-1202.5 269.5,-990.5 269.5,-969.877 254.221,-951.632 240.228,-939.246\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"242.198,-936.334 232.262,-932.657 237.737,-941.728 242.198,-936.334\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"300\" y=\"-1094\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 56 x 56 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_4 -->\r\n",
       "<g id=\"node10\" class=\"node\"><title>BN_4</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"34,-1120.5 34,-1142.5 241,-1142.5 241,-1120.5 34,-1120.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-1127.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">28x28x64 BN_4(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_4&#45;&gt;BN_4 -->\r\n",
       "<g id=\"edge10\" class=\"edge\"><title>Conv2d_4&#45;&gt;BN_4</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M149.434,-1190.47C147.387,-1180.52 144.194,-1165.01 141.626,-1152.54\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"145.019,-1151.66 139.574,-1142.57 138.163,-1153.08 145.019,-1151.66\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"176\" y=\"-1164\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_5 -->\r\n",
       "<g id=\"node11\" class=\"node\"><title>Conv2d_5</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"53,-1050.5 53,-1072.5 222,-1072.5 222,-1050.5 53,-1050.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-1057.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_5(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_4&#45;&gt;Conv2d_5 -->\r\n",
       "<g id=\"edge11\" class=\"edge\"><title>BN_4&#45;&gt;Conv2d_5</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M137.5,-1120.47C137.5,-1110.62 137.5,-1095.33 137.5,-1082.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"141,-1082.57 137.5,-1072.57 134,-1082.57 141,-1082.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-1094\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_5 -->\r\n",
       "<g id=\"node12\" class=\"node\"><title>BN_5</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"34,-980.5 34,-1002.5 241,-1002.5 241,-980.5 34,-980.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"137.5\" y=\"-987.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">28x28x64 BN_5(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_5&#45;&gt;BN_5 -->\r\n",
       "<g id=\"edge12\" class=\"edge\"><title>Conv2d_5&#45;&gt;BN_5</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M137.5,-1050.47C137.5,-1040.62 137.5,-1025.33 137.5,-1012.92\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"141,-1012.57 137.5,-1002.57 134,-1012.57 141,-1012.57\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"168\" y=\"-1024\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_5&#45;&gt;Res_2 -->\r\n",
       "<g id=\"edge13\" class=\"edge\"><title>BN_5&#45;&gt;Res_2</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M149.304,-980.466C162.066,-969.619 182.619,-952.149 197.83,-939.22\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"200.295,-941.718 205.647,-932.575 195.761,-936.385 200.295,-941.718\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"213\" y=\"-954\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_6 -->\r\n",
       "<g id=\"node14\" class=\"node\"><title>Conv2d_6</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"39,-840.5 39,-862.5 208,-862.5 208,-840.5 39,-840.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"123.5\" y=\"-847.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_6(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_2&#45;&gt;Conv2d_6 -->\r\n",
       "<g id=\"edge15\" class=\"edge\"><title>Res_2&#45;&gt;Conv2d_6</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M196.977,-910.44C187.398,-905.395 176.033,-898.896 166.5,-892 157.453,-885.455 148.23,-877.229 140.658,-869.98\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"142.76,-867.139 133.175,-862.624 137.853,-872.131 142.76,-867.139\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"197\" y=\"-884\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Res_3 -->\r\n",
       "<g id=\"node18\" class=\"node\"><title>Res_3</title>\r\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"91,-560.5 91,-582.5 288,-582.5 288,-560.5 91,-560.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"189.5\" y=\"-567.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">14x14x64 Res_3(residual)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_2&#45;&gt;Res_3 -->\r\n",
       "<g id=\"edge20\" class=\"edge\"><title>Res_2&#45;&gt;Res_3</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M223.556,-910.296C230.666,-897.327 241.5,-874.033 241.5,-852.5 241.5,-852.5 241.5,-852.5 241.5,-640.5 241.5,-619.877 226.221,-601.632 212.228,-589.246\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"214.198,-586.334 204.262,-582.657 209.737,-591.728 214.198,-586.334\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"272\" y=\"-744\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 28 x 28 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_6 -->\r\n",
       "<g id=\"node15\" class=\"node\"><title>BN_6</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"6,-770.5 6,-792.5 213,-792.5 213,-770.5 6,-770.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-777.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">14x14x64 BN_6(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_6&#45;&gt;BN_6 -->\r\n",
       "<g id=\"edge16\" class=\"edge\"><title>Conv2d_6&#45;&gt;BN_6</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M121.434,-840.466C119.387,-830.523 116.194,-815.014 113.626,-802.54\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"117.019,-801.664 111.574,-792.575 110.163,-803.075 117.019,-801.664\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"148\" y=\"-814\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_7 -->\r\n",
       "<g id=\"node16\" class=\"node\"><title>Conv2d_7</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"25,-700.5 25,-722.5 194,-722.5 194,-700.5 25,-700.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-707.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_7(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_6&#45;&gt;Conv2d_7 -->\r\n",
       "<g id=\"edge17\" class=\"edge\"><title>BN_6&#45;&gt;Conv2d_7</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M109.5,-770.466C109.5,-760.623 109.5,-745.327 109.5,-732.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"113,-732.575 109.5,-722.575 106,-732.575 113,-732.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"140\" y=\"-744\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_7 -->\r\n",
       "<g id=\"node17\" class=\"node\"><title>BN_7</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"6,-630.5 6,-652.5 213,-652.5 213,-630.5 6,-630.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"109.5\" y=\"-637.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">14x14x64 BN_7(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_7&#45;&gt;BN_7 -->\r\n",
       "<g id=\"edge18\" class=\"edge\"><title>Conv2d_7&#45;&gt;BN_7</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M109.5,-700.466C109.5,-690.623 109.5,-675.327 109.5,-662.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"113,-662.575 109.5,-652.575 106,-662.575 113,-662.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"140\" y=\"-674\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_7&#45;&gt;Res_3 -->\r\n",
       "<g id=\"edge19\" class=\"edge\"><title>BN_7&#45;&gt;Res_3</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M121.304,-630.466C134.066,-619.619 154.619,-602.149 169.83,-589.22\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"172.295,-591.718 177.647,-582.575 167.761,-586.385 172.295,-591.718\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"185\" y=\"-604\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_8 -->\r\n",
       "<g id=\"node19\" class=\"node\"><title>Conv2d_8</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"19,-490.5 19,-512.5 188,-512.5 188,-490.5 19,-490.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"103.5\" y=\"-497.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_8(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_3&#45;&gt;Conv2d_8 -->\r\n",
       "<g id=\"edge21\" class=\"edge\"><title>Res_3&#45;&gt;Conv2d_8</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M171.85,-560.458C163.314,-555.321 153.082,-548.739 144.5,-542 136.051,-535.365 127.386,-527.245 120.211,-520.095\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"122.592,-517.524 113.091,-512.835 117.594,-522.425 122.592,-517.524\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"175\" y=\"-534\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Res_4 -->\r\n",
       "<g id=\"node23\" class=\"node\"><title>Res_4</title>\r\n",
       "<polygon fill=\"#e6f598\" fill-opacity=\"0.250980\" stroke=\"#e6f598\" points=\"82.5,-210.5 82.5,-232.5 262.5,-232.5 262.5,-210.5 82.5,-210.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-217.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">7x7x64 Res_4(residual)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_3&#45;&gt;Res_4 -->\r\n",
       "<g id=\"edge26\" class=\"edge\"><title>Res_3&#45;&gt;Res_4</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M197.382,-560.06C206.271,-547.225 219.5,-524.436 219.5,-502.5 219.5,-502.5 219.5,-502.5 219.5,-290.5 219.5,-270.534 205.619,-252.186 192.944,-239.594\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"195.052,-236.775 185.343,-232.535 190.288,-241.904 195.052,-236.775\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"250\" y=\"-394\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 14 x 14 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_8 -->\r\n",
       "<g id=\"node20\" class=\"node\"><title>BN_8</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"0,-420.5 0,-442.5 191,-442.5 191,-420.5 0,-420.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-427.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">7x7x64 BN_8(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_8&#45;&gt;BN_8 -->\r\n",
       "<g id=\"edge22\" class=\"edge\"><title>Conv2d_8&#45;&gt;BN_8</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M102.32,-490.466C101.15,-480.523 99.3252,-465.014 97.8576,-452.54\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"101.33,-452.097 96.6853,-442.575 94.3777,-452.915 101.33,-452.097\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"125.5\" y=\"-464\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_9 -->\r\n",
       "<g id=\"node21\" class=\"node\"><title>Conv2d_9</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"11,-350.5 11,-372.5 180,-372.5 180,-350.5 11,-350.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-357.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3x3 Conv2d_9(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- BN_8&#45;&gt;Conv2d_9 -->\r\n",
       "<g id=\"edge23\" class=\"edge\"><title>BN_8&#45;&gt;Conv2d_9</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M95.5,-420.466C95.5,-410.623 95.5,-395.327 95.5,-382.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"99.0001,-382.575 95.5,-372.575 92.0001,-382.575 99.0001,-382.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"120.5\" y=\"-394\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_9 -->\r\n",
       "<g id=\"node22\" class=\"node\"><title>BN_9</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"0,-280.5 0,-302.5 191,-302.5 191,-280.5 0,-280.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"95.5\" y=\"-287.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">7x7x64 BN_9(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_9&#45;&gt;BN_9 -->\r\n",
       "<g id=\"edge24\" class=\"edge\"><title>Conv2d_9&#45;&gt;BN_9</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M95.5,-350.466C95.5,-340.623 95.5,-325.327 95.5,-312.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"99.0001,-312.575 95.5,-302.575 92.0001,-312.575 99.0001,-312.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"120.5\" y=\"-324\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- BN_9&#45;&gt;Res_4 -->\r\n",
       "<g id=\"edge25\" class=\"edge\"><title>BN_9&#45;&gt;Res_4</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M106.862,-280.466C119.145,-269.619 138.927,-252.149 153.568,-239.22\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"155.913,-241.818 161.092,-232.575 151.279,-236.571 155.913,-241.818\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"164.5\" y=\"-254\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_10 -->\r\n",
       "<g id=\"node24\" class=\"node\"><title>Conv2d_10</title>\r\n",
       "<polygon fill=\"#fee08b\" fill-opacity=\"0.250980\" stroke=\"#b58c15\" points=\"84,-140.5 84,-162.5 261,-162.5 261,-140.5 84,-140.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-147.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">9x9 Conv2d_10(convo)</text>\r\n",
       "</g>\r\n",
       "<!-- Res_4&#45;&gt;Conv2d_10 -->\r\n",
       "<g id=\"edge27\" class=\"edge\"><title>Res_4&#45;&gt;Conv2d_10</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M172.5,-210.466C172.5,-200.623 172.5,-185.327 172.5,-172.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"176,-172.575 172.5,-162.575 169,-172.575 176,-172.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-184\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 64 </text>\r\n",
       "</g>\r\n",
       "<!-- Pooling_1 -->\r\n",
       "<g id=\"node25\" class=\"node\"><title>Pooling_1</title>\r\n",
       "<polygon fill=\"#66c2a5\" fill-opacity=\"0.250980\" stroke=\"#66c2a5\" points=\"94,-70.5 94,-92.5 251,-92.5 251,-70.5 94,-70.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-77.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">7x7 Pooling_1(pool)</text>\r\n",
       "</g>\r\n",
       "<!-- Conv2d_10&#45;&gt;Pooling_1 -->\r\n",
       "<g id=\"edge28\" class=\"edge\"><title>Conv2d_10&#45;&gt;Pooling_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M172.5,-140.466C172.5,-130.623 172.5,-115.327 172.5,-102.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"176,-102.575 172.5,-92.5748 169,-102.575 176,-102.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-114\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 7 x 7 x 20 </text>\r\n",
       "</g>\r\n",
       "<!-- OutputLayer_1 -->\r\n",
       "<g id=\"node26\" class=\"node\"><title>OutputLayer_1</title>\r\n",
       "<polygon fill=\"#5e4fa2\" fill-opacity=\"0.125490\" stroke=\"#5e4fa2\" points=\"81.5,-0.5 81.5,-22.5 263.5,-22.5 263.5,-0.5 81.5,-0.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"172.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">2 OutputLayer_1(output)</text>\r\n",
       "</g>\r\n",
       "<!-- Pooling_1&#45;&gt;OutputLayer_1 -->\r\n",
       "<g id=\"edge29\" class=\"edge\"><title>Pooling_1&#45;&gt;OutputLayer_1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M172.5,-70.4664C172.5,-60.6231 172.5,-45.327 172.5,-32.9189\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"176,-32.5748 172.5,-22.5748 169,-32.5748 176,-32.5748\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"197.5\" y=\"-44\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 1 x 1 x 20 </text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x27058b27988>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_like_model.plot_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"LoadImageData\"></a>\n",
    "\n",
    "### Load the Image Data into SAS CAS\n",
    "\n",
    "This step loads the server-side Giraffe and Dolphin image data that you downloaded from GitHub and saved under `server_image_root` into SAS CAS as a table named `my_images`. All of the images in the table are resized to uniform dimensions of 112 px by 112 px."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the image path stored in server_image_root \n",
    "img_path = server_image_root\n",
    "\n",
    "# Load the files in the image path to table my_images\n",
    "my_images = ImageTable.load_files(sess, path=img_path)\n",
    "\n",
    "# Resize all images to 112 px by 112 px\n",
    "my_images.resize(112)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"DefineLR\"></a>\n",
    "\n",
    "### Define the Model Learning Rate Hyperparameter\n",
    "\n",
    "The example thus far has used SAS DLPy to create a simple ResNet image classification model and created a 112 x 112 input image table in SAS CAS. Now it is time to consider how to define the best learning rate for this neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SAS DLPy API includes a choice of predefined learning rate policies you can specify:\n",
    "\n",
    "- FixedLR\n",
    "- StepLR\n",
    "- MultiStepLR\n",
    "- PolynomialLR\n",
    "- ReduceLROnPlateau\n",
    "- CyclicLR\n",
    "\n",
    "SAS DLPy also allows you to create your own custom learning rate policy using the SAS FCMP function compiling procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"StepLR\"></a>\n",
    "\n",
    "#### Specify a Step Learning Rate\n",
    "\n",
    "The following code uses SAS DLPy to specify and configure step learning rate `StepLR` hyperparameters for the model `resnet_like_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, learning_rate_policy, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Specify Step Learn Rate parameters\n",
    "lr_scheduler = StepLR(learning_rate=0.0001, \n",
    "                      gamma=0.1, \n",
    "                      step_size=2\n",
    "                     )\n",
    "\n",
    "# Momentum solver using StepLR \n",
    "solver = MomentumSolver(lr_scheduler=lr_scheduler, \n",
    "                        clip_grad_max = 100, \n",
    "                        clip_grad_min = -100\n",
    "                       )\n",
    "\n",
    "# Optimizer and GPU settings\n",
    "optimizer = Optimizer(algorithm=solver, \n",
    "                      mini_batch_size=16, \n",
    "                      log_level=3, \n",
    "                      max_epochs=5, \n",
    "                      reg_l2=0.0005\n",
    "                     )\n",
    "\n",
    "gpu = Gpu(devices=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the SAS DLPy `fit()` function to train the model `resnet_like_model` with the specified hyperparameter settings and the input data in CAS table `my_images`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training from scratch.\n",
      "NOTE: Using your-server.unx.your-company.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       3.24 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001            1.439     0.5938     0.2988     0.83\n",
      "NOTE:      1    64   0.0001            1.059     0.4219     0.2988     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0        0.0001           1.249     0.5078     0.87\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001            1.584     0.6563     0.2988     0.05\n",
      "NOTE:      1    64   0.0001            1.434     0.5938     0.2988     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1        0.0001           1.509      0.625     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001            1.363     0.5625     0.2988     0.05\n",
      "NOTE:      1    64  0.00001            1.221        0.5     0.2988     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          1E-5           1.292     0.5313     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001            1.039     0.4219     0.2988     0.05\n",
      "NOTE:      1    64  0.00001            1.177     0.4844     0.2988     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3          1E-5           1.108     0.4531     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6            1.203        0.5     0.2988     0.05\n",
      "NOTE:      1    64     1E-6            1.003     0.4063     0.2988     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4          1E-6           1.103     0.4531     0.09\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       1.24 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_8zia1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.248677</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.298831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>1.508753</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.298831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.291920</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.298830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>1.108240</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.298829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.103452</td>\n",
       "      <td>0.453125</td>\n",
       "      <td>0.298828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CASUSER(UserID)</td>\n",
       "      <td>Model_8ZIA1w_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_8ZIA1w_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 5.11s</span> &#183; <span class=\"cas-user\">user 1.48s</span> &#183; <span class=\"cas-sys\">sys 1.1s</span> &#183; <span class=\"cas-memory\">mem 132MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_8zia1w\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "    Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0      1      0.000100  1.248677  0.507812  0.298831\n",
       " 1      2      0.000100  1.508753  0.625000  0.298831\n",
       " 2      3      0.000010  1.291920  0.531250  0.298830\n",
       " 3      4      0.000010  1.108240  0.453125  0.298829\n",
       " 4      5      0.000001  1.103452  0.453125  0.298828\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(UserID)  Model_8ZIA1w_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_8ZIA1w_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 5.11s, user: 1.48s, sys: 1.1s, mem: 132mb"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model \n",
    "resnet_like_model.fit(data=my_images, \n",
    "                      n_threads=4, \n",
    "                      record_seed=13309, \n",
    "                      optimizer=optimizer,\n",
    "                      gpu=gpu, \n",
    "                      log_level=2\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `resnet_like_model` when trained with a `StepLR` learning rate policy, has a final learning rate of 0.000001, a loss value of 1.103452, and a fit error of 0.453125. We can compare these statistics to those of other versions of this model that use different learning rate policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"CyclicLR\"></a>\n",
    "\n",
    "#### Specify Cyclic Learning Rate Scheduler\n",
    "\n",
    "\n",
    "The following code uses SAS DLPy to specify and configure cyclic learning rate `CyclicLR` hyperparameters for the model `resnet_like_model`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Specify Cyclic Learn Rate parameters\n",
    "lr_scheduler = CyclicLR(conn=sess, \n",
    "                        data=my_images, \n",
    "                        max_lr=0.01, \n",
    "                        batch_size=1, \n",
    "                        factor=2,\n",
    "                        learning_rate=0.0001\n",
    "                       )\n",
    "\n",
    "# Momentum solver using CyclicLR \n",
    "solver = MomentumSolver(lr_scheduler = lr_scheduler,\n",
    "                        clip_grad_max = 100, \n",
    "                        clip_grad_min = -100\n",
    "                       )\n",
    "\n",
    "# Optimizer and GPU settings\n",
    "optimizer = Optimizer(algorithm=solver, \n",
    "                      mini_batch_size=16, \n",
    "                      log_level=3, \n",
    "                      max_epochs=50, \n",
    "                      reg_l2=0.0005\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the SAS DLPy `fit()` function to train the model `resnet_like_model` with the specified hyperparameter settings and the input data in CAS table `my_images`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE: Using your-server.unx.your-company.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.01 (s).\n",
      "NOTE:  Initializing each layer cost       0.91 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001             1.41     0.5938     0.2988     0.84\n",
      "NOTE:      1    64 0.000153            1.042     0.4219     0.2988     0.05\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0        0.0002           1.226     0.5078     0.88\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505            1.555     0.6563     0.2988     0.05\n",
      "NOTE:      1    64 0.005103            1.405     0.5938     0.2988     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1        0.0051            1.48      0.625     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01            1.284     0.5625     0.2988     0.05\n",
      "NOTE:      1    64 0.009947            1.072        0.5     0.2988     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2        0.0099           1.178     0.5313     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.8094     0.4219     0.2988     0.05\n",
      "NOTE:      1    64 0.004997           0.7563     0.4375     0.2987     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3         0.005          0.7829     0.4297     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.6264     0.4219     0.2987     0.05\n",
      "NOTE:      1    64 0.000153            0.523     0.1094     0.2987     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4        0.0002          0.5747     0.2656     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.5615    0.09375     0.2987     0.05\n",
      "NOTE:      1    64 0.005103           0.5577     0.5156     0.2987     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5        0.0051          0.5596     0.3047     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.6453     0.5938     0.2987     0.05\n",
      "NOTE:      1    64 0.009947           0.6916     0.6094     0.2987     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6        0.0099          0.6684     0.6016     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.5754     0.5313     0.2987     0.05\n",
      "NOTE:      1    64 0.004997           0.4675     0.4531     0.2986     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7         0.005          0.5215     0.4922     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.5039     0.4844     0.2986     0.05\n",
      "NOTE:      1    64 0.000153           0.3232    0.09375     0.2986     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8        0.0002          0.4136     0.2891     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.2101          0     0.2986     0.05\n",
      "NOTE:      1    64 0.005103           0.1762          0     0.2986     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9        0.0051          0.1932          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.1651    0.01563     0.2986     0.05\n",
      "NOTE:      1    64 0.009947           0.1289          0     0.2986     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10       0.0099           0.147   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.09973          0     0.2986     0.05\n",
      "NOTE:      1    64 0.004997           0.0758          0     0.2986     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.005         0.08777          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.06228          0     0.2986     0.05\n",
      "NOTE:      1    64 0.000153           0.0549          0     0.2985     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12       0.0002         0.05859          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.05626    0.01563     0.2985     0.05\n",
      "NOTE:      1    64 0.005103          0.04917          0     0.2985     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13       0.0051         0.05272   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.06603    0.01563     0.2985     0.05\n",
      "NOTE:      1    64 0.009947          0.05129    0.01563     0.2985     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14       0.0099         0.05866    0.01563     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.08791    0.01563     0.2985     0.05\n",
      "NOTE:      1    64 0.004997          0.02908          0     0.2985     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15        0.005          0.0585   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.02507          0     0.2985     0.05\n",
      "NOTE:      1    64 0.000153            0.208    0.09375     0.2985     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16       0.0002          0.1166    0.04688     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.0291          0     0.2985     0.05\n",
      "NOTE:      1    64 0.005103          0.02984          0     0.2985     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0051         0.02947          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.03061          0     0.2985     0.05\n",
      "NOTE:      1    64 0.009947          0.02215          0     0.2984     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18       0.0099         0.02638          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.01869          0     0.2984     0.05\n",
      "NOTE:      1    64 0.004997           0.0214          0     0.2984     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19        0.005         0.02005          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.01921          0     0.2984     0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1    64 0.000153          0.02078          0     0.2984     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20       0.0002            0.02          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.01793          0     0.2983     0.05\n",
      "NOTE:      1    64 0.005103          0.01861          0     0.2983     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21       0.0051         0.01827          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.01453          0     0.2983     0.05\n",
      "NOTE:      1    64 0.009947          0.06549    0.01563     0.2983     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22       0.0099         0.04001   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.0292          0     0.2983     0.05\n",
      "NOTE:      1    64 0.004997          0.01704          0     0.2983     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23        0.005         0.02312          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.02375          0     0.2982     0.05\n",
      "NOTE:      1    64 0.000153          0.01335          0     0.2982     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24       0.0002         0.01855          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.009176          0     0.2982     0.05\n",
      "NOTE:      1    64 0.005103          0.01447          0     0.2982     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25       0.0051         0.01182          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.01041          0     0.2981     0.05\n",
      "NOTE:      1    64 0.009947          0.01247          0     0.2981     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26       0.0099         0.01144          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.009057          0     0.2981     0.05\n",
      "NOTE:      1    64 0.004997          0.01258          0     0.2981     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27        0.005         0.01082          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.01126          0      0.298     0.05\n",
      "NOTE:      1    64 0.000153          0.01072          0      0.298     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28       0.0002         0.01099          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.01882          0      0.298     0.05\n",
      "NOTE:      1    64 0.005103         0.009839          0      0.298     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29       0.0051         0.01433          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.0126          0     0.2979     0.05\n",
      "NOTE:      1    64 0.009947         0.006904          0     0.2979     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30       0.0099         0.00975          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.02299    0.01563     0.2979     0.05\n",
      "NOTE:      1    64 0.004997          0.01337          0     0.2979     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31        0.005         0.01818   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.008797          0     0.2978     0.05\n",
      "NOTE:      1    64 0.000153          0.01096          0     0.2978     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32       0.0002        0.009878          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.03192    0.03125     0.2978     0.05\n",
      "NOTE:      1    64 0.005103         0.006933          0     0.2978     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33       0.0051         0.01943    0.01563     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.01294          0     0.2977     0.05\n",
      "NOTE:      1    64 0.009947           0.0139          0     0.2977     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34       0.0099         0.01342          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.008486          0     0.2977     0.05\n",
      "NOTE:      1    64 0.004997          0.00689          0     0.2977     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35        0.005        0.007688          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.02187          0     0.2976     0.05\n",
      "NOTE:      1    64 0.000153         0.006366          0     0.2976     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36       0.0002         0.01412          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.007984          0     0.2976     0.05\n",
      "NOTE:      1    64 0.005103         0.006345          0     0.2976     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37       0.0051        0.007165          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.006257          0     0.2975     0.05\n",
      "NOTE:      1    64 0.009947         0.006965          0     0.2975     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38       0.0099        0.006611          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.007983          0     0.2975     0.05\n",
      "NOTE:      1    64 0.004997         0.006726          0     0.2974     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39        0.005        0.007355          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.1853    0.07813     0.2974     0.05\n",
      "NOTE:      1    64 0.000153         0.004672          0     0.2974     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40       0.0002         0.09496    0.03906     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.1677    0.07813     0.2974     0.05\n",
      "NOTE:      1    64 0.005103           0.0123          0     0.2973     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41       0.0051         0.09001    0.03906     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.0553    0.01563     0.2973     0.05\n",
      "NOTE:      1    64 0.009947           0.1522     0.0625     0.2973     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42       0.0099          0.1038    0.03906     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505           0.2111    0.07813     0.2973     0.05\n",
      "NOTE:      1    64 0.004997          0.06391    0.01563     0.2973     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43        0.005          0.1375    0.04688     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.05947    0.03125     0.2972     0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1    64 0.000153         0.005557          0     0.2972     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44       0.0002         0.03251    0.01563     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.007041          0     0.2972     0.05\n",
      "NOTE:      1    64 0.005103         0.007628          0     0.2972     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45       0.0051        0.007334          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.01899          0     0.2972     0.05\n",
      "NOTE:      1    64 0.009947         0.008192          0     0.2972     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46       0.0099         0.01359          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505         0.006053          0     0.2972     0.05\n",
      "NOTE:      1    64 0.004997           0.1831    0.09375     0.2971     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47        0.005         0.09459    0.04688     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.01532          0     0.2971     0.05\n",
      "NOTE:      1    64 0.000153         0.005194          0     0.2971     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48       0.0002         0.01026          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00505          0.01828          0     0.2971     0.05\n",
      "NOTE:      1    64 0.005103          0.03406    0.01563     0.2971     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49       0.0051         0.02617   0.007813     0.09\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       5.36 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_8zia1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>1.225861</td>\n",
       "      <td>0.507812</td>\n",
       "      <td>0.298827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>1.480205</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.298826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>1.178211</td>\n",
       "      <td>0.531250</td>\n",
       "      <td>0.298809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.782886</td>\n",
       "      <td>0.429688</td>\n",
       "      <td>0.298761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.574695</td>\n",
       "      <td>0.265625</td>\n",
       "      <td>0.298711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.559598</td>\n",
       "      <td>0.304688</td>\n",
       "      <td>0.298683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.668450</td>\n",
       "      <td>0.601562</td>\n",
       "      <td>0.298667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.521474</td>\n",
       "      <td>0.492188</td>\n",
       "      <td>0.298642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.413586</td>\n",
       "      <td>0.289062</td>\n",
       "      <td>0.298612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.193171</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.146998</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.298580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.087767</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.058592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.052717</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.298543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.058659</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.298535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.058496</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.298517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>22</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.116550</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.298492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.029471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.026379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.020049</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.019995</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.040008</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.298307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>29</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.023116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.018550</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.011824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.011436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.010817</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.010987</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.298029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>35</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.014331</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.009750</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>37</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.018181</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.297884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.009878</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.019426</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.297777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.013420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.007688</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>42</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.014118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>43</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>44</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>45</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>46</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.094963</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.297403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>47</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.090011</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.297355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>48</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.103757</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.297318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>49</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.137500</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.297274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.032512</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.297228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>51</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>52</td>\n",
       "      <td>0.009947</td>\n",
       "      <td>0.013592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>53</td>\n",
       "      <td>0.004997</td>\n",
       "      <td>0.094592</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.297144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>54</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.010259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>55</td>\n",
       "      <td>0.005103</td>\n",
       "      <td>0.026172</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.297073</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CASUSER(UserID)</td>\n",
       "      <td>Model_8ZIA1w_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_8ZIA1w_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 6.98s</span> &#183; <span class=\"cas-user\">user 5.84s</span> &#183; <span class=\"cas-sys\">sys 2.21s</span> &#183; <span class=\"cas-memory\">mem 132MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_8zia1w\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0       6      0.000153  1.225861  0.507812  0.298827\n",
       " 1       7      0.005103  1.480205  0.625000  0.298826\n",
       " 2       8      0.009947  1.178211  0.531250  0.298809\n",
       " 3       9      0.004997  0.782886  0.429688  0.298761\n",
       " 4      10      0.000153  0.574695  0.265625  0.298711\n",
       " 5      11      0.005103  0.559598  0.304688  0.298683\n",
       " 6      12      0.009947  0.668450  0.601562  0.298667\n",
       " 7      13      0.004997  0.521474  0.492188  0.298642\n",
       " 8      14      0.000153  0.413586  0.289062  0.298612\n",
       " 9      15      0.005103  0.193171  0.000000  0.298592\n",
       " 10     16      0.009947  0.146998  0.007812  0.298580\n",
       " 11     17      0.004997  0.087767  0.000000  0.298567\n",
       " 12     18      0.000153  0.058592  0.000000  0.298552\n",
       " 13     19      0.005103  0.052717  0.007812  0.298543\n",
       " 14     20      0.009947  0.058659  0.015625  0.298535\n",
       " 15     21      0.004997  0.058496  0.007812  0.298517\n",
       " 16     22      0.000153  0.116550  0.046875  0.298492\n",
       " 17     23      0.005103  0.029471  0.000000  0.298469\n",
       " 18     24      0.009947  0.026379  0.000000  0.298447\n",
       " 19     25      0.004997  0.020049  0.000000  0.298416\n",
       " 20     26      0.000153  0.019995  0.000000  0.298377\n",
       " 21     27      0.005103  0.018271  0.000000  0.298341\n",
       " 22     28      0.009947  0.040008  0.007812  0.298307\n",
       " 23     29      0.004997  0.023116  0.000000  0.298264\n",
       " 24     30      0.000153  0.018550  0.000000  0.298215\n",
       " 25     31      0.005103  0.011824  0.000000  0.298171\n",
       " 26     32      0.009947  0.011436  0.000000  0.298131\n",
       " 27     33      0.004997  0.010817  0.000000  0.298083\n",
       " 28     34      0.000153  0.010987  0.000000  0.298029\n",
       " 29     35      0.005103  0.014331  0.000000  0.297981\n",
       " 30     36      0.009947  0.009750  0.000000  0.297936\n",
       " 31     37      0.004997  0.018181  0.007812  0.297884\n",
       " 32     38      0.000153  0.009878  0.000000  0.297827\n",
       " 33     39      0.005103  0.019426  0.015625  0.297777\n",
       " 34     40      0.009947  0.013420  0.000000  0.297730\n",
       " 35     41      0.004997  0.007688  0.000000  0.297677\n",
       " 36     42      0.000153  0.014118  0.000000  0.297619\n",
       " 37     43      0.005103  0.007165  0.000000  0.297566\n",
       " 38     44      0.009947  0.006611  0.000000  0.297518\n",
       " 39     45      0.004997  0.007355  0.000000  0.297463\n",
       " 40     46      0.000153  0.094963  0.039062  0.297403\n",
       " 41     47      0.005103  0.090011  0.039062  0.297355\n",
       " 42     48      0.009947  0.103757  0.039062  0.297318\n",
       " 43     49      0.004997  0.137500  0.046875  0.297274\n",
       " 44     50      0.000153  0.032512  0.015625  0.297228\n",
       " 45     51      0.005103  0.007334  0.000000  0.297196\n",
       " 46     52      0.009947  0.013592  0.000000  0.297173\n",
       " 47     53      0.004997  0.094592  0.046875  0.297144\n",
       " 48     54      0.000153  0.010259  0.000000  0.297106\n",
       " 49     55      0.005103  0.026172  0.007812  0.297073\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(UserID)  Model_8ZIA1w_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_8ZIA1w_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 6.98s, user: 5.84s, sys: 2.21s, mem: 132mb"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "resnet_like_model.fit(data=my_images, \n",
    "                      n_threads=4, \n",
    "                      record_seed=13309, \n",
    "                      optimizer=optimizer,\n",
    "                      gpu=gpu, \n",
    "                      log_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `resnet_like_model` when trained with a `CyclicLR` learning rate policy, has a final learning rate of 0.005103, a loss value of 0.026172, and a fit error of 0.007812.  By comparision, the same model trained using the `StepLR` learning rate policy had a final learning rate of 0.000001, a loss value of 1.103452, and a fit error of 0.453125.\n",
    "\n",
    "When you compare the fit and error statistics, the `resnet_like_model` that was trained using the `CyclicLR` learning rate policy outperforms the model that was trained using the `StepLR` learning rate policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ReduceLR_Plat\"></a>\n",
    "\n",
    "#### Specify Reduce Learning Rate on Plateau\n",
    "\n",
    "The following code uses SAS DLPy to specify and configure the reduce learning rate on plateau `ReduceLROnPlateau` hyperparameters for the model `resnet_like_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Specify ReduceLROnPlateau hyperparameter settings\n",
    "lr_scheduler = ReduceLROnPlateau(conn=sess, \n",
    "                                 cool_down_iters=2, \n",
    "                                 gamma=0.1, \n",
    "                                 learning_rate=0.01, \n",
    "                                 patience=3\n",
    "                                )\n",
    "\n",
    "# Specify Momentum solver parameters\n",
    "solver = MomentumSolver(lr_scheduler = lr_scheduler,\n",
    "                        clip_grad_max = 100, \n",
    "                        clip_grad_min = -100\n",
    "                       )\n",
    "\n",
    "# Specify optimizer settings\n",
    "optimizer = Optimizer(algorithm=solver, \n",
    "                      mini_batch_size=16, \n",
    "                      log_level=3, \n",
    "                      max_epochs=50, \n",
    "                      reg_l2=0.0005\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE: Using your-server.unx.your-company.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.01 (s).\n",
      "NOTE:  Initializing each layer cost       0.97 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.1022     0.0625     0.2971     0.83\n",
      "NOTE:      1    64     0.01          0.03383          0      0.297     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0          0.01         0.06802    0.03125     0.88\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.005659          0      0.297     0.05\n",
      "NOTE:      1    64     0.01         0.004113          0      0.297     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1          0.01        0.004886          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.004471          0      0.297     0.05\n",
      "NOTE:      1    64     0.01         0.007989          0      0.297     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          0.01         0.00623          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.02363          0      0.297     0.05\n",
      "NOTE:      1    64     0.01          0.03034          0      0.297     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3          0.01         0.02698          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.007546          0     0.2969     0.05\n",
      "NOTE:      1    64     0.01          0.07085    0.04688     0.2969     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4          0.01          0.0392    0.02344     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.1046    0.04688     0.2969     0.05\n",
      "NOTE:      1    64     0.01         0.005455          0     0.2969     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5          0.01         0.05503    0.02344     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.01084          0     0.2968     0.05\n",
      "NOTE:      1    64    0.001         0.006135          0     0.2968     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6         0.001        0.008486          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.005673          0     0.2968     0.05\n",
      "NOTE:      1    64    0.001          0.01047          0     0.2968     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7         0.001         0.00807          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.01017          0     0.2968     0.05\n",
      "NOTE:      1    64    0.001          0.01151          0     0.2968     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8         0.001         0.01084          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.01507          0     0.2967     0.05\n",
      "NOTE:      1    64    0.001          0.04861    0.03125     0.2967     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9         0.001         0.03184    0.01563     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.01807          0     0.2967     0.05\n",
      "NOTE:      1    64    0.001         0.005213          0     0.2967     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10        0.001         0.01164          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.02016    0.01563     0.2967     0.05\n",
      "NOTE:      1    64    0.001         0.007811          0     0.2967     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.001         0.01399   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.007224          0     0.2967     0.05\n",
      "NOTE:      1    64   0.0001         0.008774          0     0.2967     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12       0.0001        0.007999          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.1215    0.04688     0.2967     0.05\n",
      "NOTE:      1    64   0.0001          0.01178          0     0.2967     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13       0.0001         0.06665    0.02344     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.0215          0     0.2967     0.05\n",
      "NOTE:      1    64   0.0001          0.01754    0.01563     0.2967     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14       0.0001         0.01952   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001            0.109    0.03125     0.2967     0.05\n",
      "NOTE:      1    64   0.0001          0.01231          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15       0.0001         0.06063    0.01563     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.007567          0     0.2966     0.05\n",
      "NOTE:      1    64   0.0001           0.4956     0.1563     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16       0.0001          0.2516    0.07813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001           0.1044    0.04688     0.2966     0.05\n",
      "NOTE:      1    64   0.0001           0.2668     0.1094     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0001          0.1856    0.07813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.06101    0.01563     0.2966     0.05\n",
      "NOTE:      1    64  0.00001           0.2002    0.09375     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18         1E-5          0.1306    0.05469     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.01689          0     0.2966     0.05\n",
      "NOTE:      1    64  0.00001           0.2365     0.1406     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19         1E-5          0.1267    0.07031     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.06856    0.01563     0.2966     0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1    64  0.00001           0.0224          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20         1E-5         0.04548   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.03653    0.01563     0.2966     0.05\n",
      "NOTE:      1    64  0.00001          0.03278          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21         1E-5         0.03466   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.01072          0     0.2966     0.05\n",
      "NOTE:      1    64  0.00001         0.006113          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22         1E-5        0.008419          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.006976          0     0.2966     0.05\n",
      "NOTE:      1    64  0.00001           0.1222    0.03125     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23         1E-5         0.06458    0.01563     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.007814          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-6          0.01363          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24         1E-6         0.01072          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.009193          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-6         0.007008          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25         1E-6        0.008101          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6          0.01377          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-6          0.07087    0.01563     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26         1E-6         0.04232   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6          0.03248    0.01563     0.2966     0.05\n",
      "NOTE:      1    64     1E-6           0.1341    0.04688     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27         1E-6         0.08331    0.03125     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6          0.02742          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-6           0.0104          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28         1E-6         0.01891          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.1292    0.03125     0.2966     0.05\n",
      "NOTE:      1    64     1E-6          0.01716          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29         1E-6         0.07319    0.01563     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7           0.1017    0.03125     0.2966     0.05\n",
      "NOTE:      1    64     1E-7          0.06333    0.03125     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30         1E-7         0.08251    0.03125     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7          0.04159    0.01563     0.2966     0.05\n",
      "NOTE:      1    64     1E-7          0.01084          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31         1E-7         0.02621   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7          0.00676          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-7           0.1003    0.04688     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32         1E-7         0.05354    0.02344     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7           0.2922     0.1406     0.2966     0.05\n",
      "NOTE:      1    64     1E-7          0.01375          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33         1E-7           0.153    0.07031     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.008227          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-7          0.01403          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34         1E-7         0.01113          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7           0.0413          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-7          0.02896    0.01563     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35         1E-7         0.03513   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8           0.1877    0.07813     0.2966     0.05\n",
      "NOTE:      1    64     1E-8         0.004965          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36         1E-8         0.09636    0.03906     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.006016          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-8           0.0143          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37         1E-8         0.01016          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.01792          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-8          0.01203          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38         1E-8         0.01498          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.01672          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-8           0.2326    0.09375     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39         1E-8          0.1246    0.04688     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.006434          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-8          0.01911          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40         1E-8         0.01277          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8           0.1768     0.0625     0.2966     0.05\n",
      "NOTE:      1    64     1E-8         0.007208          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41         1E-8         0.09203    0.03125     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-9         0.005529          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-9          0.01519          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42         1E-9         0.01036          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-9           0.0257          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-9          0.06578    0.01563     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43         1E-9         0.04574   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-9           0.1687    0.09375     0.2966     0.05\n",
      "NOTE:      1    64     1E-9          0.02275    0.01563     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44         1E-9         0.09572    0.05469     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-9           0.0088          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-9          0.01015          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45         1E-9        0.009474          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-9         0.006163          0     0.2966     0.05\n",
      "NOTE:      1    64     1E-9          0.07538    0.01563     0.2966     0.04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46         1E-9         0.04077   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-9           0.1211    0.07813     0.2966     0.05\n",
      "NOTE:      1    64     1E-9         0.007988          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47         1E-9         0.06454    0.03906     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    1E-10          0.01762          0     0.2966     0.05\n",
      "NOTE:      1    64    1E-10          0.06928    0.03125     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48        1E-10         0.04345    0.01563     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    1E-10          0.01124          0     0.2966     0.05\n",
      "NOTE:      1    64    1E-10         0.006298          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49        1E-10         0.00877          0     0.09\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       5.36 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_8zia1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.068017</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.297048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.004886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.006230</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.297013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>59</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.026982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>60</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.296936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>61</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.055034</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.296884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>62</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.008486</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>63</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.008070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.010844</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>65</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.031841</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.296732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>66</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.011639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.013987</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.296695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.007999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>69</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.066648</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.296667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>70</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.019518</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.296657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>71</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.060630</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.296649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>72</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.251588</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.296643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>73</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.185575</td>\n",
       "      <td>0.078125</td>\n",
       "      <td>0.296637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>74</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.130608</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.296632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>75</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.126690</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.296629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>76</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.045479</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.296626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>77</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.034656</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.296623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>78</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.008419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.064578</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.296619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>80</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.010724</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>81</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.008101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>82</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.042320</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.296616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>83</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.083306</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.296615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>84</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.018907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>85</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.073193</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.296614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>86</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.082510</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.296614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>87</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.026211</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.296613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>88</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.053545</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.296613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>89</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.152968</td>\n",
       "      <td>0.070312</td>\n",
       "      <td>0.296613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>90</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.011127</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>91</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.035131</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.296613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>92</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.096357</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.296613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>93</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.010157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>94</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.014976</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>95</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.124646</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>96</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.012774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>97</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.092029</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>98</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.010359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>99</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.045739</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>100</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.095720</td>\n",
       "      <td>0.054688</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>101</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.009474</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>102</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.040773</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>103</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.064541</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>104</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.043450</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>105</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.008770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CASUSER(UserID)</td>\n",
       "      <td>Model_8ZIA1w_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_8ZIA1w_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 7.36s</span> &#183; <span class=\"cas-user\">user 5.83s</span> &#183; <span class=\"cas-sys\">sys 2.59s</span> &#183; <span class=\"cas-memory\">mem 132MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_8zia1w\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0      56  1.000000e-02  0.068017  0.031250  0.297048\n",
       " 1      57  1.000000e-02  0.004886  0.000000  0.297035\n",
       " 2      58  1.000000e-02  0.006230  0.000000  0.297013\n",
       " 3      59  1.000000e-02  0.026982  0.000000  0.296980\n",
       " 4      60  1.000000e-02  0.039200  0.023438  0.296936\n",
       " 5      61  1.000000e-02  0.055034  0.023438  0.296884\n",
       " 6      62  1.000000e-03  0.008486  0.000000  0.296830\n",
       " 7      63  1.000000e-03  0.008070  0.000000  0.296789\n",
       " 8      64  1.000000e-03  0.010844  0.000000  0.296757\n",
       " 9      65  1.000000e-03  0.031841  0.015625  0.296732\n",
       " 10     66  1.000000e-03  0.011639  0.000000  0.296712\n",
       " 11     67  1.000000e-03  0.013987  0.007812  0.296695\n",
       " 12     68  1.000000e-04  0.007999  0.000000  0.296679\n",
       " 13     69  1.000000e-04  0.066648  0.023438  0.296667\n",
       " 14     70  1.000000e-04  0.019518  0.007812  0.296657\n",
       " 15     71  1.000000e-04  0.060630  0.015625  0.296649\n",
       " 16     72  1.000000e-04  0.251588  0.078125  0.296643\n",
       " 17     73  1.000000e-04  0.185575  0.078125  0.296637\n",
       " 18     74  1.000000e-05  0.130608  0.054688  0.296632\n",
       " 19     75  1.000000e-05  0.126690  0.070312  0.296629\n",
       " 20     76  1.000000e-05  0.045479  0.007812  0.296626\n",
       " 21     77  1.000000e-05  0.034656  0.007812  0.296623\n",
       " 22     78  1.000000e-05  0.008419  0.000000  0.296621\n",
       " 23     79  1.000000e-05  0.064578  0.015625  0.296619\n",
       " 24     80  1.000000e-06  0.010724  0.000000  0.296618\n",
       " 25     81  1.000000e-06  0.008101  0.000000  0.296617\n",
       " 26     82  1.000000e-06  0.042320  0.007812  0.296616\n",
       " 27     83  1.000000e-06  0.083306  0.031250  0.296615\n",
       " 28     84  1.000000e-06  0.018907  0.000000  0.296615\n",
       " 29     85  1.000000e-06  0.073193  0.015625  0.296614\n",
       " 30     86  1.000000e-07  0.082510  0.031250  0.296614\n",
       " 31     87  1.000000e-07  0.026211  0.007812  0.296613\n",
       " 32     88  1.000000e-07  0.053545  0.023438  0.296613\n",
       " 33     89  1.000000e-07  0.152968  0.070312  0.296613\n",
       " 34     90  1.000000e-07  0.011127  0.000000  0.296613\n",
       " 35     91  1.000000e-07  0.035131  0.007812  0.296613\n",
       " 36     92  1.000000e-08  0.096357  0.039062  0.296613\n",
       " 37     93  1.000000e-08  0.010157  0.000000  0.296613\n",
       " 38     94  1.000000e-08  0.014976  0.000000  0.296612\n",
       " 39     95  1.000000e-08  0.124646  0.046875  0.296612\n",
       " 40     96  1.000000e-08  0.012774  0.000000  0.296612\n",
       " 41     97  1.000000e-08  0.092029  0.031250  0.296612\n",
       " 42     98  1.000000e-09  0.010359  0.000000  0.296612\n",
       " 43     99  1.000000e-09  0.045739  0.007812  0.296612\n",
       " 44    100  1.000000e-09  0.095720  0.054688  0.296612\n",
       " 45    101  1.000000e-09  0.009474  0.000000  0.296612\n",
       " 46    102  1.000000e-09  0.040773  0.007812  0.296612\n",
       " 47    103  1.000000e-09  0.064541  0.039062  0.296612\n",
       " 48    104  1.000000e-10  0.043450  0.015625  0.296612\n",
       " 49    105  1.000000e-10  0.008770  0.000000  0.296612\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(UserID)  Model_8ZIA1w_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_8ZIA1w_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 7.36s, user: 5.83s, sys: 2.59s, mem: 132mb"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "resnet_like_model.fit(data=my_images, \n",
    "                      n_threads=4, \n",
    "                      record_seed=13309, \n",
    "                      optimizer=optimizer,\n",
    "                      gpu=gpu, \n",
    "                      log_level=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `resnet_like_model`, when trained with a `ReduceLROnPlateau` learning rate policy, has a final learning rate of 1.000000e-10, a loss value of 0.008770, and a fit error of 0.00.  By comparision, the same model trained with the `CyclicLR` learning rate policy has a final learning rate of 0.005103, a loss value of 0.026172, and a fit error of 0.007812.  The same model trained with the `StepLR` learning rate policy had a final learning rate of 0.000001, a loss value of 1.103452, and a fit error of 0.453125.\n",
    "\n",
    "When you compare the fit and error statistics, the model that was trained using the `ReduceLROnPlateau` learning rate policy outperforms the models that were trained using the `CyclicLR` and `StepLR` learning rate policies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"CustomLR\"></a>\n",
    "\n",
    "#### Specify Customized Learning Rate Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SAS DLPy API also provides a flexible approach to defining your own version of a learning rate policy. DLPy enables you to use the SAS FCMP function compiler to create your own custom learning rate policies.\n",
    "\n",
    "The following SAS function compiler code creates a custom learning rate policy called `reduce_lr_on_plateau`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.00395s</span> &#183; <span class=\"cas-user\">user 0.00384s</span> &#183; <span class=\"cas-sys\">sys 9e-05s</span> &#183; <span class=\"cas-memory\">mem 3.34MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 0.00395s, user: 0.00384s, sys: 9e-05s, mem: 3.34mb"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cool_down_iters = 5\n",
    "patience = 1\n",
    "sess.addRoutines(\n",
    "            routineCode = '''\n",
    "                        function reduce_lr_on_plateau(rate, initRate, gamma, loss[*]);\n",
    "                            len = dim(loss);\n",
    "                            temp_rate = initRate;\n",
    "                            cool_down_counter = {0};\n",
    "                            best = loss[1];\n",
    "                            do i=1 to len;\n",
    "                    \n",
    "                                if loss[i] < best then do;\n",
    "                                    best = loss[i];\n",
    "                                    bad_epoch = 0;\n",
    "                                end;\n",
    "                                else bad_epoch = bad_epoch + 1;\n",
    "                    \n",
    "                                if cool_down_counter > 0 then do;\n",
    "                                    cool_down_counter = cool_down_counter - 1;\n",
    "                                    bad_epoch = 0;\n",
    "                                end;\n",
    "                    \n",
    "                                if bad_epoch > {1} then do;\n",
    "                                    temp_rate = temp_rate * gamma;\n",
    "                                    cool_down_counter = {0};\n",
    "                                    bad_epoch = 0;\n",
    "                                end;\n",
    "                            end;\n",
    "                            rate = temp_rate;\n",
    "                            put rate=;\n",
    "                            return(rate);\n",
    "                        endsub;\n",
    "                        '''.format(cool_down_iters, patience),\n",
    "            package = 'pkg',\n",
    "            funcTable = dict(name = 'reduce_lr_on_plateau', replace = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You custom FCMP function can be called by invoking the FCMP function name while setting `fcmp_learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Specify the custom LR function \n",
    "# and settings in 'fcmp_learning_rate'\n",
    "lr_scheduler = FCMPLR(conn=sess, \n",
    "                      fcmp_learning_rate='reduce_lr_on_plateau',\n",
    "                      learning_rate = 0.01, \n",
    "                      gamma = 0.1\n",
    "                     )\n",
    "\n",
    "# Specify Momentum solver settings\n",
    "solver = MomentumSolver(lr_scheduler = lr_scheduler,\n",
    "                        clip_grad_max = 100, \n",
    "                        clip_grad_min = -100\n",
    "                       )\n",
    "\n",
    "# Specify optimizer settings\n",
    "optimizer = Optimizer(algorithm=solver, \n",
    "                      mini_batch_size=16, \n",
    "                      log_level=3, \n",
    "                      max_epochs=50, \n",
    "                      reg_l2=0.0005\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Inputs=_image_ is used\n",
      "NOTE: Training based on existing weights.\n",
      "NOTE: Using your-server.unx.your-company.com: 1 out of 2 available GPU devices.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 415358.\n",
      "NOTE:  The approximate memory cost is 115.00 MB.\n",
      "NOTE:  Loading weights cost       0.01 (s).\n",
      "NOTE:  Initializing each layer cost       1.08 (s).\n",
      "NOTE:  The total number of threads on each worker is 4.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 16.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 64.\n",
      "NOTE:  Target variable: _label_\n",
      "NOTE:  Number of levels for the target variable:      2\n",
      "NOTE:  Levels for the target variable:\n",
      "NOTE:  Level      0: Dolphin\n",
      "NOTE:  Level      1: Giraffe\n",
      "NOTE:  Number of input variables:     1\n",
      "NOTE:  Number of numeric input variables:      1\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.3265      0.125     0.2966     0.84\n",
      "NOTE:      1    64     0.01          0.00666          0     0.2966     0.05\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  0          0.01          0.1666     0.0625     0.88\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.1294    0.04688     0.2966     0.05\n",
      "NOTE:      1    64     0.01         0.006991          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  1          0.01         0.06817    0.02344     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01         0.007677          0     0.2966     0.05\n",
      "NOTE:      1    64     0.01         0.006171          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  2          0.01        0.006924          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.03185    0.03125     0.2966     0.05\n",
      "NOTE:      1    64     0.01         0.006843          0     0.2966     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  3          0.01         0.01935    0.01563     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01           0.0065          0     0.2966     0.05\n",
      "NOTE:      1    64     0.01          0.04113    0.01563     0.2965     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  4          0.01         0.02382   0.007813     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.09646    0.04688     0.2965     0.05\n",
      "NOTE:      1    64     0.01         0.007526          0     0.2965     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  5          0.01         0.05199    0.02344     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     0.01          0.08432     0.0625     0.2965     0.05\n",
      "NOTE:      1    64     0.01          0.03138    0.01563     0.2965     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  6          0.01         0.05785    0.03906     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.01414          0     0.2964     0.05\n",
      "NOTE:      1    64    0.001          0.00748          0     0.2964     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  7         0.001         0.01081          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.01847          0     0.2964     0.05\n",
      "NOTE:      1    64    0.001         0.006619          0     0.2964     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  8         0.001         0.01254          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.007911          0     0.2964     0.05\n",
      "NOTE:      1    64    0.001         0.007688          0     0.2964     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  9         0.001          0.0078          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.006261          0     0.2963     0.05\n",
      "NOTE:      1    64    0.001          0.00979          0     0.2963     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  10        0.001        0.008026          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.009921          0     0.2963     0.05\n",
      "NOTE:      1    64    0.001          0.00934          0     0.2963     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  11        0.001        0.009631          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001         0.008476          0     0.2963     0.05\n",
      "NOTE:      1    64    0.001          0.01096          0     0.2963     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  12        0.001        0.009716          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64    0.001          0.01692          0     0.2963     0.05\n",
      "NOTE:      1    64    0.001          0.01485          0     0.2963     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  13        0.001         0.01588          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.01129          0     0.2963     0.05\n",
      "NOTE:      1    64   0.0001          0.00496          0     0.2963     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  14       0.0001        0.008125          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.01533          0     0.2963     0.05\n",
      "NOTE:      1    64   0.0001          0.01051          0     0.2963     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  15       0.0001         0.01292          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.00807          0     0.2963     0.05\n",
      "NOTE:      1    64   0.0001          0.09595    0.04688     0.2963     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  16       0.0001         0.05201    0.02344     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001          0.02645          0     0.2963     0.05\n",
      "NOTE:      1    64   0.0001          0.01951          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  17       0.0001         0.02298          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.007551          0     0.2962     0.05\n",
      "NOTE:      1    64   0.0001          0.02055          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  18       0.0001         0.01405          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.007425          0     0.2962     0.05\n",
      "NOTE:      1    64   0.0001          0.01705          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  19       0.0001         0.01224          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64   0.0001         0.009168          0     0.2962     0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1    64   0.0001          0.01125          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  20       0.0001         0.01021          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.01308          0     0.2962     0.05\n",
      "NOTE:      1    64  0.00001          0.01133          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  21         1E-5         0.01221          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.005627          0     0.2962     0.05\n",
      "NOTE:      1    64  0.00001         0.007459          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  22         1E-5        0.006543          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.01306          0     0.2962     0.05\n",
      "NOTE:      1    64  0.00001          0.01359          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  23         1E-5         0.01333          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.006583          0     0.2962     0.05\n",
      "NOTE:      1    64  0.00001         0.009778          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  24         1E-5        0.008181          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001         0.003945          0     0.2962     0.05\n",
      "NOTE:      1    64  0.00001          0.01011          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  25         1E-5        0.007028          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.01015          0     0.2962     0.05\n",
      "NOTE:      1    64  0.00001          0.00815          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  26         1E-5        0.009152          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64  0.00001          0.00727          0     0.2962     0.05\n",
      "NOTE:      1    64  0.00001         0.008502          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  27         1E-5        0.007886          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6          0.01026          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-6         0.008784          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  28         1E-6        0.009525          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6          0.01187          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-6         0.008018          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  29         1E-6        0.009944          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.008525          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-6         0.007641          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  30         1E-6        0.008083          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6          0.01049          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-6           0.0107          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  31         1E-6          0.0106          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.007492          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-6          0.01181          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  32         1E-6        0.009651          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6           0.0165          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-6         0.006977          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  33         1E-6         0.01174          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-6         0.008887          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-6          0.01183          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  34         1E-6         0.01036          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.009942          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-7         0.007363          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  35         1E-7        0.008653          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7          0.01251          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-7         0.006001          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  36         1E-7        0.009255          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.007097          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-7         0.007166          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  37         1E-7        0.007131          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7         0.008426          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-7         0.007502          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  38         1E-7        0.007964          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7          0.01064          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-7         0.008143          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  39         1E-7        0.009394          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7          0.01066          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-7         0.006835          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  40         1E-7        0.008748          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-7          0.03679          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-7          0.01063          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  41         1E-7         0.02371          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.00849          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-8          0.01168          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  42         1E-8         0.01009          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.008648          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-8         0.005612          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  43         1E-8         0.00713          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.01006          0     0.2962     0.05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1    64     1E-8          0.00606          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  44         1E-8        0.008058          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.00788          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-8          0.01028          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  45         1E-8        0.009079          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.009063          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-8         0.008965          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  46         1E-8        0.009014          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8          0.01691          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-8          0.01521          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  47         1E-8         0.01606          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-8         0.008938          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-8         0.008237          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  48         1E-8        0.008587          0     0.09\n",
      "NOTE:  Batch nUsed Learning Rate        Loss  Fit Error     L2Norm   Time(s) (Training)\n",
      "NOTE:      0    64     1E-9         0.006039          0     0.2962     0.05\n",
      "NOTE:      1    64     1E-9         0.008475          0     0.2962     0.04\n",
      "NOTE:  Epoch Learning Rate        Loss  Fit Error   Time(s)\n",
      "NOTE:  49         1E-9        0.007257          0     0.09\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       5.37 (s).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ModelInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Model Name</td>\n",
       "      <td>model_8zia1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Model Type</td>\n",
       "      <td>Convolutional Neural Network</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Number of Layers</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Number of Input Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Number of Output Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Number of Convolutional Layers</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Number of Pooling Layers</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>Number of Fully Connected Layers</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Number of Batch Normalization Layers</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>Number of Residual Layers</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>Number of Weight Parameters</td>\n",
       "      <td>414184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>Number of Bias Parameters</td>\n",
       "      <td>1174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Total Number of Model Parameters</td>\n",
       "      <td>415358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>Approximate Memory Cost for Training (MB)</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OptIterHistory</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Epoch\">Epoch</th>\n",
       "      <th title=\"LearningRate\">LearningRate</th>\n",
       "      <th title=\"Loss\">Loss</th>\n",
       "      <th title=\"FitError\">FitError</th>\n",
       "      <th title=\"L2Norm\">L2Norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.166562</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.296609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>107</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.068174</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.296596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>108</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.006924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>109</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.019348</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.296570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>110</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.023816</td>\n",
       "      <td>0.007812</td>\n",
       "      <td>0.296549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>111</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.051994</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.296517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>112</td>\n",
       "      <td>1.000000e-02</td>\n",
       "      <td>0.057846</td>\n",
       "      <td>0.039062</td>\n",
       "      <td>0.296476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>113</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.010810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>114</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.012544</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>115</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.007800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>116</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.008026</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>117</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.009631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>118</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.009716</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>119</td>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>0.015881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>120</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.008125</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>121</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.012918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>122</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.052011</td>\n",
       "      <td>0.023438</td>\n",
       "      <td>0.296257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>123</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.022980</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>124</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.014052</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>125</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.012235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>126</td>\n",
       "      <td>1.000000e-04</td>\n",
       "      <td>0.010209</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>127</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.012207</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>128</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.006543</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>129</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.013327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>130</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>131</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>132</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.009152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>133</td>\n",
       "      <td>1.000000e-05</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>134</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.009525</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>135</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.009944</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>136</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.008083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>137</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.010595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>138</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.009651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>139</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>140</td>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>0.010358</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>141</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>142</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.009255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>143</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>144</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>145</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.009394</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>146</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.008748</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>147</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>0.023712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>148</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.010087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>149</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.007130</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>150</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.008058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>151</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.009079</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>152</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>153</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.016061</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>154</td>\n",
       "      <td>1.000000e-08</td>\n",
       "      <td>0.008587</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>155</td>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>0.007257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CASUSER(UserID)</td>\n",
       "      <td>Model_8ZIA1w_weights</td>\n",
       "      <td>416510</td>\n",
       "      <td>3</td>\n",
       "      <td>CASTable('Model_8ZIA1w_weights', caslib='CASUS...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 7.14s</span> &#183; <span class=\"cas-user\">user 5.93s</span> &#183; <span class=\"cas-sys\">sys 2.27s</span> &#183; <span class=\"cas-memory\">mem 132MB</span></small></p>"
      ],
      "text/plain": [
       "[ModelInfo]\n",
       "\n",
       "                                         Descr                         Value\n",
       " 0                                  Model Name                  model_8zia1w\n",
       " 1                                  Model Type  Convolutional Neural Network\n",
       " 2                            Number of Layers                            26\n",
       " 3                      Number of Input Layers                             1\n",
       " 4                     Number of Output Layers                             1\n",
       " 5              Number of Convolutional Layers                            10\n",
       " 6                    Number of Pooling Layers                             1\n",
       " 7            Number of Fully Connected Layers                             0\n",
       " 8        Number of Batch Normalization Layers                             9\n",
       " 9                   Number of Residual Layers                             4\n",
       " 10                Number of Weight Parameters                        414184\n",
       " 11                  Number of Bias Parameters                          1174\n",
       " 12           Total Number of Model Parameters                        415358\n",
       " 13  Approximate Memory Cost for Training (MB)                           115\n",
       "\n",
       "[OptIterHistory]\n",
       "\n",
       "     Epoch  LearningRate      Loss  FitError    L2Norm\n",
       " 0     106  1.000000e-02  0.166562  0.062500  0.296609\n",
       " 1     107  1.000000e-02  0.068174  0.023438  0.296596\n",
       " 2     108  1.000000e-02  0.006924  0.000000  0.296584\n",
       " 3     109  1.000000e-02  0.019348  0.015625  0.296570\n",
       " 4     110  1.000000e-02  0.023816  0.007812  0.296549\n",
       " 5     111  1.000000e-02  0.051994  0.023438  0.296517\n",
       " 6     112  1.000000e-02  0.057846  0.039062  0.296476\n",
       " 7     113  1.000000e-03  0.010810  0.000000  0.296429\n",
       " 8     114  1.000000e-03  0.012544  0.000000  0.296392\n",
       " 9     115  1.000000e-03  0.007800  0.000000  0.296363\n",
       " 10    116  1.000000e-03  0.008026  0.000000  0.296341\n",
       " 11    117  1.000000e-03  0.009631  0.000000  0.296322\n",
       " 12    118  1.000000e-03  0.009716  0.000000  0.296305\n",
       " 13    119  1.000000e-03  0.015881  0.000000  0.296290\n",
       " 14    120  1.000000e-04  0.008125  0.000000  0.296277\n",
       " 15    121  1.000000e-04  0.012918  0.000000  0.296266\n",
       " 16    122  1.000000e-04  0.052011  0.023438  0.296257\n",
       " 17    123  1.000000e-04  0.022980  0.000000  0.296249\n",
       " 18    124  1.000000e-04  0.014052  0.000000  0.296243\n",
       " 19    125  1.000000e-04  0.012235  0.000000  0.296238\n",
       " 20    126  1.000000e-04  0.010209  0.000000  0.296234\n",
       " 21    127  1.000000e-05  0.012207  0.000000  0.296230\n",
       " 22    128  1.000000e-05  0.006543  0.000000  0.296227\n",
       " 23    129  1.000000e-05  0.013327  0.000000  0.296225\n",
       " 24    130  1.000000e-05  0.008181  0.000000  0.296223\n",
       " 25    131  1.000000e-05  0.007028  0.000000  0.296221\n",
       " 26    132  1.000000e-05  0.009152  0.000000  0.296220\n",
       " 27    133  1.000000e-05  0.007886  0.000000  0.296219\n",
       " 28    134  1.000000e-06  0.009525  0.000000  0.296218\n",
       " 29    135  1.000000e-06  0.009944  0.000000  0.296217\n",
       " 30    136  1.000000e-06  0.008083  0.000000  0.296216\n",
       " 31    137  1.000000e-06  0.010595  0.000000  0.296216\n",
       " 32    138  1.000000e-06  0.009651  0.000000  0.296215\n",
       " 33    139  1.000000e-06  0.011738  0.000000  0.296215\n",
       " 34    140  1.000000e-06  0.010358  0.000000  0.296215\n",
       " 35    141  1.000000e-07  0.008653  0.000000  0.296215\n",
       " 36    142  1.000000e-07  0.009255  0.000000  0.296214\n",
       " 37    143  1.000000e-07  0.007131  0.000000  0.296214\n",
       " 38    144  1.000000e-07  0.007964  0.000000  0.296214\n",
       " 39    145  1.000000e-07  0.009394  0.000000  0.296214\n",
       " 40    146  1.000000e-07  0.008748  0.000000  0.296214\n",
       " 41    147  1.000000e-07  0.023712  0.000000  0.296214\n",
       " 42    148  1.000000e-08  0.010087  0.000000  0.296214\n",
       " 43    149  1.000000e-08  0.007130  0.000000  0.296214\n",
       " 44    150  1.000000e-08  0.008058  0.000000  0.296214\n",
       " 45    151  1.000000e-08  0.009079  0.000000  0.296214\n",
       " 46    152  1.000000e-08  0.009014  0.000000  0.296214\n",
       " 47    153  1.000000e-08  0.016061  0.000000  0.296214\n",
       " 48    154  1.000000e-08  0.008587  0.000000  0.296214\n",
       " 49    155  1.000000e-09  0.007257  0.000000  0.296214\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib                  Name    Rows  Columns  \\\n",
       " 0  CASUSER(UserID)  Model_8ZIA1w_weights  416510        3   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Model_8ZIA1w_weights', caslib='CASUS...  \n",
       "\n",
       "+ Elapsed: 7.14s, user: 5.93s, sys: 2.27s, mem: 132mb"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model using the custom\n",
    "# FCMP learning rate settings\n",
    "resnet_like_model.fit(data=my_images, \n",
    "                      n_threads=4, \n",
    "                      record_seed=13309, \n",
    "                      optimizer=optimizer,\n",
    "                      gpu=gpu, \n",
    "                      log_level=2\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `resnet_like_model`, when trained with a custom-written SAS FCMP function `reduce_lr_on_plateau` learning rate policy, has a final learning rate of 1.000000e-09, a loss value of 0.007257, and a fit error of 0.00.  \n",
    "\n",
    "By comparison:\n",
    "\n",
    "- The same model trained with the `ReduceLROnPlateau` learning rate policy has a final learning rate of 1.000000e-10, a loss value of 0.008770, and a fit error of 0.00. \n",
    "- The same model trained with the `CyclicLR` learning rate policy has a final learning rate of 0.005103, a loss value of 0.026172, and a fit error of 0.007812. \n",
    "- The same model trained with the `StepLR` learning rate policy has a final learning rate of 0.000001, a loss value of 1.103452, and a fit error of 0.453125.\n",
    "\n",
    "When you compare the fit and error statistics, the model that was trained using the custom-written SAS FCMP function `reduce_lr_on_plateau` learning rate policy outperforms the models that were trained using the `ReduceLROnPlateau`, `CyclicLR`, and `StepLR` learning rate policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000101s</span> &#183; <span class=\"cas-sys\">sys 9.5e-05s</span> &#183; <span class=\"cas-memory\">mem 0.204MB</span></small></p>"
      ],
      "text/plain": [
       "+ Elapsed: 0.000101s, sys: 9.5e-05s, mem: 0.204mb"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.endsession()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
