{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with Python (DLPy) and SAS Viya: Survival Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses SAS DLPy to illustrate how we can easily apply a deep learning model for survival analysis. In this employee attrition example, we'd like to predict when an employee quit his job and understand the risk factors for employee turnover. This exmaple demonstrates that the deep survival model can achieve better prediction than the popular Cox proportional hazards model.\n",
    "\n",
    "You can find the employee attrition Dataset at https://github.com/square/pysurvival/blob/master/pysurvival/datasets/employee_attrition.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "1. [Survival Analysis and Its Application](#introduction)\n",
    "2. [Employee Churn Data](#EDA)\n",
    "3. [Deep Survival Model for Employee Churn Analysis](#build_deepsurv)\n",
    "    1. [Build a Deep Learning Model with Survival Loss](#model_specification)\n",
    "    2. [Train a Deep Learning Model](#model_training)    \n",
    "4. [Evaluate Model Performance on Test Data Set](#evaluation)\n",
    "5. [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Survival Analysis, Its Applications, and Censoring <a name=\"introduction\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Survival analysis](https://en.wikipedia.org/wiki/Survival_analysis) is widely used for analyzing life time data, where the target variable is the  duration/survival time until an event of interest occurs. One of the goals in survival analysis is to predict when an event of interest is likely to happen.\n",
    "\n",
    "Survival analysis can be applied on many areas:\n",
    "\n",
    "- Medical research: predict survival time after treatment\n",
    "- Finance industry: predict when a borrower will likely to repay loan (credit risk)\n",
    "- Churn analysis: understand why and when clients unsubscribe or stop their services\n",
    "- Manufacturers: predict when an eletronic device will break\n",
    "\n",
    "What is censoring and why does it matter?\n",
    "- Censoring happens if an individual doesn't experience the event of interest during the study peroid. In this situation, the individual's survival time is censored and not fully observed. In the employee attrition example, if an employee still works in the company by the time of collecting the data, the employee's tenure time is censored.\n",
    "- Standard regression models without accounting for censoring appropriately often lead to less accurate prediction and invalid inference. Instead, survival models can account for censoring and lead to better prediction and valid inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages and launch CAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swat import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "#import pysurvival\n",
    "sys.path.append('C:\\\\Users\\\\guilin\\\\PycharmProjects\\\\python-dlpy')\n",
    "\n",
    "#import dlpy\n",
    "import dlpy\n",
    "from dlpy.layers import *\n",
    "from dlpy.model import *\n",
    "from dlpy.images import ImageTable\n",
    "from dlpy.sequential import Sequential\n",
    "from dlpy.lr_scheduler import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to CAS server\n",
    "s = CAS('dlgrd011.unx.sas.com', 11736)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Employee Attrition Data <a name=\"EDA\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the data set: (14999, 10)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>satisfaction_level</th>\n",
       "      <th>last_evaluation</th>\n",
       "      <th>number_projects</th>\n",
       "      <th>average_montly_hours</th>\n",
       "      <th>time_spend_company</th>\n",
       "      <th>work_accident</th>\n",
       "      <th>left</th>\n",
       "      <th>promotion_last_5years</th>\n",
       "      <th>department</th>\n",
       "      <th>salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.53</td>\n",
       "      <td>2</td>\n",
       "      <td>157</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>5</td>\n",
       "      <td>262</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.88</td>\n",
       "      <td>7</td>\n",
       "      <td>272</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.87</td>\n",
       "      <td>5</td>\n",
       "      <td>223</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.52</td>\n",
       "      <td>2</td>\n",
       "      <td>159</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>sales</td>\n",
       "      <td>low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   satisfaction_level  last_evaluation  number_projects  average_montly_hours  \\\n",
       "0                0.38             0.53                2                   157   \n",
       "1                0.80             0.86                5                   262   \n",
       "2                0.11             0.88                7                   272   \n",
       "3                0.72             0.87                5                   223   \n",
       "4                0.37             0.52                2                   159   \n",
       "\n",
       "   time_spend_company  work_accident  left  promotion_last_5years department  \\\n",
       "0                   3              0     1                      0      sales   \n",
       "1                   6              0     1                      0      sales   \n",
       "2                   4              0     1                      0      sales   \n",
       "3                   5              0     1                      0      sales   \n",
       "4                   3              0     1                      0      sales   \n",
       "\n",
       "   salary  \n",
       "0     low  \n",
       "1  medium  \n",
       "2  medium  \n",
       "3     low  \n",
       "4     low  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raw_dataset = pd.read_csv(\"../employee_attrition.csv\")\n",
    "df_employee = pd.read_csv(\"\\\\\\\\sashq\\\\root\\\\dept\\\\cas\\\\guilin\\\\cnn\\employee_attrition.csv\")\n",
    "print(\"Size of the data set:\", df_employee.shape)\n",
    "df_employee.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Target variable and its distribution\n",
    "- time_spend_company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## define survival time and event status variable\n",
    "time_column = 'time_spend_company' ## Time spent at the company\n",
    "df_employee[time_column].value_counts().plot(kind='pie', autopct='%1.1f%%').set_title('Survival time distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Censoring indictor variable: left\n",
    "#### 1: indicates that an employee left the company, so his/her duration time at the company is completely observed.\n",
    "#### 0: indicates that an employee is still working in the company during the time of the study, so his/her duration time at the company is incompletly observed and censored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "event_column = 'left'   ## censoring indicator whether the employee left the company   \n",
    "df_employee[event_column].value_counts().plot(kind='pie', autopct='%1.1f%%').set_title('Censoring distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In this data set, 76.2% observations have censored target values. Standard regression methods, either by ignoring censoring information or dropping those censored observations can lead to biased and less accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Input features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### satisfaction_level: employee satisfaction level by self evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "##satisfaction_level: employee satisfaction level by self evaluation\n",
    "df_employee['satisfaction_level'].plot.hist(legend=None)\n",
    "#plt.ylabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Prepare data for survival analysis\n",
    "\n",
    "#### Split data into training and test data\n",
    "- We split the dataset to train (70% samples) and test (30% samples).\n",
    "\n",
    "#### Prepare a target variable y for survival analysis\n",
    "y is obtained by combining censoring information and original target values (time_spend_company)\n",
    "-  y =   time_spend_company if left = 1 when an employee left the company\n",
    "-  y = -1*time_spend_company if left = 0 when an employee didn't leave the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table TRAIN in caslib CASUSER(guilin).\n",
      "NOTE: The table TRAIN has been created in caslib CASUSER(guilin) from binary data uploaded to Cloud Analytic Services.\n",
      "NOTE: Cloud Analytic Services made the uploaded file available as table TEST in caslib CASUSER(guilin).\n",
      "NOTE: The table TEST has been created in caslib CASUSER(guilin) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CASTable('TEST', caslib='CASUSER(guilin)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = df_employee.shape[0]\n",
    "seed = 61386\n",
    "index_train, index_test = train_test_split(range(N), test_size=0.3,random_state=seed)\n",
    "\n",
    "\n",
    "dl_train = df_employee.loc[index_train].reset_index(drop=True)\n",
    "dl_test  = df_employee.loc[index_test].reset_index(drop=True)\n",
    "\n",
    "# NOTE: create y as target variable: negative for censored observations, and postive for uncensored observations\n",
    "# y = time_spent if left=1 when an employee left the company\n",
    "# y = -1* time_spent if left=0 when an employee is still in the company\n",
    "dl_train['y'] = dl_train.apply(lambda row: (row[time_column] if row[event_column] else -row[time_column]), axis=1)\n",
    "dl_test['y']  = dl_test.apply(lambda row: (row[time_column] if row[event_column] else -row[time_column]), axis=1)\n",
    "\n",
    "\n",
    "# create id variable to identify each observation\n",
    "# dl_train['id'] = np.arange(dl_train.shape[0])\n",
    "dl_test['id'] = np.arange(dl_test.shape[0])\n",
    "\n",
    "# upload the training and test data set into CAS\n",
    "trainTbl = s.CASTable(\"train\",replace=True)\n",
    "testTbl = s.CASTable(\"test\",replace=True)\n",
    "\n",
    "CAS.upload_frame(s, dl_train,  casout=trainTbl)\n",
    "CAS.upload_frame(s, dl_test,  casout=testTbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training data: (10499, 11).  70.0% samples\n",
      "Size of test data: (4500, 12).  30.0% samples\n"
     ]
    }
   ],
   "source": [
    "# verify the data sets after split\n",
    "splitRatio = trainTbl.shape[0]/(trainTbl.shape[0]+testTbl.shape[0])\n",
    "train_string = \"Size of training data: {}.  {:.1%} samples\"\n",
    "test_string = \"Size of test data: {}.  {:.1%} samples\"  \n",
    "print(train_string.format(trainTbl.shape, splitRatio)) \n",
    "  \n",
    "print(test_string.format(testTbl.shape, 1-splitRatio)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Survival Model for Employee Attrition Analysis <a name=\"build_deepsurv\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify inputs and target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare input and target variables\n",
      "Number of input varibles: 8\n",
      "\n",
      "Input variables:\n",
      " ['number_projects', 'department', 'salary', 'satisfaction_level', 'last_evaluation', 'promotion_last_5years', 'average_montly_hours', 'work_accident']\n",
      "\n",
      "Nominal inputs: ['department', 'salary']\n",
      "\n",
      "Target variable: y\n"
     ]
    }
   ],
   "source": [
    "print(\"Prepare input and target variables\")\n",
    "inputs = set(trainTbl.columninfo()[\"ColumnInfo\"][\"Column\"]) ## set \n",
    "# a list of input variables are obtained by excluding target and censoring variables\n",
    "inputs = list(inputs -{'y',time_column,event_column}) \n",
    "\n",
    "print(\"Number of input varibles:\", len(inputs))\n",
    "print(\"\\nInput variables:\\n\", inputs)\n",
    "nominals = [\"department\",\"salary\"]\n",
    "print(\"\\nNominal inputs:\", nominals)\n",
    "# y is the target varaible instead of \"time_spend_company\"\n",
    "print(\"\\nTarget variable:\",\"y\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Build a deep survival model<a name=\"model_specification\"></a>\n",
    "- The [deep survival model](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1) is a deep learning  model with the survival loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input vars: ['number_projects', 'department', 'salary', 'satisfaction_level', 'last_evaluation', 'promotion_last_5years', 'average_montly_hours', 'work_accident']\n",
      "Number of inputs: 8\n",
      "Number of hidden units: 13\n",
      "NOTE: Input layer added.\n",
      "NOTE: Fully-connected layer added.\n",
      "NOTE: Batch normalization layer added.\n",
      "NOTE: Fully-connected layer added.\n",
      "NOTE: Batch normalization layer added.\n",
      "NOTE: Fully-connected layer added.\n",
      "NOTE: Survival layer added.\n",
      "NOTE: Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "modelName='deepsurv_model';\n",
    "\n",
    "inputVars = inputs\n",
    "print(\"Input vars:\",inputVars)\n",
    "nInput= len(inputVars)\n",
    "print(\"Number of inputs:\",nInput)\n",
    "nHidden = nInput + 5\n",
    "print(\"Number of hidden units:\",nHidden)\n",
    "deepSurvModel = Sequential(s, model_table=modelName)\n",
    "deepSurvModel.add(InputLayer(std='STD'))\n",
    "deepSurvModel.add(Dense(n=nHidden, act='identity', include_bias=False))\n",
    "deepSurvModel.add(BN(act='relu'))\n",
    "deepSurvModel.add(Dense(n=nHidden, act='identity', include_bias=False))\n",
    "deepSurvModel.add(BN(act='relu'))\n",
    "# NOTE: the last dense layer should only have one neuron, and identity activation function is used. \n",
    "deepSurvModel.add(Dense(n=1, act='identity'))\n",
    "# NOTE: specify the task layer (the last layer) as Survival layer to obtain the survival loss function\n",
    "deepSurvModel.add(Survival())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; TableInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Rows\">Rows</th>\n",
       "      <th title=\"Columns\">Columns</th>\n",
       "      <th title=\"Indexed Columns\">IndexedColumns</th>\n",
       "      <th title=\"Encoding\">Encoding</th>\n",
       "      <th title=\"Created\">CreateTimeFormatted</th>\n",
       "      <th title=\"Last Modified\">ModTimeFormatted</th>\n",
       "      <th title=\"Last Accessed\">AccessTimeFormatted</th>\n",
       "      <th title=\"Character Set\">JavaCharSet</th>\n",
       "      <th title=\"CreateTime\">CreateTime</th>\n",
       "      <th title=\"...\">...</th>\n",
       "      <th title=\"Repeated\">Repeated</th>\n",
       "      <th title=\"View\">View</th>\n",
       "      <th title=\"MultiPart\">MultiPart</th>\n",
       "      <th title=\"Loaded Source\">SourceName</th>\n",
       "      <th title=\"Source Caslib\">SourceCaslib</th>\n",
       "      <th title=\"Compressed\">Compressed</th>\n",
       "      <th title=\"Table Creator\">Creator</th>\n",
       "      <th title=\"Last Table Modifier\">Modifier</th>\n",
       "      <th title=\"Source Modified\">SourceModTimeFormatted</th>\n",
       "      <th title=\"SourceModTime\">SourceModTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>10499</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>2020-02-27T15:09:31-05:00</td>\n",
       "      <td>2020-02-27T15:09:31-05:00</td>\n",
       "      <td>2020-02-27T15:09:34-05:00</td>\n",
       "      <td>UTF8</td>\n",
       "      <td>1.898453e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>guilin</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-27T15:09:31-05:00</td>\n",
       "      <td>1.898453e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>TEST</td>\n",
       "      <td>4500</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>2020-02-27T15:09:31-05:00</td>\n",
       "      <td>2020-02-27T15:09:31-05:00</td>\n",
       "      <td>2020-02-27T15:09:34-05:00</td>\n",
       "      <td>UTF8</td>\n",
       "      <td>1.898453e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>guilin</td>\n",
       "      <td></td>\n",
       "      <td>2020-02-27T15:09:31-05:00</td>\n",
       "      <td>1.898453e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>DEEPSURV_MODEL</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>utf-8</td>\n",
       "      <td>2020-02-27T15:09:39-05:00</td>\n",
       "      <td>2020-02-27T15:09:39-05:00</td>\n",
       "      <td>2020-02-27T15:09:39-05:00</td>\n",
       "      <td>UTF8</td>\n",
       "      <td>1.898453e+09</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>guilin</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows  23 columns</p>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.000465s</span> &#183; <span class=\"cas-user\">user 0.000195s</span> &#183; <span class=\"cas-sys\">sys 0.000226s</span> &#183; <span class=\"cas-memory\">mem 0.686MB</span></small></p>"
      ],
      "text/plain": [
       "[TableInfo]\n",
       "\n",
       "              Name   Rows  Columns  IndexedColumns Encoding  \\\n",
       " 0           TRAIN  10499       11               0    utf-8   \n",
       " 1            TEST   4500       12               0    utf-8   \n",
       " 2  DEEPSURV_MODEL     53        5               0    utf-8   \n",
       " \n",
       "          CreateTimeFormatted           ModTimeFormatted  \\\n",
       " 0  2020-02-27T15:09:31-05:00  2020-02-27T15:09:31-05:00   \n",
       " 1  2020-02-27T15:09:31-05:00  2020-02-27T15:09:31-05:00   \n",
       " 2  2020-02-27T15:09:39-05:00  2020-02-27T15:09:39-05:00   \n",
       " \n",
       "          AccessTimeFormatted JavaCharSet    CreateTime  ...  Repeated  View  \\\n",
       " 0  2020-02-27T15:09:34-05:00        UTF8  1.898453e+09  ...         0     0   \n",
       " 1  2020-02-27T15:09:34-05:00        UTF8  1.898453e+09  ...         0     0   \n",
       " 2  2020-02-27T15:09:39-05:00        UTF8  1.898453e+09  ...         0     0   \n",
       " \n",
       "    MultiPart  SourceName  SourceCaslib  Compressed Creator Modifier  \\\n",
       " 0          0                                     0  guilin            \n",
       " 1          0                                     0  guilin            \n",
       " 2          0                                     0  guilin            \n",
       " \n",
       "       SourceModTimeFormatted SourceModTime  \n",
       " 0  2020-02-27T15:09:31-05:00  1.898453e+09  \n",
       " 1  2020-02-27T15:09:31-05:00  1.898453e+09  \n",
       " 2                                      NaN  \n",
       " \n",
       " [3 rows x 23 columns]\n",
       "\n",
       "+ Elapsed: 0.000465s, user: 0.000195s, sys: 0.000226s, mem: 0.686mb"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check available CAS tables\n",
    "s.tableinfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot deep survival model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: deepsurv_model Pages: 1 -->\r\n",
       "<svg width=\"168pt\" height=\"451pt\"\r\n",
       " viewBox=\"0.00 0.00 168.00 451.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 447)\">\r\n",
       "<title>deepsurv_model</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-447 164,-447 164,4 -4,4\"/>\r\n",
       "<!-- Input1 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>Input1</title>\r\n",
       "<polygon fill=\"#3288bd\" fill-opacity=\"0.250980\" stroke=\"#3288bd\" points=\"26.5,-420.5 26.5,-442.5 133.5,-442.5 133.5,-420.5 26.5,-420.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"80\" y=\"-427.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Input1(input)</text>\r\n",
       "</g>\r\n",
       "<!-- F.C.1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>F.C.1</title>\r\n",
       "<polygon fill=\"#ffffbf\" fill-opacity=\"0.250980\" stroke=\"#aeae82\" points=\"20,-350.5 20,-372.5 140,-372.5 140,-350.5 20,-350.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"80\" y=\"-357.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">0x13 F.C.1(fc)</text>\r\n",
       "</g>\r\n",
       "<!-- Input1&#45;&gt;F.C.1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>Input1&#45;&gt;F.C.1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M80,-420.466C80,-410.623 80,-395.327 80,-382.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"83.5001,-382.575 80,-372.575 76.5001,-382.575 83.5001,-382.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"86\" y=\"-394\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 0 </text>\r\n",
       "</g>\r\n",
       "<!-- B.N.1 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>B.N.1</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"0,-280.5 0,-302.5 160,-302.5 160,-280.5 0,-280.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"80\" y=\"-287.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">13 B.N.1(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- F.C.1&#45;&gt;B.N.1 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>F.C.1&#45;&gt;B.N.1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M80,-350.466C80,-340.623 80,-325.327 80,-312.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"83.5001,-312.575 80,-302.575 76.5001,-312.575 83.5001,-312.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"88.5\" y=\"-324\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 13 </text>\r\n",
       "</g>\r\n",
       "<!-- F.C.2 -->\r\n",
       "<g id=\"node4\" class=\"node\"><title>F.C.2</title>\r\n",
       "<polygon fill=\"#ffffbf\" fill-opacity=\"0.250980\" stroke=\"#aeae82\" points=\"16,-210.5 16,-232.5 144,-232.5 144,-210.5 16,-210.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"80\" y=\"-217.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">13x13 F.C.2(fc)</text>\r\n",
       "</g>\r\n",
       "<!-- B.N.1&#45;&gt;F.C.2 -->\r\n",
       "<g id=\"edge3\" class=\"edge\"><title>B.N.1&#45;&gt;F.C.2</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M80,-280.466C80,-270.623 80,-255.327 80,-242.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"83.5001,-242.575 80,-232.575 76.5001,-242.575 83.5001,-242.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"88.5\" y=\"-254\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 13 </text>\r\n",
       "</g>\r\n",
       "<!-- B.N.2 -->\r\n",
       "<g id=\"node5\" class=\"node\"><title>B.N.2</title>\r\n",
       "<polygon fill=\"#fdae61\" fill-opacity=\"0.250980\" stroke=\"#fdae61\" points=\"0,-140.5 0,-162.5 160,-162.5 160,-140.5 0,-140.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"80\" y=\"-147.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">13 B.N.2(batchnorm)</text>\r\n",
       "</g>\r\n",
       "<!-- F.C.2&#45;&gt;B.N.2 -->\r\n",
       "<g id=\"edge4\" class=\"edge\"><title>F.C.2&#45;&gt;B.N.2</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M80,-210.466C80,-200.623 80,-185.327 80,-172.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"83.5001,-172.575 80,-162.575 76.5001,-172.575 83.5001,-172.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"88.5\" y=\"-184\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 13 </text>\r\n",
       "</g>\r\n",
       "<!-- F.C.3 -->\r\n",
       "<g id=\"node6\" class=\"node\"><title>F.C.3</title>\r\n",
       "<polygon fill=\"#ffffbf\" fill-opacity=\"0.250980\" stroke=\"#aeae82\" points=\"20,-70.5 20,-92.5 140,-92.5 140,-70.5 20,-70.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"80\" y=\"-77.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">13x1 F.C.3(fc)</text>\r\n",
       "</g>\r\n",
       "<!-- B.N.2&#45;&gt;F.C.3 -->\r\n",
       "<g id=\"edge5\" class=\"edge\"><title>B.N.2&#45;&gt;F.C.3</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M80,-140.466C80,-130.623 80,-115.327 80,-102.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"83.5001,-102.575 80,-92.5748 76.5001,-102.575 83.5001,-102.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"88.5\" y=\"-114\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 13 </text>\r\n",
       "</g>\r\n",
       "<!-- Survival1 -->\r\n",
       "<g id=\"node7\" class=\"node\"><title>Survival1</title>\r\n",
       "<polygon fill=\"#9e0142\" fill-opacity=\"0.250980\" stroke=\"#9e0142\" points=\"3.5,-0.5 3.5,-22.5 156.5,-22.5 156.5,-0.5 3.5,-0.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"80\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">1 Survival1(survival)</text>\r\n",
       "</g>\r\n",
       "<!-- F.C.3&#45;&gt;Survival1 -->\r\n",
       "<g id=\"edge6\" class=\"edge\"><title>F.C.3&#45;&gt;Survival1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M80,-70.4664C80,-60.6231 80,-45.327 80,-32.9189\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"83.5001,-32.5748 80,-22.5748 76.5001,-32.5748 83.5001,-32.5748\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"86\" y=\"-44\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 1 </text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x28b6658d9c8>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepSurvModel.plot_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Id</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Type</th>\n",
       "      <th>Kernel Size</th>\n",
       "      <th>Stride</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Output Size</th>\n",
       "      <th>Number of Parameters</th>\n",
       "      <th>FLOPS(forward pass)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Input1</td>\n",
       "      <td>input</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>F.C.1</td>\n",
       "      <td>fc</td>\n",
       "      <td>(0, 13)</td>\n",
       "      <td></td>\n",
       "      <td>Identity</td>\n",
       "      <td>13</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>B.N.1</td>\n",
       "      <td>batchnorm</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Relu</td>\n",
       "      <td>13</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>F.C.2</td>\n",
       "      <td>fc</td>\n",
       "      <td>(13, 13)</td>\n",
       "      <td></td>\n",
       "      <td>Identity</td>\n",
       "      <td>13</td>\n",
       "      <td>(169, 0)</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>B.N.2</td>\n",
       "      <td>batchnorm</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Relu</td>\n",
       "      <td>13</td>\n",
       "      <td>(0, 2)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>F.C.3</td>\n",
       "      <td>fc</td>\n",
       "      <td>(13, 1)</td>\n",
       "      <td></td>\n",
       "      <td>Identity</td>\n",
       "      <td>1</td>\n",
       "      <td>(13, 0)</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Survival1</td>\n",
       "      <td>survival</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Total number of parameters</td>\n",
       "      <td>Total FLOPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Summary</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>186</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Layer Id      Layer       Type Kernel Size Stride Activation Output Size  \\\n",
       "0        0     Input1      input                          None           0   \n",
       "1        1      F.C.1         fc     (0, 13)          Identity          13   \n",
       "2        2      B.N.1  batchnorm                          Relu          13   \n",
       "3        3      F.C.2         fc    (13, 13)          Identity          13   \n",
       "4        4      B.N.2  batchnorm                          Relu          13   \n",
       "5        5      F.C.3         fc     (13, 1)          Identity           1   \n",
       "6        6  Survival1   survival                          None           1   \n",
       "7                                                                            \n",
       "8  Summary                                                                   \n",
       "\n",
       "         Number of Parameters FLOPS(forward pass)  \n",
       "0                      (0, 0)                   0  \n",
       "1                      (0, 0)                   0  \n",
       "2                      (0, 2)                   0  \n",
       "3                    (169, 0)                 169  \n",
       "4                      (0, 2)                   0  \n",
       "5                     (13, 0)                  13  \n",
       "6                      (0, 0)                   0  \n",
       "7  Total number of parameters         Total FLOPS  \n",
       "8                         186                 182  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deepSurvModel.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train deep survival model <a name=\"model_training\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify  the momentum SGD optimizer and a learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following argument(s) learning_rate, gamma, step_size, power are overwritten by the according arguments specified in lr_scheduler.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = ReduceLROnPlateau(conn=s, cool_down_iters=3, gamma=0.8, learning_rate=0.001, patience=5)\n",
    "solver = MomentumSolver(lr_scheduler = lr_scheduler,\n",
    "                        momentum=0.9,\n",
    "                        clip_grad_max = 100, clip_grad_min = -100)\n",
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=400, log_level=4, max_epochs=100, reg_l2=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Train deep survival model with momentum SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepsurv_model\n",
      "NOTE: Training from scratch.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 482.\n",
      "NOTE:  The approximate memory cost is 1.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       0.01 (s).\n",
      "NOTE:  The total number of threads on each worker is 2.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 400.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 800.\n",
      "NOTE:  Target variable: y\n",
      "NOTE:  Number of input variables:     8\n",
      "NOTE:  Number of nominal input variables:      2\n",
      "NOTE:  Number of numeric input variables:      6\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.343     0.4764  3.573e+04  3.927e+04          0    0.02596     0.00\n",
      "NOTE:      1   800    0.001            1.096     0.4864  3.025e+04  3.195e+04          0    0.02595     0.00\n",
      "NOTE:      2   800    0.001            1.058      0.497  2.976e+04  3.012e+04          0    0.02595     0.00\n",
      "NOTE:      3   800    0.001            1.127      0.496  3.114e+04  3.165e+04          0    0.02595     0.00\n",
      "NOTE:      4   800    0.001            1.145     0.5223  3.359e+04  3.072e+04          0    0.02595     0.00\n",
      "NOTE:      5   800    0.001            1.253     0.4697  3.288e+04  3.712e+04          0    0.02595     0.00\n",
      "NOTE:      6   800    0.001            1.234     0.4895  3.414e+04  3.561e+04          0    0.02595     0.00\n",
      "NOTE:      7   800    0.001            1.165     0.4801  3.172e+04  3.435e+04          0    0.02595     0.00\n",
      "NOTE:      8   800    0.001            1.279     0.4731  3.284e+04  3.656e+04          0    0.02595     0.00\n",
      "NOTE:      9   800    0.001           0.9728     0.5381  3.058e+04  2.625e+04          0    0.02595     0.00\n",
      "NOTE:     10   800    0.001            1.277     0.5058  3.561e+04  3.479e+04          0    0.02595     0.00\n",
      "NOTE:     11   800    0.001            1.074      0.491  2.973e+04  3.082e+04          0    0.02594     0.00\n",
      "NOTE:     12   800    0.001            1.075     0.4822  2.899e+04  3.114e+04          0    0.02594     0.00\n",
      "NOTE:     13   800    0.001            1.197     0.5149  3.469e+04  3.267e+04          0    0.02594     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  0         0.001           1.164     0.4938  4.516e+05   4.63e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.156     0.5106  3.298e+04   3.16e+04          0    0.02594     0.00\n",
      "NOTE:      1   800    0.001            1.267     0.5756  4.073e+04  3.004e+04          0    0.02594     0.00\n",
      "NOTE:      2   800    0.001            1.095     0.5592  3.271e+04  2.579e+04          0    0.02594     0.00\n",
      "NOTE:      3   800    0.001            1.077     0.5398  3.255e+04  2.775e+04          0    0.02593     0.00\n",
      "NOTE:      4   800    0.001            1.076     0.5332  3.263e+04  2.856e+04          0    0.02593     0.00\n",
      "NOTE:      5   800    0.001            1.171     0.5504  3.572e+04  2.918e+04          0    0.02593     0.00\n",
      "NOTE:      6   800    0.001            1.167     0.5578  3.732e+04  2.958e+04          0    0.02593     0.00\n",
      "NOTE:      7   800    0.001            1.177     0.5664  3.732e+04  2.856e+04          0    0.02593     0.00\n",
      "NOTE:      8   800    0.001            1.102     0.5425   3.53e+04  2.976e+04          0    0.02593     0.00\n",
      "NOTE:      9   800    0.001             1.11     0.5807  3.583e+04  2.587e+04          0    0.02592     0.00\n",
      "NOTE:     10   800    0.001            1.169     0.5794  3.951e+04  2.868e+04          0    0.02592     0.00\n",
      "NOTE:     11   800    0.001           0.9956     0.5862  3.193e+04  2.253e+04          0    0.02592     0.00\n",
      "NOTE:     12   800    0.001            1.293     0.5675  4.365e+04  3.327e+04          0    0.02592     0.00\n",
      "NOTE:     13   800    0.001            1.046     0.5593  3.364e+04   2.65e+04          0    0.02592     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  1         0.001           1.136     0.5579  5.018e+05  3.977e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.009      0.618  3.734e+04  2.308e+04          0    0.02592     0.00\n",
      "NOTE:      1   800    0.001             1.15     0.6233  4.225e+04  2.554e+04          0    0.02592     0.00\n",
      "NOTE:      2   800    0.001            1.053     0.5802  3.286e+04  2.378e+04          0    0.02591     0.00\n",
      "NOTE:      3   800    0.001            1.156     0.6455  4.072e+04  2.236e+04          0    0.02591     0.00\n",
      "NOTE:      4   800    0.001            1.057      0.639  3.985e+04  2.251e+04          0    0.02591     0.00\n",
      "NOTE:      5   800    0.001            1.146      0.646  4.292e+04  2.352e+04          0    0.02591     0.00\n",
      "NOTE:      6   800    0.001            1.047     0.6987   4.42e+04  1.906e+04          0    0.02591     0.00\n",
      "NOTE:      7   800    0.001            1.017     0.6441  3.676e+04  2.032e+04          0    0.02591     0.00\n",
      "NOTE:      8   800    0.001            1.165     0.6109  4.124e+04  2.627e+04          0    0.02591     0.00\n",
      "NOTE:      9   800    0.001             1.19     0.6456  4.417e+04  2.424e+04          0     0.0259     0.00\n",
      "NOTE:     10   800    0.001           0.9153     0.6529  3.594e+04  1.911e+04          0     0.0259     0.00\n",
      "NOTE:     11   800    0.001           0.9996     0.6928  4.031e+04  1.787e+04          0     0.0259     0.00\n",
      "NOTE:     12   800    0.001            1.093     0.6877  4.329e+04  1.966e+04          0     0.0259     0.00\n",
      "NOTE:     13   800    0.001           0.9931     0.6668  3.708e+04  1.853e+04          0     0.0259     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  2         0.001           1.071     0.6463  5.589e+05  3.058e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.208       0.69  4.564e+04   2.05e+04          0     0.0259     0.00\n",
      "NOTE:      1   800    0.001           0.9975     0.6923  4.115e+04  1.829e+04          0     0.0259     0.00\n",
      "NOTE:      2   800    0.001           0.9759     0.6548  3.488e+04  1.839e+04          0     0.0259     0.00\n",
      "NOTE:      3   800    0.001            1.079      0.693  4.458e+04  1.975e+04          0     0.0259     0.00\n",
      "NOTE:      4   800    0.001            1.112     0.6904   4.23e+04  1.897e+04          0     0.0259     0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The table deepsurv_model_weights could not be located in caslib CASUSER(guilin) of Cloud Analytic Services.\n",
      "ERROR: The action stopped due to errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      5   800    0.001           0.9938      0.719   4.32e+04  1.688e+04          0    0.02589     0.00\n",
      "NOTE:      6   800    0.001            1.152     0.7106  4.819e+04  1.963e+04          0    0.02589     0.00\n",
      "NOTE:      7   800    0.001            1.009      0.725  4.188e+04  1.589e+04          0    0.02589     0.00\n",
      "NOTE:      8   800    0.001            1.054     0.7134  4.579e+04  1.839e+04          0    0.02589     0.00\n",
      "NOTE:      9   800    0.001            1.136     0.7524  5.273e+04  1.735e+04          0    0.02589     0.00\n",
      "NOTE:     10   800    0.001             1.06     0.7524  4.831e+04   1.59e+04          0    0.02589     0.00\n",
      "NOTE:     11   800    0.001             1.02     0.7163  4.086e+04  1.618e+04          0    0.02589     0.00\n",
      "NOTE:     12   800    0.001            1.028     0.7254  4.579e+04  1.733e+04          0    0.02589     0.00\n",
      "NOTE:     13   800    0.001            1.077     0.6886  4.507e+04  2.038e+04          0    0.02589     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  3         0.001           1.064     0.7096  6.204e+05  2.538e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.251     0.7294  5.491e+04  2.037e+04          0    0.02589     0.00\n",
      "NOTE:      1   800    0.001           0.9831     0.7446  4.326e+04  1.484e+04          0    0.02589     0.00\n",
      "NOTE:      2   800    0.001           0.9254     0.7853  4.433e+04  1.212e+04          0    0.02589     0.00\n",
      "NOTE:      3   800    0.001                1     0.7416  4.518e+04  1.575e+04          0    0.02589     0.00\n",
      "NOTE:      4   800    0.001            1.087     0.7772  5.297e+04  1.519e+04          0    0.02589     0.00\n",
      "NOTE:      5   800    0.001            1.163     0.7406  5.546e+04  1.942e+04          0    0.02589     0.00\n",
      "NOTE:      6   800    0.001            1.056     0.7891  4.515e+04  1.207e+04          0    0.02589     0.00\n",
      "NOTE:      7   800    0.001            1.076     0.7673  4.844e+04  1.469e+04          0    0.02589     0.00\n",
      "NOTE:      8   800    0.001              1.1     0.7382  4.902e+04  1.739e+04          0    0.02588     0.00\n",
      "NOTE:      9   800    0.001            1.086     0.7225   4.54e+04  1.744e+04          0    0.02588     0.00\n",
      "NOTE:     10   800    0.001            1.072     0.7506  4.827e+04  1.604e+04          0    0.02588     0.00\n",
      "NOTE:     11   800    0.001           0.9496     0.7988  4.768e+04  1.201e+04          0    0.02588     0.00\n",
      "NOTE:     12   800    0.001            1.062     0.7396   4.73e+04  1.666e+04          0    0.02588     0.00\n",
      "NOTE:     13   800    0.001            1.157     0.7852  5.572e+04  1.524e+04          0    0.02588     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  4         0.001           1.069      0.757  6.831e+05  2.192e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.121     0.7492   4.73e+04  1.583e+04          0    0.02588     0.00\n",
      "NOTE:      1   800    0.001            1.123     0.7298  4.843e+04  1.793e+04          0    0.02588     0.00\n",
      "NOTE:      2   800    0.001            1.057     0.7628  4.974e+04  1.546e+04          0    0.02588     0.00\n",
      "NOTE:      3   800    0.001            1.109     0.7785  5.385e+04  1.532e+04          0    0.02588     0.00\n",
      "NOTE:      4   800    0.001            1.021     0.8231  5.089e+04  1.094e+04          0    0.02588     0.00\n",
      "NOTE:      5   800    0.001                1     0.7862  4.853e+04   1.32e+04          0    0.02588     0.00\n",
      "NOTE:      6   800    0.001            1.061        0.8  5.274e+04  1.318e+04          0    0.02588     0.00\n",
      "NOTE:      7   800    0.001            1.129     0.7766  5.521e+04  1.588e+04          0    0.02588     0.00\n",
      "NOTE:      8   800    0.001           0.9123     0.8346  4.652e+04       9223          0    0.02588     0.00\n",
      "NOTE:      9   800    0.001            1.015     0.8036  4.762e+04  1.164e+04          0    0.02588     0.00\n",
      "NOTE:     10   800    0.001            1.053     0.7537  4.633e+04  1.514e+04          0    0.02588     0.00\n",
      "NOTE:     11   800    0.001           0.9504      0.824  4.667e+04       9967          0    0.02588     0.00\n",
      "NOTE:     12   800    0.001            1.022     0.8275  5.328e+04  1.111e+04          0    0.02588     0.00\n",
      "NOTE:     13   800    0.001           0.9597     0.7796  4.821e+04  1.363e+04          0    0.02588     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  5         0.001           1.038     0.7868  6.953e+05  1.885e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9798     0.8353  5.045e+04       9948          0    0.02588     0.00\n",
      "NOTE:      1   800    0.001            1.135     0.8133  5.913e+04  1.358e+04          0    0.02588     0.00\n",
      "NOTE:      2   800    0.001            1.176     0.8056  5.906e+04  1.425e+04          0    0.02588     0.00\n",
      "NOTE:      3   800    0.001           0.9505     0.8174  4.931e+04  1.102e+04          0    0.02588     0.00\n",
      "NOTE:      4   800    0.001           0.9753     0.8224  4.842e+04  1.045e+04          1    0.02588     0.00\n",
      "NOTE:      5   800    0.001           0.9832     0.8191  4.565e+04  1.008e+04          0    0.02588     0.00\n",
      "NOTE:      6   800    0.001            1.083     0.8131  5.337e+04  1.227e+04          0    0.02588     0.00\n",
      "NOTE:      7   800    0.001           0.9873     0.7745  4.572e+04  1.331e+04          0    0.02588     0.00\n",
      "NOTE:      8   800    0.001             1.07     0.8274  5.558e+04  1.159e+04          0    0.02588     0.00\n",
      "NOTE:      9   800    0.001            1.035     0.8216  5.486e+04  1.191e+04          0    0.02588     0.00\n",
      "NOTE:     10   800    0.001            1.028     0.8386  5.166e+04       9944          0    0.02588     0.00\n",
      "NOTE:     11   800    0.001           0.9565     0.8182  5.099e+04  1.133e+04          0    0.02588     0.00\n",
      "NOTE:     12   800    0.001            1.054     0.8466   5.46e+04       9891          0    0.02588     0.00\n",
      "NOTE:     13   800    0.001            1.113     0.8159   5.81e+04  1.311e+04          0    0.02588     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  6         0.001           1.038     0.8192  7.369e+05  1.627e+05          1     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9518     0.8598  5.143e+04       8389          0    0.02588     0.00\n",
      "NOTE:      1   800    0.001           0.9863     0.8295  5.207e+04   1.07e+04          0    0.02588     0.00\n",
      "NOTE:      2   800    0.001            1.039     0.7989  5.136e+04  1.293e+04          0    0.02588     0.00\n",
      "NOTE:      3   800    0.001            1.076     0.8445  5.978e+04  1.101e+04          0    0.02588     0.00\n",
      "NOTE:      4   800    0.001            0.949     0.8482  5.053e+04       9046          0    0.02588     0.00\n",
      "NOTE:      5   800    0.001            1.005     0.8264    4.8e+04  1.009e+04          0    0.02588     0.00\n",
      "NOTE:      6   800    0.001            1.045     0.8437  5.317e+04       9853          0    0.02588     0.00\n",
      "NOTE:      7   800    0.001            1.045     0.8585  5.651e+04       9314          0    0.02588     0.00\n",
      "NOTE:      8   800    0.001           0.9894     0.8721  5.381e+04       7891          0    0.02588     0.00\n",
      "NOTE:      9   800    0.001            1.025      0.806  4.849e+04  1.167e+04          0    0.02588     0.00\n",
      "NOTE:     10   800    0.001           0.9721     0.8361  4.929e+04       9659          0    0.02588     0.00\n",
      "NOTE:     11   800    0.001            1.004     0.8518  5.563e+04       9679          0    0.02588     0.00\n",
      "NOTE:     12   800    0.001            0.978      0.848  5.078e+04       9099          0    0.02588     0.00\n",
      "NOTE:     13   800    0.001           0.9983     0.8623  5.692e+04       9092          0    0.02588     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  7         0.001           1.005      0.842  7.378e+05  1.384e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.003      0.848  5.137e+04       9210          0    0.02588     0.00\n",
      "NOTE:      1   800    0.001            1.013     0.8654  5.306e+04       8255          0    0.02588     0.00\n",
      "NOTE:      2   800    0.001            1.054     0.8481  5.453e+04       9768          0    0.02588     0.00\n",
      "NOTE:      3   800    0.001           0.9391     0.8278  4.933e+04  1.026e+04          0    0.02588     0.00\n",
      "NOTE:      4   800    0.001            1.012     0.8838  5.815e+04       7645          0    0.02588     0.00\n",
      "NOTE:      5   800    0.001            1.152      0.845  6.201e+04  1.138e+04          0    0.02588     0.00\n",
      "NOTE:      6   800    0.001            1.003     0.8606   5.41e+04       8765          0    0.02588     0.00\n",
      "NOTE:      7   800    0.001            1.032     0.8372  5.441e+04  1.058e+04          0    0.02588     0.00\n",
      "NOTE:      8   800    0.001            0.987     0.8768  5.463e+04       7673          0    0.02588     0.00\n",
      "NOTE:      9   800    0.001           0.9897     0.8872  5.598e+04       7119          0    0.02588     0.00\n",
      "NOTE:     10   800    0.001           0.9505     0.8561  4.989e+04       8383          0    0.02588     0.00\n",
      "NOTE:     11   800    0.001           0.9308     0.8827  5.129e+04       6816          0    0.02588     0.00\n",
      "NOTE:     12   800    0.001            1.069      0.879  6.147e+04       8463          0    0.02588     0.00\n",
      "NOTE:     13   800    0.001            1.038     0.8399  5.742e+04  1.095e+04          0    0.02588     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  8         0.001           1.012     0.8597  7.676e+05  1.253e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9901     0.8618  5.592e+04       8964          0    0.02588     0.00\n",
      "NOTE:      1   800    0.001            1.042     0.8704  5.632e+04       8386          0    0.02588     0.00\n",
      "NOTE:      2   800    0.001            1.143     0.8802  6.511e+04       8863          0    0.02588     0.00\n",
      "NOTE:      3   800    0.001           0.9961     0.8922  5.877e+04       7103          0    0.02589     0.00\n",
      "NOTE:      4   800    0.001           0.9782     0.8936   5.45e+04       6491          0    0.02589     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      5   800    0.001                1     0.8579  5.346e+04       8853          0    0.02589     0.00\n",
      "NOTE:      6   800    0.001           0.9473       0.89  5.583e+04       6898          0    0.02589     0.00\n",
      "NOTE:      7   800    0.001            1.104     0.8866  6.166e+04       7891          0    0.02589     0.00\n",
      "NOTE:      8   800    0.001                1     0.8809  5.722e+04       7736          0    0.02589     0.00\n",
      "NOTE:      9   800    0.001            1.016     0.8769  5.781e+04       8116          0    0.02589     0.00\n",
      "NOTE:     10   800    0.001           0.9893     0.8749  5.444e+04       7786          0    0.02589     0.00\n",
      "NOTE:     11   800    0.001            1.119     0.8582  5.999e+04       9911          0    0.02589     0.00\n",
      "NOTE:     12   800    0.001            0.996     0.8653  5.232e+04       8143          0    0.02589     0.00\n",
      "NOTE:     13   800    0.001           0.9812     0.8874  5.567e+04       7062          0    0.02589     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  9         0.001           1.022     0.8769   7.99e+05  1.122e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.068     0.8785  6.099e+04       8434          0    0.02589     0.00\n",
      "NOTE:      1   800    0.001           0.9545      0.851  5.252e+04       9193          0    0.02589     0.00\n",
      "NOTE:      2   800    0.001            1.163     0.8815  6.581e+04       8848          0    0.02589     0.00\n",
      "NOTE:      3   800    0.001            1.026     0.8884   5.77e+04       7247          0    0.02589     0.00\n",
      "NOTE:      4   800    0.001           0.8906     0.8637  4.925e+04       7773          0    0.02589     0.00\n",
      "NOTE:      5   800    0.001            1.035     0.8887  5.594e+04       7006          0    0.02589     0.00\n",
      "NOTE:      6   800    0.001            1.069     0.8498  5.809e+04  1.027e+04          0    0.02589     0.00\n",
      "NOTE:      7   800    0.001            1.034      0.894  6.096e+04       7228          0    0.02589     0.00\n",
      "NOTE:      8   800    0.001           0.9552     0.8762  5.531e+04       7814          0    0.02589     0.00\n",
      "NOTE:      9   800    0.001           0.9545     0.8932  5.614e+04       6710          0    0.02589     0.00\n",
      "NOTE:     10   800    0.001            1.019      0.909  5.808e+04       5812          0    0.02589     0.00\n",
      "NOTE:     11   800    0.001           0.9976      0.865  5.492e+04       8574          0    0.02589     0.00\n",
      "NOTE:     12   800    0.001           0.9164     0.9026  5.228e+04       5644          0     0.0259     0.00\n",
      "NOTE:     13   800    0.001            1.072     0.8939  5.879e+04       6975          0     0.0259     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  10        0.001           1.011     0.8811  7.968e+05  1.075e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.018     0.8786  5.786e+04       7996          0     0.0259     0.00\n",
      "NOTE:      1   800    0.001           0.9836     0.8814  5.484e+04       7380          0     0.0259     0.00\n",
      "NOTE:      2   800    0.001           0.9462     0.8908  5.488e+04       6726          0     0.0259     0.00\n",
      "NOTE:      3   800    0.001           0.9827     0.8823  5.436e+04       7252          0     0.0259     0.00\n",
      "NOTE:      4   800    0.001            1.054     0.8825  5.488e+04       7310          0     0.0259     0.00\n",
      "NOTE:      5   800    0.001            1.087     0.8746  6.025e+04       8640          0     0.0259     0.00\n",
      "NOTE:      6   800    0.001           0.9632     0.8899  5.495e+04       6796          0     0.0259     0.00\n",
      "NOTE:      7   800    0.001           0.9648      0.907  5.849e+04       5996          0     0.0259     0.00\n",
      "NOTE:      8   800    0.001            1.024     0.8676  5.446e+04       8310          0     0.0259     0.00\n",
      "NOTE:      9   800    0.001           0.9852     0.9101   5.73e+04       5658          0     0.0259     0.00\n",
      "NOTE:     10   800    0.001            0.992     0.9045  5.566e+04       5878          0     0.0259     0.00\n",
      "NOTE:     11   800    0.001           0.9312     0.8954  5.656e+04       6608          0     0.0259     0.00\n",
      "NOTE:     12   800    0.001           0.9478     0.9143  5.738e+04       5381          0     0.0259     0.00\n",
      "NOTE:     13   800    0.001           0.9984     0.9126  5.731e+04       5486          0     0.0259     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  11        0.001          0.9913     0.8921  7.892e+05  9.542e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.018     0.8891  5.953e+04       7426          0     0.0259     0.00\n",
      "NOTE:      1   800    0.001            1.032      0.905  5.951e+04       6247          0    0.02591     0.00\n",
      "NOTE:      2   800    0.001             1.12     0.8896  6.198e+04       7694          0    0.02591     0.00\n",
      "NOTE:      3   800    0.001           0.9347      0.896  5.464e+04       6339          0    0.02591     0.00\n",
      "NOTE:      4   800    0.001            1.019     0.8804  5.757e+04       7818          0    0.02591     0.00\n",
      "NOTE:      5   800    0.001           0.9635     0.8751  5.364e+04       7654          0    0.02591     0.00\n",
      "NOTE:      6   800    0.001            1.017     0.8891   5.66e+04       7063          0    0.02591     0.00\n",
      "NOTE:      7   800    0.001           0.8461     0.8676  4.629e+04       7067          0    0.02591     0.00\n",
      "NOTE:      8   800    0.001           0.9766     0.9039  5.566e+04       5916          0    0.02591     0.00\n",
      "NOTE:      9   800    0.001            1.046     0.8747  5.916e+04       8478          0    0.02591     0.00\n",
      "NOTE:     10   800    0.001            1.041      0.883   6.29e+04       8336          0    0.02591     0.00\n",
      "NOTE:     11   800    0.001           0.8794     0.9245  5.474e+04       4469          0    0.02591     0.00\n",
      "NOTE:     12   800    0.001           0.9446     0.8912   5.38e+04       6571          0    0.02591     0.00\n",
      "NOTE:     13   800    0.001           0.9635     0.8636  5.361e+04       8468          0    0.02591     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  12        0.001          0.9859      0.888  7.896e+05  9.955e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9993     0.9015  6.011e+04       6568          0    0.02591     0.00\n",
      "NOTE:      1   800    0.001           0.9367     0.9021  5.644e+04       6128          0    0.02591     0.00\n",
      "NOTE:      2   800    0.001            1.063     0.8806  5.883e+04       7974          0    0.02591     0.00\n",
      "NOTE:      3   800    0.001           0.9325     0.9103  5.485e+04       5405          0    0.02591     0.00\n",
      "NOTE:      4   800    0.001           0.9534     0.8461  4.797e+04       8726          0    0.02592     0.00\n",
      "NOTE:      5   800    0.001           0.8768     0.9086  5.425e+04       5459          0    0.02592     0.00\n",
      "NOTE:      6   800    0.001           0.9147     0.8907  5.616e+04       6889          0    0.02592     0.00\n",
      "NOTE:      7   800    0.001            1.008     0.8462  5.558e+04   1.01e+04          0    0.02592     0.00\n",
      "NOTE:      8   800    0.001           0.9303     0.8829  5.294e+04       7020          0    0.02592     0.00\n",
      "NOTE:      9   800    0.001            1.047     0.8986  5.978e+04       6748          0    0.02592     0.00\n",
      "NOTE:     10   800    0.001           0.8873     0.9111  5.382e+04       5251          0    0.02592     0.00\n",
      "NOTE:     11   800    0.001             1.02     0.8647  5.671e+04       8874          0    0.02592     0.00\n",
      "NOTE:     12   800    0.001           0.9156     0.8862  5.517e+04       7084          0    0.02592     0.00\n",
      "NOTE:     13   800    0.001             1.11     0.8938  6.678e+04       7933          0    0.02592     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  13        0.001           0.971     0.8874  7.894e+05  1.002e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            0.953     0.9126  6.138e+04       5876          0    0.02592     0.00\n",
      "NOTE:      1   800    0.001           0.9215     0.8779  5.181e+04       7209          0    0.02592     0.00\n",
      "NOTE:      2   800    0.001            1.065     0.8865  5.893e+04       7545          0    0.02592     0.00\n",
      "NOTE:      3   800    0.001           0.9732     0.8903  5.577e+04       6872          0    0.02592     0.00\n",
      "NOTE:      4   800    0.001            1.119     0.8779  5.921e+04       8238          0    0.02592     0.00\n",
      "NOTE:      5   800    0.001           0.9672     0.9251  5.809e+04       4703          0    0.02592     0.00\n",
      "NOTE:      6   800    0.001           0.9914     0.8782  5.712e+04       7924          0    0.02592     0.00\n",
      "NOTE:      7   800    0.001            1.029     0.8951  5.998e+04       7031          0    0.02592     0.00\n",
      "NOTE:      8   800    0.001            1.012     0.8771  5.493e+04       7699          0    0.02592     0.00\n",
      "NOTE:      9   800    0.001           0.9043     0.8726  5.325e+04       7774          0    0.02593     0.00\n",
      "NOTE:     10   800    0.001           0.9186     0.8725  5.488e+04       8016          0    0.02593     0.00\n",
      "NOTE:     11   800    0.001            1.009     0.8764  5.332e+04       7522          0    0.02593     0.00\n",
      "NOTE:     12   800    0.001            1.025      0.861  5.506e+04       8888          0    0.02593     0.00\n",
      "NOTE:     13   800    0.001            1.081     0.9006  6.412e+04       7081          0    0.02593     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  14        0.001          0.9978     0.8863  7.979e+05  1.024e+05          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9711     0.8708  5.463e+04       8107          0    0.02593     0.00\n",
      "NOTE:      1   800    0.001           0.9317     0.8912  5.657e+04       6907          0    0.02593     0.00\n",
      "NOTE:      2   800    0.001           0.9995      0.906  5.931e+04       6151          0    0.02593     0.00\n",
      "NOTE:      3   800    0.001           0.9697      0.909  6.052e+04       6055          0    0.02593     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      4   800    0.001           0.9104     0.8887  5.429e+04       6800          0    0.02593     0.00\n",
      "NOTE:      5   800    0.001            1.047     0.8849  5.893e+04       7666          0    0.02593     0.00\n",
      "NOTE:      6   800    0.001            1.054     0.9275  6.489e+04       5075          0    0.02593     0.00\n",
      "NOTE:      7   800    0.001            0.989     0.8798  5.536e+04       7562          0    0.02593     0.00\n",
      "NOTE:      8   800    0.001           0.9951     0.9269  6.477e+04       5109          0    0.02593     0.00\n",
      "NOTE:      9   800    0.001           0.9815     0.8884  5.443e+04       6839          0    0.02593     0.00\n",
      "NOTE:     10   800    0.001           0.9344     0.9042  5.978e+04       6332          0    0.02593     0.00\n",
      "NOTE:     11   800    0.001            1.002     0.8497  5.426e+04       9599          0    0.02593     0.00\n",
      "NOTE:     12   800    0.001           0.9562      0.873   5.16e+04       7509          0    0.02593     0.00\n",
      "NOTE:     13   800    0.001            1.013     0.8908  5.977e+04       7329          0    0.02594     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  15        0.001          0.9825     0.8929  8.091e+05  9.704e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9644     0.8997  5.534e+04       6172          0    0.02594     0.00\n",
      "NOTE:      1   800    0.001           0.9778     0.9385  6.186e+04       4052          0    0.02594     0.00\n",
      "NOTE:      2   800    0.001            0.953     0.9125  5.601e+04       5372          0    0.02594     0.00\n",
      "NOTE:      3   800    0.001            1.024     0.8838   5.93e+04       7793          0    0.02594     0.00\n",
      "NOTE:      4   800    0.001           0.9752     0.8834   5.23e+04       6906          0    0.02594     0.00\n",
      "NOTE:      5   800    0.001           0.9297     0.9005  5.968e+04       6593          0    0.02594     0.00\n",
      "NOTE:      6   800    0.001           0.9749     0.9084  5.872e+04       5921          0    0.02594     0.00\n",
      "NOTE:      7   800    0.001            0.981     0.9159  6.325e+04       5806          0    0.02594     0.00\n",
      "NOTE:      8   800    0.001            1.027     0.9198  6.269e+04       5468          0    0.02594     0.00\n",
      "NOTE:      9   800    0.001            1.012     0.8958  6.354e+04       7391          0    0.02594     0.00\n",
      "NOTE:     10   800    0.001           0.9755     0.8932  5.559e+04       6645          0    0.02594     0.00\n",
      "NOTE:     11   800    0.001           0.8968     0.9128  5.488e+04       5241          0    0.02594     0.00\n",
      "NOTE:     12   800    0.001            1.034     0.9325  6.449e+04       4665          0    0.02594     0.00\n",
      "NOTE:     13   800    0.001            1.065     0.9001  6.409e+04       7110          0    0.02594     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  16        0.001          0.9849     0.9071  8.317e+05  8.514e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9726     0.9097  5.994e+04       5950          0    0.02595     0.00\n",
      "NOTE:      1   800    0.001           0.9999     0.8785  5.894e+04       8148          0    0.02595     0.00\n",
      "NOTE:      2   800    0.001             1.06     0.8832  6.002e+04       7939          0    0.02595     0.00\n",
      "NOTE:      3   800    0.001           0.8941     0.9135  5.753e+04       5446          0    0.02595     0.00\n",
      "NOTE:      4   800    0.001           0.9642     0.9243  5.809e+04       4759          0    0.02595     0.00\n",
      "NOTE:      5   800    0.001           0.9564     0.9202  5.949e+04       5162          0    0.02595     0.00\n",
      "NOTE:      6   800    0.001           0.9124     0.9084  5.451e+04       5499          0    0.02595     0.00\n",
      "NOTE:      7   800    0.001           0.8884     0.8941  5.288e+04       6260          0    0.02595     0.00\n",
      "NOTE:      8   800    0.001            1.021     0.8881  5.945e+04       7487          0    0.02595     0.00\n",
      "NOTE:      9   800    0.001           0.9168     0.9417  5.581e+04       3455          0    0.02595     0.00\n",
      "NOTE:     10   800    0.001           0.9456     0.9048  5.724e+04       6020          0    0.02595     0.00\n",
      "NOTE:     11   800    0.001            1.032     0.9221  6.369e+04       5384          0    0.02595     0.00\n",
      "NOTE:     12   800    0.001           0.9582     0.9126  6.103e+04       5847          0    0.02595     0.00\n",
      "NOTE:     13   800    0.001           0.9802     0.9082  6.542e+04       6616          0    0.02596     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  17        0.001          0.9644     0.9075   8.24e+05  8.397e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9593      0.901  5.528e+04       6074          0    0.02596     0.00\n",
      "NOTE:      1   800    0.001           0.9767     0.9208  5.893e+04       5066          0    0.02596     0.00\n",
      "NOTE:      2   800    0.001            1.016     0.9113   6.33e+04       6161          0    0.02596     0.00\n",
      "NOTE:      3   800    0.001            1.032     0.8752  5.598e+04       7984          0    0.02596     0.00\n",
      "NOTE:      4   800    0.001            1.086     0.9083  6.685e+04       6745          0    0.02596     0.00\n",
      "NOTE:      5   800    0.001           0.9606     0.8992  5.429e+04       6087          0    0.02596     0.00\n",
      "NOTE:      6   800    0.001             1.05     0.9235  6.937e+04       5749          0    0.02596     0.00\n",
      "NOTE:      7   800    0.001           0.9939     0.9242  6.133e+04       5031          0    0.02596     0.00\n",
      "NOTE:      8   800    0.001           0.9411     0.8974   5.25e+04       6004          0    0.02596     0.00\n",
      "NOTE:      9   800    0.001           0.9936     0.9049  6.025e+04       6329          0    0.02596     0.00\n",
      "NOTE:     10   800    0.001           0.9258     0.8976  5.669e+04       6464          0    0.02596     0.00\n",
      "NOTE:     11   800    0.001           0.9474     0.8938  5.681e+04       6747          0    0.02597     0.00\n",
      "NOTE:     12   800    0.001            0.851     0.9234  5.489e+04       4555          0    0.02597     0.00\n",
      "NOTE:     13   800    0.001           0.9317     0.9062  5.623e+04       5819          0    0.02597     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  18        0.001           0.976     0.9065  8.227e+05  8.482e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9127     0.9384  5.788e+04       3797          0    0.02597     0.00\n",
      "NOTE:      1   800    0.001           0.9677     0.9117  5.667e+04       5488          0    0.02597     0.00\n",
      "NOTE:      2   800    0.001             0.94     0.9055  5.782e+04       6037          0    0.02597     0.00\n",
      "NOTE:      3   800    0.001             1.05     0.9051  6.568e+04       6887          0    0.02597     0.00\n",
      "NOTE:      4   800    0.001           0.9355     0.9066  5.768e+04       5942          0    0.02597     0.00\n",
      "NOTE:      5   800    0.001           0.8523     0.9008  4.998e+04       5505          0    0.02597     0.00\n",
      "NOTE:      6   800    0.001           0.9876     0.9186  6.267e+04       5557          0    0.02597     0.00\n",
      "NOTE:      7   800    0.001            1.003     0.8966  5.581e+04       6434          0    0.02597     0.00\n",
      "NOTE:      8   800    0.001            1.018     0.9066  6.235e+04       6420          0    0.02597     0.00\n",
      "NOTE:      9   800    0.001           0.8813      0.895  5.351e+04       6281          0    0.02597     0.00\n",
      "NOTE:     10   800    0.001            1.044     0.9089  6.379e+04       6390          0    0.02597     0.00\n",
      "NOTE:     11   800    0.001           0.9872     0.9056  5.897e+04       6148          0    0.02597     0.00\n",
      "NOTE:     12   800    0.001            0.977     0.9322  6.294e+04       4575          0    0.02598     0.00\n",
      "NOTE:     13   800    0.001           0.8686     0.9141  5.427e+04       5098          0    0.02598     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  19        0.001          0.9589     0.9105    8.2e+05  8.056e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.8778      0.897  5.146e+04       5911          0    0.02598     0.00\n",
      "NOTE:      1   800    0.001           0.9356     0.9292  6.235e+04       4751          0    0.02598     0.00\n",
      "NOTE:      2   800    0.001            1.039     0.8906  6.035e+04       7413          0    0.02598     0.00\n",
      "NOTE:      3   800    0.001            1.009      0.906  5.877e+04       6101          0    0.02598     0.00\n",
      "NOTE:      4   800    0.001           0.8863     0.9151  5.299e+04       4917          0    0.02598     0.00\n",
      "NOTE:      5   800    0.001            0.883     0.9187  5.481e+04       4851          0    0.02598     0.00\n",
      "NOTE:      6   800    0.001           0.9933     0.9127  6.232e+04       5959          0    0.02598     0.00\n",
      "NOTE:      7   800    0.001            1.011     0.8698   5.96e+04       8924          0    0.02598     0.00\n",
      "NOTE:      8   800    0.001           0.9315     0.8908  5.725e+04       7018          0    0.02598     0.00\n",
      "NOTE:      9   800    0.001            1.083     0.9006  6.562e+04       7241          0    0.02598     0.00\n",
      "NOTE:     10   800    0.001           0.9636     0.8944  5.739e+04       6773          0    0.02598     0.00\n",
      "NOTE:     11   800    0.001           0.9398     0.8984  5.387e+04       6089          0    0.02598     0.00\n",
      "NOTE:     12   800    0.001           0.8855     0.8841  5.341e+04       7001          0    0.02598     0.00\n",
      "NOTE:     13   800    0.001           0.9604      0.908  6.202e+04       6282          0    0.02599     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  20        0.001          0.9571      0.901  8.122e+05  8.923e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9151     0.8809  5.521e+04       7462          0    0.02599     0.00\n",
      "NOTE:      1   800    0.001            1.052     0.8979  6.341e+04       7209          0    0.02599     0.00\n",
      "NOTE:      2   800    0.001            0.916     0.9038  5.762e+04       6134          0    0.02599     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      3   800    0.001           0.9644     0.9031  5.589e+04       5995          0    0.02599     0.00\n",
      "NOTE:      4   800    0.001           0.9233     0.8936  5.626e+04       6699          0    0.02599     0.00\n",
      "NOTE:      5   800    0.001           0.8905      0.909  5.592e+04       5595          0    0.02599     0.00\n",
      "NOTE:      6   800    0.001            0.985     0.9049  5.793e+04       6091          0    0.02599     0.00\n",
      "NOTE:      7   800    0.001            1.067     0.9398  6.521e+04       4176          0    0.02599     0.00\n",
      "NOTE:      8   800    0.001           0.9942     0.8977      6e+04       6836          0    0.02599     0.00\n",
      "NOTE:      9   800    0.001            0.913     0.9189  5.558e+04       4907          0    0.02599     0.00\n",
      "NOTE:     10   800    0.001            0.947     0.9136  5.241e+04       4954          0    0.02599     0.00\n",
      "NOTE:     11   800    0.001           0.9568     0.9271  6.124e+04       4814          0    0.02599     0.00\n",
      "NOTE:     12   800    0.001           0.8668     0.8876   5.29e+04       6698          0    0.02599     0.00\n",
      "NOTE:     13   800    0.001           0.8872     0.8787  5.631e+04       7770          0    0.02599     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  21        0.001          0.9485     0.9042  8.059e+05  8.534e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001             1.03     0.9095  6.274e+04       6241          0    0.02599     0.00\n",
      "NOTE:      1   800    0.001            1.012     0.8772   5.72e+04       8007          0    0.02599     0.00\n",
      "NOTE:      2   800    0.001            1.024     0.8943   6.11e+04       7225          0    0.02599     0.00\n",
      "NOTE:      3   800    0.001           0.9362     0.9013  5.758e+04       6303          0    0.02599     0.00\n",
      "NOTE:      4   800    0.001           0.9784     0.9216  6.354e+04       5405          0    0.02599     0.00\n",
      "NOTE:      5   800    0.001           0.8958     0.9233  5.416e+04       4497          0      0.026     0.00\n",
      "NOTE:      6   800    0.001           0.9268     0.8921   5.39e+04       6522          0      0.026     0.00\n",
      "NOTE:      7   800    0.001           0.9394     0.8884  5.551e+04       6976          0      0.026     0.00\n",
      "NOTE:      8   800    0.001            1.051     0.8964  6.539e+04       7559          0      0.026     0.00\n",
      "NOTE:      9   800    0.001           0.9616      0.922  6.236e+04       5278          0      0.026     0.00\n",
      "NOTE:     10   800    0.001           0.9146     0.9215  5.766e+04       4910          0      0.026     0.00\n",
      "NOTE:     11   800    0.001            0.911     0.9262  5.898e+04       4698          0      0.026     0.00\n",
      "NOTE:     12   800    0.001            0.913     0.9202  5.769e+04       5004          0      0.026     0.00\n",
      "NOTE:     13   800    0.001            0.919     0.9163  5.934e+04       5424          0      0.026     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  22        0.001           0.958     0.9078  8.272e+05  8.405e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9424     0.9117  5.823e+04       5643          0      0.026     0.00\n",
      "NOTE:      1   800    0.001            1.025     0.9225  6.469e+04       5432          0      0.026     0.00\n",
      "NOTE:      2   800    0.001           0.8098     0.8898  5.019e+04       6218          0      0.026     0.00\n",
      "NOTE:      3   800    0.001           0.8721     0.9213  5.462e+04       4665          0      0.026     0.00\n",
      "NOTE:      4   800    0.001            1.079     0.8961  6.453e+04       7481          0      0.026     0.00\n",
      "NOTE:      5   800    0.001           0.9009      0.907  5.319e+04       5451          0      0.026     0.00\n",
      "NOTE:      6   800    0.001           0.8805      0.925  5.731e+04       4646          0      0.026     0.00\n",
      "NOTE:      7   800    0.001           0.8667     0.8842  4.996e+04       6541          0    0.02601     0.00\n",
      "NOTE:      8   800    0.001            0.874     0.9134  5.864e+04       5558          0    0.02601     0.00\n",
      "NOTE:      9   800    0.001           0.9057     0.9249  5.974e+04       4854          0    0.02601     0.00\n",
      "NOTE:     10   800    0.001           0.9525     0.9178  6.069e+04       5438          0    0.02601     0.00\n",
      "NOTE:     11   800    0.001            1.014     0.9111  6.301e+04       6151          0    0.02601     0.00\n",
      "NOTE:     12   800    0.001           0.8933     0.9339  6.182e+04       4377          0    0.02601     0.00\n",
      "NOTE:     13   800    0.001           0.8739     0.9362  5.666e+04       3858          0    0.02601     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  23        0.001          0.9207     0.9142  8.133e+05  7.631e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.8723     0.9139  5.698e+04       5370          0    0.02601     0.00\n",
      "NOTE:      1   800    0.001           0.8565     0.9262  5.718e+04       4558          0    0.02601     0.00\n",
      "NOTE:      2   800    0.001           0.9245     0.8854  5.582e+04       7229          0    0.02601     0.00\n",
      "NOTE:      3   800    0.001           0.9193      0.908  5.512e+04       5586          0    0.02601     0.00\n",
      "NOTE:      4   800    0.001           0.9492     0.9153  6.049e+04       5598          0    0.02601     0.00\n",
      "NOTE:      5   800    0.001           0.9252     0.9062  6.106e+04       6318          0    0.02601     0.00\n",
      "NOTE:      6   800    0.001            1.013      0.941  6.421e+04       4026          0    0.02601     0.00\n",
      "NOTE:      7   800    0.001           0.9621     0.9111  5.954e+04       5813          0    0.02601     0.00\n",
      "NOTE:      8   800    0.001           0.9134       0.91  5.442e+04       5380          0    0.02601     0.00\n",
      "NOTE:      9   800    0.001           0.9615     0.8812  5.776e+04       7785          0    0.02601     0.00\n",
      "NOTE:     10   800    0.001           0.8864     0.9439    5.9e+04       3509          0    0.02602     0.00\n",
      "NOTE:     11   800    0.001           0.8756     0.9285  5.834e+04       4491          0    0.02602     0.00\n",
      "NOTE:     12   800    0.001           0.8855     0.9062  5.831e+04       6035          0    0.02602     0.00\n",
      "NOTE:     13   800    0.001            1.042     0.9029    6.3e+04       6777          0    0.02602     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  24        0.001          0.9276     0.9128  8.212e+05  7.848e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9589     0.9074  6.003e+04       6124          0    0.02602     0.00\n",
      "NOTE:      1   800    0.001           0.9021     0.9091  5.499e+04       5501          0    0.02602     0.00\n",
      "NOTE:      2   800    0.001           0.9136     0.9239  5.493e+04       4524          0    0.02602     0.00\n",
      "NOTE:      3   800    0.001           0.9617     0.9069  5.858e+04       6016          0    0.02602     0.00\n",
      "NOTE:      4   800    0.001            1.011     0.8944  6.065e+04       7163          0    0.02602     0.00\n",
      "NOTE:      5   800    0.001            1.016     0.8992  6.109e+04       6844          0    0.02602     0.00\n",
      "NOTE:      6   800    0.001           0.9239      0.892  5.817e+04       7043          0    0.02602     0.00\n",
      "NOTE:      7   800    0.001            1.093     0.8803  6.307e+04       8573          0    0.02602     0.00\n",
      "NOTE:      8   800    0.001             1.03     0.9285  6.862e+04       5281          0    0.02602     0.00\n",
      "NOTE:      9   800    0.001            0.952     0.8889  5.694e+04       7119          0    0.02602     0.00\n",
      "NOTE:     10   800    0.001           0.9037     0.8901  5.313e+04       6561          0    0.02602     0.00\n",
      "NOTE:     11   800    0.001           0.9882      0.903   6.48e+04       6965          0    0.02602     0.00\n",
      "NOTE:     12   800    0.001           0.9363      0.942  6.521e+04       4014          0    0.02602     0.00\n",
      "NOTE:     13   800    0.001            1.098     0.9281  7.037e+04       5448          0    0.02602     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  25        0.001          0.9778      0.907  8.506e+05  8.718e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.8487     0.9217  5.223e+04       4439          0    0.02602     0.00\n",
      "NOTE:      1   800    0.001           0.9193     0.9175  5.669e+04       5098          0    0.02603     0.00\n",
      "NOTE:      2   800    0.001           0.9226     0.9055  5.782e+04       6037          0    0.02603     0.00\n",
      "NOTE:      3   800    0.001           0.8754     0.9377  6.062e+04       4027          0    0.02603     0.00\n",
      "NOTE:      4   800    0.001           0.8846     0.9159   5.36e+04       4924          0    0.02603     0.00\n",
      "NOTE:      5   800    0.001           0.8453      0.917  5.416e+04       4900          0    0.02603     0.00\n",
      "NOTE:      6   800    0.001                1     0.9128  6.309e+04       6025          0    0.02603     0.00\n",
      "NOTE:      7   800    0.001           0.9694     0.9021  5.772e+04       6264          0    0.02603     0.00\n",
      "NOTE:      8   800    0.001           0.8418     0.9134  5.661e+04       5368          0    0.02603     0.00\n",
      "NOTE:      9   800    0.001            1.046     0.9026  6.304e+04       6800          0    0.02603     0.00\n",
      "NOTE:     10   800    0.001            1.005     0.8996  6.104e+04       6811          0    0.02603     0.00\n",
      "NOTE:     11   800    0.001           0.9397     0.9133  5.954e+04       5650          0    0.02603     0.00\n",
      "NOTE:     12   800    0.001            0.976     0.9391    6.6e+04       4279          0    0.02603     0.00\n",
      "NOTE:     13   800    0.001           0.9107     0.9112  5.712e+04       5570          0    0.02603     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  26        0.001          0.9275     0.9149  8.193e+05  7.619e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      0   800    0.001             0.93     0.8766  5.883e+04       8279          0    0.02603     0.00\n",
      "NOTE:      1   800    0.001           0.9376     0.9268  6.005e+04       4741          0    0.02603     0.00\n",
      "NOTE:      2   800    0.001            1.005     0.9058  6.159e+04       6406          0    0.02603     0.00\n",
      "NOTE:      3   800    0.001            1.032     0.9016  6.209e+04       6777          0    0.02603     0.00\n",
      "NOTE:      4   800    0.001           0.9458     0.9097  5.835e+04       5790          0    0.02603     0.00\n",
      "NOTE:      5   800    0.001           0.8423      0.912   5.15e+04       4968          0    0.02603     0.00\n",
      "NOTE:      6   800    0.001           0.9233     0.9178  5.776e+04       5172          0    0.02603     0.00\n",
      "NOTE:      7   800    0.001           0.9875     0.9199  6.358e+04       5533          0    0.02603     0.00\n",
      "NOTE:      8   800    0.001           0.9158     0.9263  6.224e+04       4955          0    0.02604     0.00\n",
      "NOTE:      9   800    0.001            1.025     0.9195  6.614e+04       5788          0    0.02604     0.00\n",
      "NOTE:     10   800    0.001            0.957     0.8988  5.925e+04       6673          0    0.02604     0.00\n",
      "NOTE:     11   800    0.001            0.966     0.9053  6.309e+04       6602          0    0.02604     0.00\n",
      "NOTE:     12   800    0.001           0.8441     0.8995  5.357e+04       5988          0    0.02604     0.00\n",
      "NOTE:     13   800    0.001           0.8982     0.9042  5.986e+04       6341          0    0.02604     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  27        0.001          0.9435     0.9089  8.379e+05  8.401e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            0.882     0.8952  5.385e+04       6302          0    0.02604     0.00\n",
      "NOTE:      1   800    0.001           0.9392     0.9404  6.237e+04       3953          0    0.02604     0.00\n",
      "NOTE:      2   800    0.001            1.008     0.9126  6.645e+04       6364          0    0.02604     0.00\n",
      "NOTE:      3   800    0.001           0.8874     0.8841   5.04e+04       6606          0    0.02604     0.00\n",
      "NOTE:      4   800    0.001             1.02      0.925   6.39e+04       5180          0    0.02604     0.00\n",
      "NOTE:      5   800    0.001           0.9766     0.8996  5.817e+04       6493          0    0.02604     0.00\n",
      "NOTE:      6   800    0.001           0.7914     0.9176  5.007e+04       4498          0    0.02604     0.00\n",
      "NOTE:      7   800    0.001            0.862     0.9122  5.578e+04       5370          0    0.02604     0.00\n",
      "NOTE:      8   800    0.001           0.9172     0.9183   5.66e+04       5037          0    0.02604     0.00\n",
      "NOTE:      9   800    0.001            1.044     0.9249  6.752e+04       5484          0    0.02604     0.00\n",
      "NOTE:     10   800    0.001            0.828     0.9194  5.353e+04       4691          0    0.02604     0.00\n",
      "NOTE:     11   800    0.001           0.9425     0.9175  5.882e+04       5290          0    0.02604     0.00\n",
      "NOTE:     12   800    0.001           0.9157     0.9257   5.93e+04       4760          0    0.02604     0.00\n",
      "NOTE:     13   800    0.001           0.9465      0.895  5.944e+04       6976          0    0.02604     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  28        0.001          0.9257     0.9138  8.162e+05    7.7e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.8924     0.9016  5.552e+04       6059          0    0.02604     0.00\n",
      "NOTE:      1   800    0.001           0.8407     0.9302  5.841e+04       4380          0    0.02604     0.00\n",
      "NOTE:      2   800    0.001           0.8906     0.9036  5.477e+04       5843          0    0.02604     0.00\n",
      "NOTE:      3   800    0.001           0.9359     0.8992  5.838e+04       6548          0    0.02604     0.00\n",
      "NOTE:      4   800    0.001           0.9663     0.9255  5.684e+04       4574          0    0.02605     0.00\n",
      "NOTE:      5   800    0.001           0.8531     0.9088  5.379e+04       5398          0    0.02605     0.00\n",
      "NOTE:      6   800    0.001           0.8932     0.9274   5.76e+04       4509          0    0.02605     0.00\n",
      "NOTE:      7   800    0.001           0.9391     0.9219  6.106e+04       5175          0    0.02605     0.00\n",
      "NOTE:      8   800    0.001           0.9227     0.9146  6.003e+04       5602          0    0.02605     0.00\n",
      "NOTE:      9   800    0.001             0.93     0.9299   5.99e+04       4514          0    0.02605     0.00\n",
      "NOTE:     10   800    0.001            1.046     0.9404  6.913e+04       4379          0    0.02605     0.00\n",
      "NOTE:     11   800    0.001           0.7425     0.9293  4.651e+04       3537          0    0.02605     0.00\n",
      "NOTE:     12   800    0.001           0.9459     0.9153  6.187e+04       5724          0    0.02605     0.00\n",
      "NOTE:     13   800    0.001           0.9115      0.926  6.081e+04       4857          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  29        0.001          0.9079     0.9197  8.146e+05   7.11e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9649     0.9052  6.166e+04       6454          0    0.02605     0.00\n",
      "NOTE:      1   800    0.001           0.9286     0.8957  6.017e+04       7006          0    0.02605     0.00\n",
      "NOTE:      2   800    0.001           0.8742     0.9044  5.172e+04       5466          0    0.02605     0.00\n",
      "NOTE:      3   800    0.001           0.9392     0.9203  5.988e+04       5185          0    0.02605     0.00\n",
      "NOTE:      4   800    0.001           0.9433      0.909  5.522e+04       5530          0    0.02605     0.00\n",
      "NOTE:      5   800    0.001            1.044      0.921  6.504e+04       5577          0    0.02605     0.00\n",
      "NOTE:      6   800    0.001           0.8792     0.9152  5.883e+04       5452          0    0.02605     0.00\n",
      "NOTE:      7   800    0.001           0.9509     0.9122  6.236e+04       6006          0    0.02605     0.00\n",
      "NOTE:      8   800    0.001           0.8853     0.9464  6.042e+04       3423          0    0.02605     0.00\n",
      "NOTE:      9   800    0.001           0.7615     0.8996  4.414e+04       4925          0    0.02605     0.00\n",
      "NOTE:     10   800    0.001           0.9426     0.9224  5.845e+04       4915          0    0.02605     0.00\n",
      "NOTE:     11   800    0.001           0.8832     0.9632  6.408e+04       2445          0    0.02605     0.00\n",
      "NOTE:     12   800    0.001            1.004     0.8961  5.644e+04       6542          0    0.02605     0.00\n",
      "NOTE:     13   800    0.001           0.9244     0.9213  5.875e+04       5017          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  30        0.001          0.9233      0.917  8.172e+05  7.394e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001            1.061     0.8811  6.274e+04       8469          0    0.02605     0.00\n",
      "NOTE:      1   800    0.001           0.8466     0.9422   5.82e+04       3568          0    0.02605     0.00\n",
      "NOTE:      2   800    0.001           0.9235     0.9172  5.775e+04       5215          0    0.02605     0.00\n",
      "NOTE:      3   800    0.001             0.92     0.9272  5.941e+04       4664          0    0.02606     0.00\n",
      "NOTE:      4   800    0.001            0.896     0.9264  5.667e+04       4504          0    0.02606     0.00\n",
      "NOTE:      5   800    0.001           0.9599     0.9101  6.406e+04       6326          0    0.02606     0.00\n",
      "NOTE:      6   800    0.001            1.025     0.8788  6.109e+04       8427          0    0.02606     0.00\n",
      "NOTE:      7   800    0.001           0.8555     0.8977  5.543e+04       6317          0    0.02606     0.00\n",
      "NOTE:      8   800    0.001            1.008     0.9016  6.205e+04       6773          0    0.02606     0.00\n",
      "NOTE:      9   800    0.001           0.9178     0.8813  5.689e+04       7660          0    0.02606     0.00\n",
      "NOTE:     10   800    0.001           0.9057     0.9324  5.781e+04       4191          0    0.02606     0.00\n",
      "NOTE:     11   800    0.001            0.782     0.9207   5.28e+04       4546          0    0.02606     0.00\n",
      "NOTE:     12   800    0.001            1.016     0.9039   5.99e+04       6369          0    0.02606     0.00\n",
      "NOTE:     13   800    0.001           0.8757      0.916  5.413e+04       4963          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  31        0.001          0.9281      0.909  8.189e+05  8.199e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9174     0.9062  6.047e+04       6263          0    0.02606     0.00\n",
      "NOTE:      1   800    0.001             0.93     0.9292  5.692e+04       4334          0    0.02606     0.00\n",
      "NOTE:      2   800    0.001            1.089     0.9163  6.503e+04       5940          0    0.02606     0.00\n",
      "NOTE:      3   800    0.001           0.8848     0.9403  5.779e+04       3668          0    0.02606     0.00\n",
      "NOTE:      4   800    0.001           0.9622      0.932  6.115e+04       4465          0    0.02606     0.00\n",
      "NOTE:      5   800    0.001             0.86     0.8915  5.239e+04       6379          0    0.02606     0.00\n",
      "NOTE:      6   800    0.001           0.8469     0.9186  5.872e+04       5205          0    0.02606     0.00\n",
      "NOTE:      7   800    0.001           0.9066     0.8872  5.697e+04       7243          0    0.02606     0.00\n",
      "NOTE:      8   800    0.001           0.7783     0.8973  4.895e+04       5605          0    0.02606     0.00\n",
      "NOTE:      9   800    0.001            0.935     0.9444  5.956e+04       3508          0    0.02606     0.00\n",
      "NOTE:     10   800    0.001           0.9851     0.9105  6.414e+04       6305          0    0.02606     0.00\n",
      "NOTE:     11   800    0.001           0.9457     0.9023  5.615e+04       6079          0    0.02606     0.00\n",
      "NOTE:     12   800    0.001           0.9192     0.9388  6.066e+04       3954          0    0.02606     0.00\n",
      "NOTE:     13   800    0.001           0.9334     0.8965  5.814e+04       6714          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  32        0.001           0.921     0.9152   8.17e+05  7.566e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.8967     0.9139  5.594e+04       5267          0    0.02606     0.00\n",
      "NOTE:      1   800    0.001           0.9623     0.9234   6.14e+04       5090          0    0.02606     0.00\n",
      "NOTE:      2   800    0.001           0.8934     0.8855  5.379e+04       6958          0    0.02606     0.00\n",
      "NOTE:      3   800    0.001            0.969     0.9146  6.256e+04       5840          0    0.02606     0.00\n",
      "NOTE:      4   800    0.001           0.9711     0.8934  5.827e+04       6949          0    0.02606     0.00\n",
      "NOTE:      5   800    0.001           0.9059     0.9331  5.848e+04       4194          0    0.02606     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      6   800    0.001             1.05     0.8995  6.595e+04       7371          0    0.02606     0.00\n",
      "NOTE:      7   800    0.001           0.8587     0.9192  5.855e+04       5150          0    0.02606     0.00\n",
      "NOTE:      8   800    0.001           0.8915      0.905  5.865e+04       6158          0    0.02606     0.00\n",
      "NOTE:      9   800    0.001            1.044     0.8866  6.442e+04       8237          0    0.02606     0.00\n",
      "NOTE:     10   800    0.001           0.8793     0.9056  5.695e+04       5936          0    0.02606     0.00\n",
      "NOTE:     11   800    0.001           0.9527     0.9291  6.491e+04       4952          0    0.02606     0.00\n",
      "NOTE:     12   800    0.001           0.8636     0.9415  5.922e+04       3679          0    0.02606     0.00\n",
      "NOTE:     13   800    0.001           0.9257     0.9393  6.138e+04       3967          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  33        0.001          0.9331     0.9133  8.405e+05  7.975e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9455     0.9375  6.079e+04       4053          0    0.02606     0.00\n",
      "NOTE:      1   800    0.001           0.9355     0.9222  5.862e+04       4945          0    0.02606     0.00\n",
      "NOTE:      2   800    0.001           0.9287     0.8949  5.527e+04       6490          0    0.02607     0.00\n",
      "NOTE:      3   800    0.001           0.9588     0.8909  5.606e+04       6866          0    0.02607     0.00\n",
      "NOTE:      4   800    0.001           0.9171     0.9283  5.942e+04       4586          0    0.02607     0.00\n",
      "NOTE:      5   800    0.001           0.8794      0.934  5.894e+04       4162          0    0.02607     0.00\n",
      "NOTE:      6   800    0.001           0.8482     0.9095  5.259e+04       5233          0    0.02607     0.00\n",
      "NOTE:      7   800    0.001           0.8485     0.9228  5.753e+04       4810          0    0.02607     0.00\n",
      "NOTE:      8   800    0.001           0.9084     0.9394  6.222e+04       4012          0    0.02607     0.00\n",
      "NOTE:      9   800    0.001           0.9585     0.9363  6.338e+04       4312          0    0.02607     0.00\n",
      "NOTE:     10   800    0.001            1.008     0.8974  6.273e+04       7170          0    0.02607     0.00\n",
      "NOTE:     11   800    0.001           0.9456     0.9116  5.815e+04       5642          0    0.02607     0.00\n",
      "NOTE:     12   800    0.001           0.9317     0.9225  5.659e+04       4753          0    0.02607     0.00\n",
      "NOTE:     13   800    0.001           0.9216     0.9226  6.126e+04       5141          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  34        0.001          0.9239     0.9194  8.235e+05  7.218e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800    0.001           0.9392     0.9255  5.697e+04       4588          0    0.02607     0.00\n",
      "NOTE:      1   800    0.001           0.9055      0.915  6.408e+04       5952          0    0.02607     0.00\n",
      "NOTE:      2   800    0.001           0.8448     0.9221  5.781e+04       4881          0    0.02607     0.00\n",
      "NOTE:      3   800    0.001           0.9132     0.9006   5.73e+04       6323          0    0.02607     0.00\n",
      "NOTE:      4   800    0.001           0.8874     0.9121   5.86e+04       5648          0    0.02607     0.00\n",
      "NOTE:      5   800    0.001           0.9084     0.9287  5.948e+04       4569          0    0.02607     0.00\n",
      "NOTE:      6   800    0.001             1.02     0.9202  6.406e+04       5557          0    0.02607     0.00\n",
      "NOTE:      7   800    0.001           0.8618      0.908   5.46e+04       5534          0    0.02607     0.00\n",
      "NOTE:      8   800    0.001           0.9256     0.9245   6.49e+04       5303          0    0.02607     0.00\n",
      "NOTE:      9   800    0.001           0.8818     0.9068  5.529e+04       5681          0    0.02607     0.00\n",
      "NOTE:     10   800    0.001            0.958     0.9044  6.038e+04       6382          0    0.02607     0.00\n",
      "NOTE:     11   800    0.001           0.9677     0.9194  6.648e+04       5828          0    0.02607     0.00\n",
      "NOTE:     12   800    0.001           0.9766     0.9277  6.382e+04       4976          0    0.02607     0.00\n",
      "NOTE:     13   800    0.001           0.9825     0.8993  6.178e+04       6915          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  35        0.001          0.9266     0.9154  8.455e+05  7.814e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.9434     0.9154  6.221e+04       5752          0    0.02607     0.00\n",
      "NOTE:      1   800   0.0008           0.8836     0.9168  5.678e+04       5153          0    0.02607     0.00\n",
      "NOTE:      2   800   0.0008            1.043      0.907  6.557e+04       6721          0    0.02607     0.00\n",
      "NOTE:      3   800   0.0008           0.9178     0.9154  5.913e+04       5467          0    0.02607     0.00\n",
      "NOTE:      4   800   0.0008           0.8593     0.9404  5.828e+04       3693          0    0.02607     0.00\n",
      "NOTE:      5   800   0.0008            1.008      0.919  6.109e+04       5381          0    0.02607     0.00\n",
      "NOTE:      6   800   0.0008           0.9132     0.9202  5.924e+04       5134          0    0.02607     0.00\n",
      "NOTE:      7   800   0.0008            1.007      0.912   6.17e+04       5950          0    0.02607     0.00\n",
      "NOTE:      8   800   0.0008           0.8898     0.9297  6.127e+04       4632          0    0.02607     0.00\n",
      "NOTE:      9   800   0.0008           0.9399     0.9068  5.574e+04       5732          0    0.02607     0.00\n",
      "NOTE:     10   800   0.0008           0.8356     0.9216  5.898e+04       5018          0    0.02607     0.00\n",
      "NOTE:     11   800   0.0008           0.9064     0.9175  5.882e+04       5290          0    0.02607     0.00\n",
      "NOTE:     12   800   0.0008           0.9057     0.9133  5.525e+04       5245          0    0.02607     0.00\n",
      "NOTE:     13   800   0.0008           0.8849     0.8873  5.179e+04       6575          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  36       0.0008           0.924      0.916  8.258e+05  7.574e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.8604     0.9374  5.305e+04       3540          0    0.02607     0.00\n",
      "NOTE:      1   800   0.0008           0.9658     0.9182  5.904e+04       5259          0    0.02607     0.00\n",
      "NOTE:      2   800   0.0008           0.9868     0.8969  6.085e+04       6993          0    0.02607     0.00\n",
      "NOTE:      3   800   0.0008           0.8883     0.9077  5.606e+04       5703          0    0.02607     0.00\n",
      "NOTE:      4   800   0.0008           0.8764     0.9237  5.937e+04       4906          0    0.02607     0.00\n",
      "NOTE:      5   800   0.0008            0.965     0.9035   6.35e+04       6782          0    0.02607     0.00\n",
      "NOTE:      6   800   0.0008           0.9139     0.9308  5.777e+04       4296          0    0.02607     0.00\n",
      "NOTE:      7   800   0.0008           0.9909     0.9318  6.424e+04       4704          0    0.02607     0.00\n",
      "NOTE:      8   800   0.0008            1.021     0.9244  6.882e+04       5629          0    0.02607     0.00\n",
      "NOTE:      9   800   0.0008            0.928     0.9305  6.204e+04       4634          0    0.02607     0.00\n",
      "NOTE:     10   800   0.0008            0.909     0.9205  6.123e+04       5285          0    0.02607     0.00\n",
      "NOTE:     11   800   0.0008            1.038     0.9196  6.239e+04       5453          0    0.02607     0.00\n",
      "NOTE:     12   800   0.0008           0.9631     0.9233   6.31e+04       5245          0    0.02607     0.00\n",
      "NOTE:     13   800   0.0008           0.9303     0.9308  6.296e+04       4682          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  37       0.0008          0.9455     0.9212  8.544e+05  7.311e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.9516      0.946  6.382e+04       3644          0    0.02607     0.00\n",
      "NOTE:      1   800   0.0008           0.8921     0.9293  6.141e+04       4675          0    0.02607     0.00\n",
      "NOTE:      2   800   0.0008           0.9913     0.9128  6.337e+04       6054          0    0.02607     0.00\n",
      "NOTE:      3   800   0.0008           0.9123      0.896  5.847e+04       6788          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008           0.9005      0.934  5.983e+04       4228          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008           0.8865      0.907  6.027e+04       6179          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.8866     0.9426  5.921e+04       3608          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008            0.888     0.9301  6.091e+04       4576          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008           0.8808     0.9276  5.998e+04       4681          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008            0.866     0.9237  5.641e+04       4661          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008            1.001     0.8938  6.263e+04       7439          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008           0.8715     0.9243  5.762e+04       4719          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.9017     0.9195  5.724e+04       5010          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008           0.9148     0.9131  5.833e+04       5552          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  38       0.0008          0.9103     0.9212  8.395e+05  7.181e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.9003     0.9176  5.613e+04       5041          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008           0.9196     0.8955  5.848e+04       6827          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008           0.9362     0.9035  6.054e+04       6467          0    0.02608     0.00\n",
      "NOTE:      3   800   0.0008           0.8311     0.9228  5.757e+04       4816          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008           0.9413     0.9119  5.889e+04       5693          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008           0.8963     0.9283  5.546e+04       4281          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008            1.021     0.9195  6.194e+04       5422          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.8869     0.9167  5.589e+04       5076          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008           0.8268     0.9117  5.311e+04       5143          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008           0.9426     0.9335  6.282e+04       4475          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008             1.01     0.8928  6.145e+04       7377          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008           0.8868     0.9152  5.609e+04       5200          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.8969     0.8999  5.598e+04       6228          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008           0.8723     0.8982   5.15e+04       5834          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  39       0.0008           0.912     0.9119  8.058e+05  7.788e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.8542     0.9188   5.35e+04       4728          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008            1.041     0.8928  6.166e+04       7403          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008           0.8921     0.9053  5.142e+04       5381          0    0.02608     0.00\n",
      "NOTE:      3   800   0.0008           0.8782     0.9104  5.926e+04       5833          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008           0.8835     0.9349  5.936e+04       4135          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008           0.8536     0.9002  5.433e+04       6020          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.9977      0.919  6.211e+04       5472          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.8396     0.9174  5.578e+04       5025          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008            0.945     0.9058  5.896e+04       6132          0    0.02608     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      9   800   0.0008           0.9199     0.8941  5.702e+04       6753          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008            1.007     0.9016  6.236e+04       6807          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008           0.9875     0.9191  6.576e+04       5791          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.9761     0.9065  6.021e+04       6214          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008           0.9154     0.9192   6.02e+04       5293          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  40       0.0008          0.9279     0.9103  8.219e+05  8.099e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008            0.861     0.8929  5.481e+04       6573          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008           0.9133     0.9384  6.068e+04       3981          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008                1     0.8979  6.275e+04       7132          0    0.02608     0.00\n",
      "NOTE:      3   800   0.0008           0.9416     0.9392  6.445e+04       4173          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008           0.8739     0.9437   5.64e+04       3365          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008            1.029     0.9053   6.54e+04       6840          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.8294     0.9227  5.324e+04       4461          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.8526     0.9324  5.396e+04       3915          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008           0.9343     0.9068  5.539e+04       5691          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008           0.9209     0.8878  5.481e+04       6926          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008           0.9913      0.903  6.303e+04       6767          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008            1.156     0.9145  7.159e+04       6691          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.9061     0.9334  6.315e+04       4508          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008           0.9908      0.915  6.597e+04       6127          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  41       0.0008          0.9429     0.9164  8.456e+05  7.715e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.8848     0.9233  5.911e+04       4913          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008           0.8485     0.9201  5.648e+04       4902          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008           0.8298     0.9119  5.372e+04       5192          0    0.02608     0.00\n",
      "NOTE:      3   800   0.0008            1.068     0.8739  6.366e+04       9187          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008           0.9666     0.8942  5.892e+04       6968          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008           0.9715     0.9155  6.329e+04       5840          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.9056     0.9261  5.913e+04       4715          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.8494     0.9104  5.498e+04       5414          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008           0.9745     0.8968   6.24e+04       7183          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008           0.8396      0.932  5.832e+04       4255          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008           0.9001     0.9317  6.455e+04       4730          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008             1.02      0.892  5.952e+04       7205          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.8534      0.918  5.743e+04       5129          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008           0.9287     0.9438  6.162e+04       3667          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  42       0.0008          0.9172     0.9131  8.331e+05   7.93e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.8364     0.9213  5.599e+04       4784          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008           0.7921     0.9344  5.298e+04       3720          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008           0.9271     0.9106  6.151e+04       6042          0    0.02608     0.00\n",
      "NOTE:      3   800   0.0008           0.9621     0.9339    6.5e+04       4603          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008           0.8531     0.9191  5.176e+04       4557          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008           0.8961     0.9136  5.724e+04       5416          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.8184     0.9351  5.496e+04       3814          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.9176     0.9275  5.883e+04       4600          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008           0.7844      0.938  5.468e+04       3612          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008           0.9251     0.9325  6.173e+04       4466          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008           0.8391     0.9117  5.335e+04       5170          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008           0.9773     0.9144  5.902e+04       5522          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.8093     0.9073  5.549e+04       5669          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008           0.9906     0.9212  6.376e+04       5456          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  43       0.0008          0.8806     0.9228  8.063e+05  6.743e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008            0.843     0.9255  5.845e+04       4704          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008           0.9596     0.8951  6.327e+04       7418          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008           0.9538     0.9379  6.379e+04       4222          0    0.02608     0.00\n",
      "NOTE:      3   800   0.0008           0.8396     0.9337  5.786e+04       4108          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008            0.917     0.9221  5.844e+04       4940          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008            1.002     0.8997  6.159e+04       6863          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.9005      0.899  5.841e+04       6560          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.9951     0.9377  6.274e+04       4172          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008            0.905     0.9417  6.077e+04       3762          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008           0.9468     0.9494  6.736e+04       3588          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008           0.8852     0.9307  5.872e+04       4375          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008           0.8918     0.9228  6.108e+04       5112          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.9411     0.8887  5.461e+04       6840          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008            1.007     0.9173  6.174e+04       5569          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  44       0.0008          0.9277     0.9216  8.488e+05  7.223e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.9964     0.8869  6.113e+04       7792          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008            1.008       0.94  6.755e+04       4314          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008           0.9375     0.9193  6.305e+04       5536          0    0.02608     0.00\n",
      "NOTE:      3   800   0.0008           0.8999     0.9086   5.39e+04       5423          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008            1.029     0.9014  6.429e+04       7034          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008            0.895     0.9107  5.727e+04       5617          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.9808     0.8904  5.549e+04       6834          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.8271     0.9188  5.355e+04       4734          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008            0.935     0.9213  6.295e+04       5380          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008            1.056     0.9147  6.854e+04       6393          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008           0.9339     0.9003  5.737e+04       6352          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008           0.8807     0.9205  5.727e+04       4946          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.7683     0.9426  5.623e+04       3426          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008           0.9413     0.9368  6.042e+04       4078          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  45       0.0008           0.935     0.9151   8.39e+05  7.786e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.8382     0.9278  5.526e+04       4301          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008           0.9022     0.9481  6.259e+04       3425          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008           0.8803     0.9336  6.257e+04       4450          0    0.02608     0.00\n",
      "NOTE:      3   800   0.0008            0.881     0.8968  5.343e+04       6149          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008           0.9174     0.9042  5.969e+04       6324          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008           0.9248     0.9158  6.017e+04       5534          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.9101     0.9018  5.939e+04       6465          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.8904     0.9086  5.303e+04       5337          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008           0.9114     0.8777  5.508e+04       7672          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008           0.8822     0.9341  5.973e+04       4214          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008           0.8341     0.9537  6.188e+04       3007          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008           0.9032     0.9345  6.057e+04       4246          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.9028     0.9207  5.841e+04       5032          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008           0.9213     0.9232  5.992e+04       4987          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  46       0.0008          0.8928     0.9203  8.217e+05  7.114e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.8705     0.9414  5.764e+04       3587          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008           0.9996     0.8956  6.146e+04       7168          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008           0.9505     0.9207  6.279e+04       5407          0    0.02608     0.00\n",
      "NOTE:      3   800   0.0008           0.8829     0.9254  5.725e+04       4615          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008           0.7776     0.9267  4.931e+04       3900          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008           0.8511     0.9275  5.552e+04       4342          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.7242     0.9464  5.061e+04       2867          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008            0.886     0.9438  5.865e+04       3493          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008           0.8649     0.9321   6.11e+04       4453          0    0.02609     0.00\n",
      "NOTE:      9   800   0.0008           0.8338      0.899  5.181e+04       5824          0    0.02609     0.00\n",
      "NOTE:     10   800   0.0008           0.9043     0.9011  5.785e+04       6351          0    0.02609     0.00\n",
      "NOTE:     11   800   0.0008           0.8972     0.9196  6.228e+04       5442          0    0.02609     0.00\n",
      "NOTE:     12   800   0.0008           0.9082     0.9185  5.541e+04       4918          0    0.02609     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     13   800   0.0008           0.8384     0.9262  5.767e+04       4595          0    0.02609     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  47       0.0008          0.8707     0.9227  7.994e+05  6.696e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.9704     0.9226  6.309e+04       5289          0    0.02609     0.00\n",
      "NOTE:      1   800   0.0008           0.8796     0.9209  5.747e+04       4935          0    0.02609     0.00\n",
      "NOTE:      2   800   0.0008           0.9409     0.8833  5.552e+04       7335          0    0.02609     0.00\n",
      "NOTE:      3   800   0.0008           0.8426     0.9093  5.626e+04       5612          0    0.02609     0.00\n",
      "NOTE:      4   800   0.0008           0.9704     0.9411   6.28e+04       3929          0    0.02609     0.00\n",
      "NOTE:      5   800   0.0008            1.001     0.9087  5.873e+04       5904          0    0.02609     0.00\n",
      "NOTE:      6   800   0.0008           0.9319     0.9036  5.793e+04       6180          0    0.02609     0.00\n",
      "NOTE:      7   800   0.0008           0.8459      0.928  5.038e+04       3906          0    0.02609     0.00\n",
      "NOTE:      8   800   0.0008           0.7437     0.9421  5.543e+04       3406          0    0.02609     0.00\n",
      "NOTE:      9   800   0.0008            0.848     0.9065  5.501e+04       5673          0    0.02609     0.00\n",
      "NOTE:     10   800   0.0008           0.9271     0.9289  6.162e+04       4720          0    0.02609     0.00\n",
      "NOTE:     11   800   0.0008           0.8809     0.9506   6.54e+04       3396          0    0.02609     0.00\n",
      "NOTE:     12   800   0.0008           0.9933     0.9015  6.078e+04       6645          0    0.02609     0.00\n",
      "NOTE:     13   800   0.0008           0.8596     0.9251  5.711e+04       4622          0    0.02609     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  48       0.0008          0.9025     0.9195  8.175e+05  7.155e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.9529      0.945  6.236e+04       3627          0    0.02609     0.00\n",
      "NOTE:      1   800   0.0008           0.8626     0.9001  5.601e+04       6217          0    0.02609     0.00\n",
      "NOTE:      2   800   0.0008            1.074     0.9276   6.95e+04       5424          0    0.02609     0.00\n",
      "NOTE:      3   800   0.0008           0.9758     0.9154  6.398e+04       5913          0    0.02609     0.00\n",
      "NOTE:      4   800   0.0008           0.9138     0.9092  5.957e+04       5949          0    0.02609     0.00\n",
      "NOTE:      5   800   0.0008           0.9913     0.9373   6.36e+04       4256          0    0.02609     0.00\n",
      "NOTE:      6   800   0.0008           0.9298     0.8737  5.295e+04       7651          0    0.02609     0.00\n",
      "NOTE:      7   800   0.0008            1.074     0.9122  6.368e+04       6127          0    0.02609     0.00\n",
      "NOTE:      8   800   0.0008           0.8471     0.9297  5.885e+04       4452          0    0.02609     0.00\n",
      "NOTE:      9   800   0.0008            0.823     0.9297  6.095e+04       4610          0    0.02609     0.00\n",
      "NOTE:     10   800   0.0008           0.8218     0.9222  5.194e+04       4380          0    0.02609     0.00\n",
      "NOTE:     11   800   0.0008           0.8731     0.9014  4.971e+04       5440          0    0.02609     0.00\n",
      "NOTE:     12   800   0.0008           0.9772     0.9365  6.392e+04       4337          0    0.02609     0.00\n",
      "NOTE:     13   800   0.0008           0.9397     0.9374  6.238e+04       4162          0    0.02609     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  49       0.0008          0.9326     0.9204  8.394e+05  7.255e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.9092     0.9259   5.94e+04       4756          0    0.02609     0.00\n",
      "NOTE:      1   800   0.0008           0.9166     0.9352  6.067e+04       4203          0    0.02609     0.00\n",
      "NOTE:      2   800   0.0008           0.9042     0.9297  5.772e+04       4366          0    0.02609     0.00\n",
      "NOTE:      3   800   0.0008           0.9306     0.9202  6.086e+04       5275          0    0.02609     0.00\n",
      "NOTE:      4   800   0.0008           0.9648     0.8881  5.512e+04       6944          0    0.02609     0.00\n",
      "NOTE:      5   800   0.0008           0.8281     0.9346  5.456e+04       3820          0    0.02609     0.00\n",
      "NOTE:      6   800   0.0008           0.9605     0.9286  6.403e+04       4923          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.8253     0.9037   5.45e+04       5806          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008           0.8075     0.9292  5.506e+04       4195          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008            1.016     0.9244  6.412e+04       5241          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008           0.9259     0.9273  6.159e+04       4825          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008            0.882     0.9542  6.426e+04       3084          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008            0.917     0.9081  5.862e+04       5929          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008           0.8834     0.9389  5.956e+04       3878          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  50       0.0008          0.9051     0.9251  8.301e+05  6.725e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.9568     0.8924  5.994e+04       7229          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008           0.8948     0.9415  6.296e+04       3913          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008           0.8855     0.9079  5.473e+04       5550          0    0.02608     0.00\n",
      "NOTE:      3   800   0.0008           0.8673      0.939  5.836e+04       3789          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008           0.8322     0.9344  5.681e+04       3990          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008           0.9438     0.9477  6.502e+04       3587          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.9208     0.9163  6.149e+04       5616          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.8756     0.9055  5.702e+04       5949          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008           0.9867     0.9226  6.256e+04       5251          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008           0.9265     0.9415  6.264e+04       3889          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008           0.7839     0.9279  5.257e+04       4084          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008           0.9486      0.929  5.965e+04       4557          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.8271      0.897  5.315e+04       6100          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008           0.8834     0.9205  5.492e+04       4742          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  51       0.0008          0.8952     0.9233  8.218e+05  6.825e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.9151     0.9442  6.303e+04       3723          0    0.02609     0.00\n",
      "NOTE:      1   800   0.0008           0.9322     0.9248   5.88e+04       4783          0    0.02609     0.00\n",
      "NOTE:      2   800   0.0008           0.8935     0.9471  6.076e+04       3393          0    0.02609     0.00\n",
      "NOTE:      3   800   0.0008           0.8627     0.9149  5.921e+04       5504          0    0.02609     0.00\n",
      "NOTE:      4   800   0.0008             0.94     0.9205  5.546e+04       4791          0    0.02609     0.00\n",
      "NOTE:      5   800   0.0008           0.9284     0.9337  6.345e+04       4505          0    0.02609     0.00\n",
      "NOTE:      6   800   0.0008           0.9534     0.9435  6.432e+04       3855          0    0.02609     0.00\n",
      "NOTE:      7   800   0.0008           0.9061     0.9015  5.543e+04       6060          0    0.02609     0.00\n",
      "NOTE:      8   800   0.0008           0.9067     0.9243  5.939e+04       4867          0    0.02609     0.00\n",
      "NOTE:      9   800   0.0008           0.9171     0.8749   5.16e+04       7380          0    0.02609     0.00\n",
      "NOTE:     10   800   0.0008           0.8092     0.9354   5.55e+04       3834          0    0.02609     0.00\n",
      "NOTE:     11   800   0.0008           0.9632     0.9095  6.073e+04       6045          0    0.02609     0.00\n",
      "NOTE:     12   800   0.0008           0.9958     0.9464  6.874e+04       3894          0    0.02609     0.00\n",
      "NOTE:     13   800   0.0008            1.028     0.9257  7.045e+04       5654          0    0.02609     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  52       0.0008          0.9251     0.9254  8.469e+05  6.829e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800   0.0008           0.8447     0.9341  5.654e+04       3991          0    0.02608     0.00\n",
      "NOTE:      1   800   0.0008           0.8555     0.9416   6.05e+04       3755          0    0.02608     0.00\n",
      "NOTE:      2   800   0.0008           0.8856     0.9085  5.095e+04       5133          0    0.02609     0.00\n",
      "NOTE:      3   800   0.0008           0.8848      0.914    5.6e+04       5270          0    0.02608     0.00\n",
      "NOTE:      4   800   0.0008           0.8741     0.9467  6.209e+04       3494          0    0.02608     0.00\n",
      "NOTE:      5   800   0.0008           0.8838     0.9323  6.008e+04       4363          0    0.02608     0.00\n",
      "NOTE:      6   800   0.0008           0.9024     0.9171  6.255e+04       5656          0    0.02608     0.00\n",
      "NOTE:      7   800   0.0008           0.9131     0.9286   6.09e+04       4685          0    0.02608     0.00\n",
      "NOTE:      8   800   0.0008           0.8015     0.8936  5.303e+04       6317          0    0.02608     0.00\n",
      "NOTE:      9   800   0.0008           0.9115     0.9287  6.276e+04       4821          0    0.02608     0.00\n",
      "NOTE:     10   800   0.0008           0.8183      0.953  6.031e+04       2976          0    0.02608     0.00\n",
      "NOTE:     11   800   0.0008           0.9244     0.9357  5.992e+04       4115          0    0.02608     0.00\n",
      "NOTE:     12   800   0.0008           0.9183      0.912  5.591e+04       5397          0    0.02608     0.00\n",
      "NOTE:     13   800   0.0008            0.825     0.9295  5.079e+04       3853          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  53       0.0008          0.8745     0.9272  8.123e+05  6.383e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00064           0.8252      0.928   5.82e+04       4516          0    0.02608     0.00\n",
      "NOTE:      1   800  0.00064           0.8801     0.9029  5.447e+04       5859          0    0.02608     0.00\n",
      "NOTE:      2   800  0.00064           0.9105       0.93  5.873e+04       4424          0    0.02608     0.00\n",
      "NOTE:      3   800  0.00064           0.9137     0.9233   5.86e+04       4865          0    0.02608     0.00\n",
      "NOTE:      4   800  0.00064           0.9892     0.9458  7.109e+04       4077          0    0.02608     0.00\n",
      "NOTE:      5   800  0.00064           0.8534     0.9286  5.342e+04       4108          0    0.02608     0.00\n",
      "NOTE:      6   800  0.00064           0.8271      0.928  5.536e+04       4293          0    0.02608     0.00\n",
      "NOTE:      7   800  0.00064           0.9725     0.9159  6.358e+04       5836          0    0.02608     0.00\n",
      "NOTE:      8   800  0.00064           0.8259      0.944  6.358e+04       3769          0    0.02608     0.00\n",
      "NOTE:      9   800  0.00064           0.8939     0.9488  6.129e+04       3310          0    0.02608     0.00\n",
      "NOTE:     10   800  0.00064           0.9215     0.9142  6.014e+04       5647          0    0.02608     0.00\n",
      "NOTE:     11   800  0.00064           0.8559     0.9247  5.666e+04       4614          0    0.02608     0.00\n",
      "NOTE:     12   800  0.00064            0.905     0.9247  5.786e+04       4713          0    0.02608     0.00\n",
      "NOTE:     13   800  0.00064           0.8522     0.8913  5.304e+04       6466          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  54       0.0006          0.8876     0.9255   8.26e+05   6.65e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      0   800  0.00064           0.9591     0.8951  5.694e+04       6670          0    0.02608     0.00\n",
      "NOTE:      1   800  0.00064            0.886     0.9278  5.967e+04       4646          0    0.02608     0.00\n",
      "NOTE:      2   800  0.00064           0.9157     0.9125   5.95e+04       5705          0    0.02608     0.00\n",
      "NOTE:      3   800  0.00064           0.8896     0.9228   6.04e+04       5055          0    0.02608     0.00\n",
      "NOTE:      4   800  0.00064           0.8548     0.8934  5.083e+04       6066          0    0.02608     0.00\n",
      "NOTE:      5   800  0.00064           0.9668      0.894  5.646e+04       6697          0    0.02608     0.00\n",
      "NOTE:      6   800  0.00064           0.8608     0.9086  5.645e+04       5677          0    0.02608     0.00\n",
      "NOTE:      7   800  0.00064           0.9243     0.9311  6.316e+04       4676          0    0.02608     0.00\n",
      "NOTE:      8   800  0.00064           0.8374     0.9282  5.426e+04       4195          0    0.02608     0.00\n",
      "NOTE:      9   800  0.00064            0.974     0.9136   5.98e+04       5656          0    0.02608     0.00\n",
      "NOTE:     10   800  0.00064           0.9327        0.9  5.924e+04       6579          0    0.02608     0.00\n",
      "NOTE:     11   800  0.00064           0.9059     0.9215  6.014e+04       5125          0    0.02608     0.00\n",
      "NOTE:     12   800  0.00064           0.8883     0.9052  5.392e+04       5646          0    0.02608     0.00\n",
      "NOTE:     13   800  0.00064           0.8905     0.9068  5.625e+04       5784          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  55       0.0006          0.9061     0.9117   8.07e+05  7.818e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00064           0.9785     0.9046  5.998e+04       6327          0    0.02608     0.00\n",
      "NOTE:      1   800  0.00064           0.8916     0.9106  5.642e+04       5539          0    0.02608     0.00\n",
      "NOTE:      2   800  0.00064           0.9905     0.9318  6.342e+04       4643          0    0.02608     0.00\n",
      "NOTE:      3   800  0.00064           0.9449     0.9213   6.01e+04       5131          0    0.02608     0.00\n",
      "NOTE:      4   800  0.00064             1.01     0.9058  5.846e+04       6077          0    0.02608     0.00\n",
      "NOTE:      5   800  0.00064           0.9116     0.9018  5.683e+04       6187          0    0.02608     0.00\n",
      "NOTE:      6   800  0.00064           0.9107     0.8911  5.513e+04       6737          0    0.02608     0.00\n",
      "NOTE:      7   800  0.00064           0.8648     0.9272  5.862e+04       4604          0    0.02608     0.00\n",
      "NOTE:      8   800  0.00064           0.9428      0.911  6.018e+04       5883          0    0.02608     0.00\n",
      "NOTE:      9   800  0.00064           0.9349     0.9004  5.951e+04       6580          0    0.02608     0.00\n",
      "NOTE:     10   800  0.00064            0.842     0.9497  5.905e+04       3129          0    0.02608     0.00\n",
      "NOTE:     11   800  0.00064            0.792     0.9424  5.509e+04       3367          0    0.02608     0.00\n",
      "NOTE:     12   800  0.00064           0.9105     0.9451  5.911e+04       3435          0    0.02608     0.00\n",
      "NOTE:     13   800  0.00064           0.9768     0.9094  5.956e+04       5931          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  56       0.0006          0.9215     0.9178  8.215e+05  7.357e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00064            1.036      0.915   6.37e+04       5917          0    0.02608     0.00\n",
      "NOTE:      1   800  0.00064           0.8981     0.9131  5.495e+04       5232          0    0.02608     0.00\n",
      "NOTE:      2   800  0.00064           0.9867     0.9246  6.448e+04       5259          0    0.02608     0.00\n",
      "NOTE:      3   800  0.00064           0.9647      0.937  6.518e+04       4380          0    0.02608     0.00\n",
      "NOTE:      4   800  0.00064           0.9288     0.8868  6.014e+04       7678          0    0.02608     0.00\n",
      "NOTE:      5   800  0.00064           0.8442     0.9044   5.58e+04       5901          0    0.02608     0.00\n",
      "NOTE:      6   800  0.00064           0.9596     0.9319  6.392e+04       4673          0    0.02608     0.00\n",
      "NOTE:      7   800  0.00064           0.8348     0.9088  5.575e+04       5592          0    0.02608     0.00\n",
      "NOTE:      8   800  0.00064           0.8989     0.9261  6.311e+04       5033          0    0.02608     0.00\n",
      "NOTE:      9   800  0.00064           0.8394     0.9179  5.659e+04       5059          0    0.02608     0.00\n",
      "NOTE:     10   800  0.00064           0.9599     0.9327   5.93e+04       4277          0    0.02608     0.00\n",
      "NOTE:     11   800  0.00064           0.8687     0.9185  6.159e+04       5468          0    0.02608     0.00\n",
      "NOTE:     12   800  0.00064           0.8351     0.9075  5.176e+04       5276          0    0.02608     0.00\n",
      "NOTE:     13   800  0.00064           0.8934      0.903  5.946e+04       6387          0    0.02608     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  57       0.0006          0.9106     0.9165  8.357e+05  7.613e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00064           0.8674     0.8938  5.297e+04       6296          0    0.02608     0.00\n",
      "NOTE:      1   800  0.00064           0.9674        0.9  6.134e+04       6812          0    0.02608     0.00\n",
      "NOTE:      2   800  0.00064           0.9027      0.924  5.987e+04       4928          0    0.02608     0.00\n",
      "NOTE:      3   800  0.00064           0.9713     0.8997  6.351e+04       7081          0    0.02607     0.00\n",
      "NOTE:      4   800  0.00064           0.9575     0.9398    6.5e+04       4167          0    0.02607     0.00\n",
      "NOTE:      5   800  0.00064           0.8862     0.9225   6.04e+04       5073          0    0.02607     0.00\n",
      "NOTE:      6   800  0.00064           0.8608     0.9286  5.843e+04       4493          0    0.02607     0.00\n",
      "NOTE:      7   800  0.00064           0.9735     0.9324  6.491e+04       4708          0    0.02607     0.00\n",
      "NOTE:      8   800  0.00064           0.8948     0.8997  5.565e+04       6206          0    0.02607     0.00\n",
      "NOTE:      9   800  0.00064           0.8905     0.9334  6.076e+04       4332          0    0.02607     0.00\n",
      "NOTE:     10   800  0.00064           0.9343     0.9514  6.385e+04       3264          0    0.02607     0.00\n",
      "NOTE:     11   800  0.00064           0.9008     0.9271   6.29e+04       4946          0    0.02607     0.00\n",
      "NOTE:     12   800  0.00064           0.9617     0.9231  5.985e+04       4989          0    0.02607     0.00\n",
      "NOTE:     13   800  0.00064           0.8595     0.9181  5.525e+04       4931          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  58       0.0006          0.9163     0.9212  8.447e+05  7.223e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00064            0.937      0.917  6.115e+04       5537          0    0.02607     0.00\n",
      "NOTE:      1   800  0.00064           0.9185     0.9269   6.19e+04       4882          0    0.02607     0.00\n",
      "NOTE:      2   800  0.00064           0.8835     0.9256  6.123e+04       4920          0    0.02607     0.00\n",
      "NOTE:      3   800  0.00064           0.9804     0.9045  6.362e+04       6720          0    0.02607     0.00\n",
      "NOTE:      4   800  0.00064           0.9964     0.9167  6.624e+04       6021          0    0.02607     0.00\n",
      "NOTE:      5   800  0.00064           0.7589     0.9189   5.19e+04       4578          0    0.02607     0.00\n",
      "NOTE:      6   800  0.00064           0.9666     0.9147  6.271e+04       5847          0    0.02607     0.00\n",
      "NOTE:      7   800  0.00064           0.7245     0.9431  5.408e+04       3262          0    0.02607     0.00\n",
      "NOTE:      8   800  0.00064           0.9238     0.9512  6.384e+04       3276          0    0.02607     0.00\n",
      "NOTE:      9   800  0.00064           0.9182      0.893  5.802e+04       6955          0    0.02607     0.00\n",
      "NOTE:     10   800  0.00064           0.9605     0.8946  5.546e+04       6534          0    0.02607     0.00\n",
      "NOTE:     11   800  0.00064            0.728     0.9266  5.159e+04       4088          0    0.02607     0.00\n",
      "NOTE:     12   800  0.00064           0.9152     0.9209  6.271e+04       5384          0    0.02607     0.00\n",
      "NOTE:     13   800  0.00064           0.7905      0.936  5.675e+04       3879          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  59       0.0006          0.8859     0.9204  8.312e+05  7.188e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00064           0.9095     0.9303  6.505e+04       4872          0    0.02607     0.00\n",
      "NOTE:      1   800  0.00064           0.9443     0.9239  5.954e+04       4906          0    0.02607     0.00\n",
      "NOTE:      2   800  0.00064           0.9385     0.9315  6.408e+04       4711          0    0.02607     0.00\n",
      "NOTE:      3   800  0.00064           0.8808     0.9039  5.757e+04       6124          0    0.02607     0.00\n",
      "NOTE:      4   800  0.00064            0.898       0.94  5.826e+04       3718          0    0.02607     0.00\n",
      "NOTE:      5   800  0.00064           0.8627     0.9277  5.638e+04       4391          0    0.02607     0.00\n",
      "NOTE:      6   800  0.00064           0.9669      0.923  6.366e+04       5309          0    0.02607     0.00\n",
      "NOTE:      7   800  0.00064           0.8923     0.9321  5.993e+04       4366          0    0.02607     0.00\n",
      "NOTE:      8   800  0.00064           0.8484     0.9205   5.34e+04       4611          0    0.02607     0.00\n",
      "NOTE:      9   800  0.00064           0.8435     0.9376  6.044e+04       4020          0    0.02607     0.00\n",
      "NOTE:     10   800  0.00064           0.8439     0.9316  5.459e+04       4009          0    0.02607     0.00\n",
      "NOTE:     11   800  0.00064           0.8901     0.9334  5.715e+04       4075          0    0.02607     0.00\n",
      "NOTE:     12   800  0.00064           0.8797     0.9328  5.733e+04       4130          0    0.02607     0.00\n",
      "NOTE:     13   800  0.00064           0.8679     0.9285  5.749e+04       4430          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  60       0.0006          0.8905     0.9283  8.249e+05  6.367e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00064            1.002     0.9085  6.406e+04       6453          0    0.02607     0.00\n",
      "NOTE:      1   800  0.00064           0.9442     0.9246  6.404e+04       5221          0    0.02607     0.00\n",
      "NOTE:      2   800  0.00064           0.9835     0.9261  6.397e+04       5103          0    0.02607     0.00\n",
      "NOTE:      3   800  0.00064           0.7445     0.9555  5.601e+04       2608          0    0.02607     0.00\n",
      "NOTE:      4   800  0.00064             0.84     0.9497  5.916e+04       3136          0    0.02607     0.00\n",
      "NOTE:      5   800  0.00064           0.9865      0.926  6.178e+04       4940          0    0.02607     0.00\n",
      "NOTE:      6   800  0.00064           0.8599     0.9012  5.246e+04       5754          0    0.02607     0.00\n",
      "NOTE:      7   800  0.00064            1.003     0.9092  6.397e+04       6389          0    0.02607     0.00\n",
      "NOTE:      8   800  0.00064           0.7995     0.9376  5.185e+04       3448          0    0.02607     0.00\n",
      "NOTE:      9   800  0.00064           0.9076     0.9251  6.283e+04       5088          0    0.02607     0.00\n",
      "NOTE:     10   800  0.00064            0.718     0.9313  5.191e+04       3828          0    0.02607     0.00\n",
      "NOTE:     11   800  0.00064            1.048     0.9082  6.765e+04       6834          0    0.02607     0.00\n",
      "NOTE:     12   800  0.00064           0.9753     0.9284  6.152e+04       4742          0    0.02607     0.00\n",
      "NOTE:     13   800  0.00064           0.8308     0.9154  5.463e+04       5051          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  61       0.0006          0.9031     0.9242  8.358e+05   6.86e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00064           0.9694     0.8975  5.637e+04       6437          0    0.02607     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      1   800  0.00064            0.935     0.9131   5.99e+04       5697          0    0.02607     0.00\n",
      "NOTE:      2   800  0.00064           0.9378     0.8857  5.913e+04       7632          0    0.02607     0.00\n",
      "NOTE:      3   800  0.00064           0.8982     0.9118  5.848e+04       5655          0    0.02607     0.00\n",
      "NOTE:      4   800  0.00064           0.8924     0.9399   5.98e+04       3824          0    0.02607     0.00\n",
      "NOTE:      5   800  0.00064           0.9044     0.9373   6.36e+04       4252          0    0.02607     0.00\n",
      "NOTE:      6   800  0.00064           0.9025     0.9035  5.569e+04       5949          0    0.02607     0.00\n",
      "NOTE:      7   800  0.00064           0.9701      0.899  6.115e+04       6869          0    0.02607     0.00\n",
      "NOTE:      8   800  0.00064           0.9086     0.9208  6.189e+04       5323          0    0.02607     0.00\n",
      "NOTE:      9   800  0.00064           0.9519     0.9243  6.137e+04       5024          0    0.02607     0.00\n",
      "NOTE:     10   800  0.00064           0.9748     0.9377  6.548e+04       4350          0    0.02607     0.00\n",
      "NOTE:     11   800  0.00064           0.9065       0.93  6.067e+04       4563          0    0.02607     0.00\n",
      "NOTE:     12   800  0.00064           0.8866     0.9408  6.197e+04       3899          0    0.02607     0.00\n",
      "NOTE:     13   800  0.00064           0.7896     0.9216  4.995e+04       4247          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  62       0.0006          0.9163     0.9189  8.354e+05  7.372e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000512            0.914     0.9284  5.927e+04       4569          0    0.02607     0.00\n",
      "NOTE:      1   800 0.000512           0.8786     0.9171  5.149e+04       4652          0    0.02607     0.00\n",
      "NOTE:      2   800 0.000512           0.9747     0.9153   6.44e+04       5963          0    0.02607     0.00\n",
      "NOTE:      3   800 0.000512           0.9145     0.9345  6.327e+04       4435          0    0.02607     0.00\n",
      "NOTE:      4   800 0.000512           0.9085     0.9065  5.996e+04       6187          0    0.02607     0.00\n",
      "NOTE:      5   800 0.000512           0.8792     0.9085  5.711e+04       5753          0    0.02607     0.00\n",
      "NOTE:      6   800 0.000512           0.9119     0.9064  5.949e+04       6146          0    0.02607     0.00\n",
      "NOTE:      7   800 0.000512           0.8679     0.9411  5.912e+04       3698          0    0.02607     0.00\n",
      "NOTE:      8   800 0.000512             1.05     0.9081  6.791e+04       6869          0    0.02607     0.00\n",
      "NOTE:      9   800 0.000512           0.8586     0.9344  6.315e+04       4437          0    0.02607     0.00\n",
      "NOTE:     10   800 0.000512            0.841     0.9138  5.909e+04       5575          0    0.02607     0.00\n",
      "NOTE:     11   800 0.000512           0.9675     0.9416  7.013e+04       4348          0    0.02607     0.00\n",
      "NOTE:     12   800 0.000512           0.9439     0.9214  5.871e+04       5005          0    0.02607     0.00\n",
      "NOTE:     13   800 0.000512           0.8777     0.9351  5.922e+04       4113          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  63       0.0005          0.9135     0.9224  8.523e+05  7.175e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000512           0.9326     0.9226  5.995e+04       5033          0    0.02607     0.00\n",
      "NOTE:      1   800 0.000512            1.063      0.907  6.798e+04       6973          0    0.02607     0.00\n",
      "NOTE:      2   800 0.000512           0.8969     0.9396  6.186e+04       3977          0    0.02607     0.00\n",
      "NOTE:      3   800 0.000512           0.8704     0.9395  5.949e+04       3832          0    0.02607     0.00\n",
      "NOTE:      4   800 0.000512           0.9075     0.9234   5.64e+04       4679          0    0.02607     0.00\n",
      "NOTE:      5   800 0.000512           0.8687     0.9416  6.225e+04       3861          0    0.02607     0.00\n",
      "NOTE:      6   800 0.000512           0.8587     0.9226  5.742e+04       4815          0    0.02607     0.00\n",
      "NOTE:      7   800 0.000512           0.7176     0.9277  4.751e+04       3704          0    0.02607     0.00\n",
      "NOTE:      8   800 0.000512           0.8939     0.9346  6.251e+04       4371          0    0.02607     0.00\n",
      "NOTE:      9   800 0.000512           0.7812     0.9413  5.578e+04       3477          0    0.02607     0.00\n",
      "NOTE:     10   800 0.000512           0.8646     0.9092  5.866e+04       5857          0    0.02607     0.00\n",
      "NOTE:     11   800 0.000512           0.8496     0.9342  5.651e+04       3983          0    0.02607     0.00\n",
      "NOTE:     12   800 0.000512           0.8295     0.9486  6.185e+04       3350          0    0.02607     0.00\n",
      "NOTE:     13   800 0.000512           0.8931     0.8934  5.342e+04       6372          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  64       0.0005          0.8734     0.9274  8.216e+05  6.428e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000512           0.9115     0.9413  6.184e+04       3857          0    0.02607     0.00\n",
      "NOTE:      1   800 0.000512           0.9666     0.8953  5.946e+04       6954          0    0.02607     0.00\n",
      "NOTE:      2   800 0.000512           0.9819     0.9136  6.358e+04       6012          0    0.02607     0.00\n",
      "NOTE:      3   800 0.000512           0.8598     0.9478   5.84e+04       3214          0    0.02607     0.00\n",
      "NOTE:      4   800 0.000512           0.8583     0.9085  5.334e+04       5374          0    0.02607     0.00\n",
      "NOTE:      5   800 0.000512           0.8777      0.923  5.668e+04       4726          0    0.02607     0.00\n",
      "NOTE:      6   800 0.000512           0.9437     0.9249   6.41e+04       5202          0    0.02607     0.00\n",
      "NOTE:      7   800 0.000512            1.012     0.8917  6.606e+04       8025          0    0.02607     0.00\n",
      "NOTE:      8   800 0.000512           0.9299     0.9255  5.973e+04       4810          0    0.02607     0.00\n",
      "NOTE:      9   800 0.000512           0.8926     0.9294  5.872e+04       4462          0    0.02607     0.00\n",
      "NOTE:     10   800 0.000512           0.8776      0.912  5.639e+04       5438          0    0.02607     0.00\n",
      "NOTE:     11   800 0.000512            0.866     0.9231  5.645e+04       4704          0    0.02607     0.00\n",
      "NOTE:     12   800 0.000512           0.8805     0.9291  6.083e+04       4639          0    0.02607     0.00\n",
      "NOTE:     13   800 0.000512           0.8736     0.9362   5.68e+04       3871          0    0.02607     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  65       0.0005          0.9094     0.9211  8.324e+05  7.129e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000512           0.8339     0.9246  5.618e+04       4579          0    0.02607     0.00\n",
      "NOTE:      1   800 0.000512           0.9842     0.9306  6.613e+04       4928          0    0.02606     0.00\n",
      "NOTE:      2   800 0.000512           0.8236     0.9456  5.968e+04       3432          0    0.02606     0.00\n",
      "NOTE:      3   800 0.000512           0.9473     0.9049  5.615e+04       5903          0    0.02606     0.00\n",
      "NOTE:      4   800 0.000512           0.8872     0.9299  5.811e+04       4378          0    0.02606     0.00\n",
      "NOTE:      5   800 0.000512           0.8929     0.9256  5.342e+04       4293          0    0.02606     0.00\n",
      "NOTE:      6   800 0.000512           0.8988     0.9182  5.767e+04       5139          0    0.02606     0.00\n",
      "NOTE:      7   800 0.000512             1.06     0.9274  6.818e+04       5339          0    0.02606     0.00\n",
      "NOTE:      8   800 0.000512           0.8131     0.9541  5.904e+04       2838          0    0.02606     0.00\n",
      "NOTE:      9   800 0.000512            0.981     0.9262  6.397e+04       5094          0    0.02606     0.00\n",
      "NOTE:     10   800 0.000512           0.9635     0.9243  6.442e+04       5274          0    0.02606     0.00\n",
      "NOTE:     11   800 0.000512            1.025     0.9291  6.687e+04       5106          0    0.02606     0.00\n",
      "NOTE:     12   800 0.000512           0.9766     0.9271  6.167e+04       4852          0    0.02606     0.00\n",
      "NOTE:     13   800 0.000512           0.9896     0.9244  6.732e+04       5507          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  66       0.0005          0.9341      0.928  8.588e+05  6.666e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000512           0.9005     0.9408  6.268e+04       3942          0    0.02606     0.00\n",
      "NOTE:      1   800 0.000512           0.8715      0.922  5.815e+04       4918          0    0.02606     0.00\n",
      "NOTE:      2   800 0.000512           0.9917     0.8932  5.551e+04       6637          0    0.02606     0.00\n",
      "NOTE:      3   800 0.000512           0.9148      0.917   5.95e+04       5384          0    0.02606     0.00\n",
      "NOTE:      4   800 0.000512           0.9268     0.9205  6.052e+04       5227          0    0.02606     0.00\n",
      "NOTE:      5   800 0.000512           0.8234     0.9306   5.54e+04       4134          0    0.02606     0.00\n",
      "NOTE:      6   800 0.000512            0.872     0.9016  5.544e+04       6053          0    0.02606     0.00\n",
      "NOTE:      7   800 0.000512           0.9343     0.9327  6.155e+04       4443          0    0.02606     0.00\n",
      "NOTE:      8   800 0.000512           0.8805     0.9281  6.011e+04       4654          0    0.02606     0.00\n",
      "NOTE:      9   800 0.000512           0.9614     0.9484  6.625e+04       3608          0    0.02606     0.00\n",
      "NOTE:     10   800 0.000512           0.8421     0.9203  5.555e+04       4811          0    0.02606     0.00\n",
      "NOTE:     11   800 0.000512           0.9286      0.921  6.067e+04       5203          0    0.02606     0.00\n",
      "NOTE:     12   800 0.000512           0.8651     0.9149  5.632e+04       5239          0    0.02606     0.00\n",
      "NOTE:     13   800 0.000512           0.8397     0.9282  5.408e+04       4181          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  67       0.0005          0.8966     0.9231  8.217e+05  6.843e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000512           0.8701     0.9127  5.876e+04       5618          0    0.02606     0.00\n",
      "NOTE:      1   800 0.000512           0.8671     0.9043  5.292e+04       5603          0    0.02606     0.00\n",
      "NOTE:      2   800 0.000512           0.9408     0.9265  6.215e+04       4932          0    0.02606     0.00\n",
      "NOTE:      3   800 0.000512           0.9093     0.9465  6.314e+04       3572          0    0.02606     0.00\n",
      "NOTE:      4   800 0.000512           0.9136     0.9332  6.479e+04       4639          0    0.02606     0.00\n",
      "NOTE:      5   800 0.000512           0.9866     0.9266   6.28e+04       4977          0    0.02606     0.00\n",
      "NOTE:      6   800 0.000512           0.9579     0.9376  6.317e+04       4204          0    0.02606     0.00\n",
      "NOTE:      7   800 0.000512           0.8764     0.9372   5.96e+04       3991          0    0.02606     0.00\n",
      "NOTE:      8   800 0.000512           0.8769     0.9245  5.676e+04       4635          0    0.02606     0.00\n",
      "NOTE:      9   800 0.000512           0.8564      0.912  5.116e+04       4937          0    0.02606     0.00\n",
      "NOTE:     10   800 0.000512           0.8339     0.9276  5.626e+04       4388          0    0.02606     0.00\n",
      "NOTE:     11   800 0.000512           0.9198     0.9229  6.033e+04       5037          0    0.02606     0.00\n",
      "NOTE:     12   800 0.000512           0.9586     0.9236  6.614e+04       5468          0    0.02606     0.00\n",
      "NOTE:     13   800 0.000512           0.8004     0.8962  5.231e+04       6060          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  68       0.0005          0.8977     0.9242  8.303e+05  6.806e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000512           0.8135     0.9286  5.936e+04       4564          0    0.02606     0.00\n",
      "NOTE:      1   800 0.000512           0.7907     0.9429  5.657e+04       3428          0    0.02606     0.00\n",
      "NOTE:      2   800 0.000512           0.9258     0.9344  6.082e+04       4272          0    0.02606     0.00\n",
      "NOTE:      3   800 0.000512           0.9898       0.91  6.437e+04       6365          0    0.02606     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      4   800 0.000512           0.8457     0.9334  5.678e+04       4051          0    0.02606     0.00\n",
      "NOTE:      5   800 0.000512            1.014     0.9305  6.712e+04       5014          0    0.02606     0.00\n",
      "NOTE:      6   800 0.000512           0.9235     0.9165  6.124e+04       5581          0    0.02606     0.00\n",
      "NOTE:      7   800 0.000512           0.8325       0.91  5.839e+04       5774          0    0.02606     0.00\n",
      "NOTE:      8   800 0.000512           0.8063     0.9333  5.826e+04       4161          0    0.02606     0.00\n",
      "NOTE:      9   800 0.000512            1.023     0.9031  6.214e+04       6667          0    0.02606     0.00\n",
      "NOTE:     10   800 0.000512            0.796     0.9094  5.602e+04       5583          0    0.02606     0.00\n",
      "NOTE:     11   800 0.000512           0.7614     0.9521  5.323e+04       2679          0    0.02606     0.00\n",
      "NOTE:     12   800 0.000512           0.9506     0.9316  6.239e+04       4581          0    0.02606     0.00\n",
      "NOTE:     13   800 0.000512           0.7954     0.9371  5.284e+04       3548          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  69       0.0005          0.8763      0.926  8.295e+05  6.627e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000512           0.8276     0.9541  5.818e+04       2798          0    0.02606     0.00\n",
      "NOTE:      1   800 0.000512           0.9679     0.9107  5.999e+04       5883          0    0.02606     0.00\n",
      "NOTE:      2   800 0.000512           0.8173     0.9174  5.184e+04       4670          0    0.02606     0.00\n",
      "NOTE:      3   800 0.000512           0.9256     0.9346  6.122e+04       4285          0    0.02606     0.00\n",
      "NOTE:      4   800 0.000512           0.8683     0.9192  5.348e+04       4701          0    0.02606     0.00\n",
      "NOTE:      5   800 0.000512           0.9413     0.9482  6.695e+04       3656          0    0.02606     0.00\n",
      "NOTE:      6   800 0.000512            0.921     0.9212  6.018e+04       5150          0    0.02606     0.00\n",
      "NOTE:      7   800 0.000512           0.8957     0.9237  6.078e+04       5021          0    0.02606     0.00\n",
      "NOTE:      8   800 0.000512           0.8692     0.9367  5.969e+04       4031          0    0.02606     0.00\n",
      "NOTE:      9   800 0.000512           0.8011     0.9506  5.561e+04       2891          0    0.02606     0.00\n",
      "NOTE:     10   800 0.000512            1.014      0.938  6.766e+04       4476          0    0.02606     0.00\n",
      "NOTE:     11   800 0.000512           0.8959     0.9128  5.806e+04       5546          0    0.02606     0.00\n",
      "NOTE:     12   800 0.000512            1.068      0.915   6.95e+04       6460          0    0.02606     0.00\n",
      "NOTE:     13   800 0.000512           0.9134     0.9244  6.043e+04       4945          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  70       0.0005           0.909      0.929  8.436e+05  6.451e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000512           0.8524     0.9392  5.848e+04       3787          0    0.02606     0.00\n",
      "NOTE:      1   800 0.000512           0.9118     0.8992  5.814e+04       6518          0    0.02606     0.00\n",
      "NOTE:      2   800 0.000512           0.8159     0.8981  5.675e+04       6437          0    0.02606     0.00\n",
      "NOTE:      3   800 0.000512           0.8157     0.9416  6.058e+04       3759          0    0.02606     0.00\n",
      "NOTE:      4   800 0.000512           0.9639     0.9266  6.545e+04       5184          0    0.02606     0.00\n",
      "NOTE:      5   800 0.000512           0.9089     0.9325  6.019e+04       4354          0    0.02606     0.00\n",
      "NOTE:      6   800 0.000512           0.9137     0.9336  6.067e+04       4314          0    0.02606     0.00\n",
      "NOTE:      7   800 0.000512           0.8429     0.9274  6.005e+04       4698          0    0.02606     0.00\n",
      "NOTE:      8   800 0.000512           0.8374     0.9146  5.516e+04       5151          0    0.02606     0.00\n",
      "NOTE:      9   800 0.000512           0.9119     0.9418  5.804e+04       3584          0    0.02606     0.00\n",
      "NOTE:     10   800 0.000512           0.9191     0.9359   6.19e+04       4240          0    0.02606     0.00\n",
      "NOTE:     11   800 0.000512           0.7533     0.9414  5.228e+04       3255          0    0.02606     0.00\n",
      "NOTE:     12   800 0.000512           0.9141     0.9183  5.689e+04       5059          0    0.02606     0.00\n",
      "NOTE:     13   800 0.000512           0.8596     0.9247  5.571e+04       4536          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  71       0.0005          0.8729     0.9267  8.203e+05  6.488e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00041            1.034     0.8926  6.213e+04       7478          0    0.02606     0.00\n",
      "NOTE:      1   800  0.00041           0.8581     0.9475  5.762e+04       3190          0    0.02606     0.00\n",
      "NOTE:      2   800  0.00041           0.9034     0.9305  5.869e+04       4385          0    0.02606     0.00\n",
      "NOTE:      3   800  0.00041           0.8037     0.8798  4.872e+04       6656          0    0.02606     0.00\n",
      "NOTE:      4   800  0.00041           0.8877     0.9015  5.725e+04       6252          0    0.02606     0.00\n",
      "NOTE:      5   800  0.00041           0.8456     0.9286  5.415e+04       4164          0    0.02606     0.00\n",
      "NOTE:      6   800  0.00041           0.9706      0.921  6.516e+04       5587          0    0.02606     0.00\n",
      "NOTE:      7   800  0.00041           0.9376     0.9045  6.313e+04       6665          0    0.02606     0.00\n",
      "NOTE:      8   800  0.00041            0.872     0.9225  5.772e+04       4851          0    0.02606     0.00\n",
      "NOTE:      9   800  0.00041           0.8194     0.9454  5.902e+04       3406          0    0.02606     0.00\n",
      "NOTE:     10   800  0.00041           0.8698     0.9212  5.601e+04       4789          0    0.02606     0.00\n",
      "NOTE:     11   800  0.00041            0.977     0.9255  6.889e+04       5544          0    0.02606     0.00\n",
      "NOTE:     12   800  0.00041           0.8952     0.9146  5.627e+04       5256          0    0.02606     0.00\n",
      "NOTE:     13   800  0.00041           0.7315     0.9203  4.978e+04       4310          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  72       0.0004          0.8861     0.9182  8.145e+05  7.253e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00041           0.8551     0.9357  5.697e+04       3914          0    0.02606     0.00\n",
      "NOTE:      1   800  0.00041           0.9592     0.8998  6.289e+04       7000          0    0.02606     0.00\n",
      "NOTE:      2   800  0.00041           0.9345     0.9368   6.08e+04       4101          0    0.02606     0.00\n",
      "NOTE:      3   800  0.00041            1.035      0.914  6.286e+04       5917          0    0.02606     0.00\n",
      "NOTE:      4   800  0.00041           0.9556     0.9366  6.311e+04       4273          0    0.02606     0.00\n",
      "NOTE:      5   800  0.00041           0.8382     0.9146  5.681e+04       5306          0    0.02606     0.00\n",
      "NOTE:      6   800  0.00041            1.029     0.9135  6.961e+04       6588          0    0.02606     0.00\n",
      "NOTE:      7   800  0.00041           0.9753     0.8915  6.213e+04       7564          0    0.02606     0.00\n",
      "NOTE:      8   800  0.00041            0.824     0.9001  5.242e+04       5821          0    0.02606     0.00\n",
      "NOTE:      9   800  0.00041           0.9759     0.9391  6.447e+04       4183          0    0.02606     0.00\n",
      "NOTE:     10   800  0.00041           0.8881     0.9401  6.239e+04       3978          0    0.02606     0.00\n",
      "NOTE:     11   800  0.00041           0.7983      0.915  5.431e+04       5047          0    0.02606     0.00\n",
      "NOTE:     12   800  0.00041           0.9113     0.9233  5.619e+04       4668          0    0.02606     0.00\n",
      "NOTE:     13   800  0.00041           0.9019     0.9195   5.94e+04       5201          0    0.02606     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  73       0.0004          0.9201     0.9199  8.444e+05  7.356e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00041           0.9366     0.9477  6.712e+04       3705          0    0.02606     0.00\n",
      "NOTE:      1   800  0.00041            0.973     0.9286  6.607e+04       5083          0    0.02606     0.00\n",
      "NOTE:      2   800  0.00041           0.9044      0.862  5.195e+04       8319          0    0.02606     0.00\n",
      "NOTE:      3   800  0.00041           0.9751     0.9116  6.243e+04       6052          0    0.02606     0.00\n",
      "NOTE:      4   800  0.00041           0.8507     0.9352  5.911e+04       4093          0    0.02606     0.00\n",
      "NOTE:      5   800  0.00041           0.8052      0.943  5.638e+04       3408          0    0.02606     0.00\n",
      "NOTE:      6   800  0.00041           0.9643     0.9189  6.297e+04       5557          0    0.02606     0.00\n",
      "NOTE:      7   800  0.00041           0.8525      0.918  5.969e+04       5335          0    0.02606     0.00\n",
      "NOTE:      8   800  0.00041           0.9833     0.9122  6.139e+04       5912          0    0.02605     0.00\n",
      "NOTE:      9   800  0.00041           0.9232     0.9211  6.077e+04       5209          0    0.02605     0.00\n",
      "NOTE:     10   800  0.00041           0.9744     0.8951  5.834e+04       6838          0    0.02605     0.00\n",
      "NOTE:     11   800  0.00041           0.8817     0.9156  5.719e+04       5271          0    0.02605     0.00\n",
      "NOTE:     12   800  0.00041            1.005     0.9108  5.829e+04       5707          0    0.02605     0.00\n",
      "NOTE:     13   800  0.00041           0.9326     0.9371  6.084e+04       4083          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  74       0.0004          0.9259     0.9187  8.425e+05  7.457e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00041            1.019     0.8876  5.864e+04       7423          0    0.02605     0.00\n",
      "NOTE:      1   800  0.00041           0.9312     0.9465  6.314e+04       3568          0    0.02605     0.00\n",
      "NOTE:      2   800  0.00041            1.042      0.894  6.377e+04       7557          0    0.02605     0.00\n",
      "NOTE:      3   800  0.00041           0.8691     0.9209  5.903e+04       5071          0    0.02605     0.00\n",
      "NOTE:      4   800  0.00041           0.8441     0.9186  5.929e+04       5252          0    0.02605     0.00\n",
      "NOTE:      5   800  0.00041           0.9118     0.9215  6.271e+04       5340          0    0.02605     0.00\n",
      "NOTE:      6   800  0.00041            0.839     0.9384  5.873e+04       3853          0    0.02605     0.00\n",
      "NOTE:      7   800  0.00041           0.8836     0.9421  5.795e+04       3562          0    0.02605     0.00\n",
      "NOTE:      8   800  0.00041           0.8039     0.9284   5.36e+04       4133          0    0.02605     0.00\n",
      "NOTE:      9   800  0.00041           0.9306     0.9303  5.975e+04       4477          0    0.02605     0.00\n",
      "NOTE:     10   800  0.00041           0.9806     0.9078  6.487e+04       6592          0    0.02605     0.00\n",
      "NOTE:     11   800  0.00041           0.8434     0.9375  5.513e+04       3678          0    0.02605     0.00\n",
      "NOTE:     12   800  0.00041           0.9347     0.9206  6.302e+04       5437          0    0.02605     0.00\n",
      "NOTE:     13   800  0.00041           0.8658     0.9329  6.166e+04       4434          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  75       0.0004          0.9071     0.9228  8.413e+05  7.038e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00041           0.8662      0.934  5.831e+04       4123          0    0.02605     0.00\n",
      "NOTE:      1   800  0.00041           0.7983      0.939  5.344e+04       3471          0    0.02605     0.00\n",
      "NOTE:      2   800  0.00041           0.9464     0.9174   5.99e+04       5391          0    0.02605     0.00\n",
      "NOTE:      3   800  0.00041           0.9138     0.8979  6.037e+04       6862          0    0.02605     0.00\n",
      "NOTE:      4   800  0.00041            1.033     0.9294  6.744e+04       5122          0    0.02605     0.00\n",
      "NOTE:      5   800  0.00041            0.833     0.9059  5.364e+04       5574          0    0.02605     0.00\n",
      "NOTE:      6   800  0.00041           0.8211      0.928  5.597e+04       4345          0    0.02605     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      7   800  0.00041           0.8644     0.9132  5.554e+04       5278          0    0.02605     0.00\n",
      "NOTE:      8   800  0.00041           0.8638     0.9246   5.89e+04       4804          0    0.02605     0.00\n",
      "NOTE:      9   800  0.00041           0.9603     0.9141  6.341e+04       5955          0    0.02605     0.00\n",
      "NOTE:     10   800  0.00041           0.8732     0.9337  5.492e+04       3899          0    0.02605     0.00\n",
      "NOTE:     11   800  0.00041           0.8869     0.9321  6.125e+04       4458          0    0.02605     0.00\n",
      "NOTE:     12   800  0.00041           0.9779     0.9296  6.546e+04       4958          0    0.02605     0.00\n",
      "NOTE:     13   800  0.00041           0.9196     0.9089    5.6e+04       5611          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  76       0.0004           0.897     0.9219  8.245e+05  6.985e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00041           0.9119     0.9122  5.763e+04       5548          0    0.02605     0.00\n",
      "NOTE:      1   800  0.00041            0.716     0.9377  4.969e+04       3302          0    0.02605     0.00\n",
      "NOTE:      2   800  0.00041           0.9949      0.878  5.907e+04       8205          0    0.02605     0.00\n",
      "NOTE:      3   800  0.00041           0.9355     0.9365  6.411e+04       4348          0    0.02605     0.00\n",
      "NOTE:      4   800  0.00041           0.7982     0.9383   5.53e+04       3637          0    0.02605     0.00\n",
      "NOTE:      5   800  0.00041           0.9578     0.9151   5.61e+04       5207          0    0.02605     0.00\n",
      "NOTE:      6   800  0.00041            1.029     0.8926  6.445e+04       7756          0    0.02605     0.00\n",
      "NOTE:      7   800  0.00041           0.9126     0.9415  6.147e+04       3819          0    0.02605     0.00\n",
      "NOTE:      8   800  0.00041           0.9426     0.9366   6.04e+04       4091          0    0.02605     0.00\n",
      "NOTE:      9   800  0.00041           0.8628      0.942  5.798e+04       3571          0    0.02605     0.00\n",
      "NOTE:     10   800  0.00041            0.784     0.9267  5.171e+04       4091          0    0.02605     0.00\n",
      "NOTE:     11   800  0.00041           0.9054     0.9111  6.102e+04       5955          0    0.02605     0.00\n",
      "NOTE:     12   800  0.00041           0.8493     0.9289   5.82e+04       4454          0    0.02605     0.00\n",
      "NOTE:     13   800  0.00041           0.7449     0.9307  5.281e+04       3930          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  77       0.0004          0.8818     0.9226  8.099e+05  6.791e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00041           0.9413     0.9302  6.366e+04       4776          0    0.02605     0.00\n",
      "NOTE:      1   800  0.00041           0.8129     0.9047  5.147e+04       5422          0    0.02605     0.00\n",
      "NOTE:      2   800  0.00041           0.7474     0.9257  5.179e+04       4155          0    0.02605     0.00\n",
      "NOTE:      3   800  0.00041           0.9148     0.9186  6.035e+04       5345          0    0.02605     0.00\n",
      "NOTE:      4   800  0.00041           0.8997     0.9329  6.357e+04       4571          0    0.02605     0.00\n",
      "NOTE:      5   800  0.00041           0.9273     0.9439  5.759e+04       3422          0    0.02605     0.00\n",
      "NOTE:      6   800  0.00041           0.9237     0.9317  6.562e+04       4808          0    0.02605     0.00\n",
      "NOTE:      7   800  0.00041           0.8165     0.9184  5.293e+04       4705          0    0.02605     0.00\n",
      "NOTE:      8   800  0.00041           0.9371     0.9206  6.165e+04       5321          0    0.02605     0.00\n",
      "NOTE:      9   800  0.00041           0.8644     0.9268  5.604e+04       4424          0    0.02605     0.00\n",
      "NOTE:     10   800  0.00041            0.823     0.9244  5.117e+04       4183          0    0.02605     0.00\n",
      "NOTE:     11   800  0.00041           0.8994     0.9076   5.43e+04       5528          0    0.02605     0.00\n",
      "NOTE:     12   800  0.00041           0.9039      0.934  6.313e+04       4459          0    0.02605     0.00\n",
      "NOTE:     13   800  0.00041           0.8314     0.9097  5.425e+04       5388          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  78       0.0004          0.8745     0.9239  8.075e+05  6.651e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00041           0.8264     0.9199  5.715e+04       4978          0    0.02605     0.00\n",
      "NOTE:      1   800  0.00041           0.9504     0.9278  6.548e+04       5096          0    0.02605     0.00\n",
      "NOTE:      2   800  0.00041           0.8906     0.9566  6.006e+04       2726          0    0.02605     0.00\n",
      "NOTE:      3   800  0.00041           0.8683     0.9355  6.169e+04       4255          0    0.02605     0.00\n",
      "NOTE:      4   800  0.00041           0.9457     0.9385  6.407e+04       4198          0    0.02605     0.00\n",
      "NOTE:      5   800  0.00041           0.8364     0.9128  5.299e+04       5060          0    0.02605     0.00\n",
      "NOTE:      6   800  0.00041           0.8827     0.9279   5.87e+04       4561          0    0.02605     0.00\n",
      "NOTE:      7   800  0.00041            0.894     0.9116  5.805e+04       5631          0    0.02605     0.00\n",
      "NOTE:      8   800  0.00041           0.9589     0.9225  6.208e+04       5218          0    0.02605     0.00\n",
      "NOTE:      9   800  0.00041           0.8713     0.9287  5.881e+04       4514          0    0.02605     0.00\n",
      "NOTE:     10   800  0.00041           0.8797     0.9417  6.374e+04       3946          0    0.02605     0.00\n",
      "NOTE:     11   800  0.00041           0.9296     0.9274  6.371e+04       4990          0    0.02605     0.00\n",
      "NOTE:     12   800  0.00041           0.7139     0.9456  5.097e+04       2933          0    0.02605     0.00\n",
      "NOTE:     13   800  0.00041            0.895     0.9116  6.215e+04       6030          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  79       0.0004          0.8816      0.929  8.397e+05  6.414e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00041             1.06     0.9077  6.492e+04       6602          0    0.02605     0.00\n",
      "NOTE:      1   800  0.00041           0.9599     0.9227  6.552e+04       5490          0    0.02605     0.00\n",
      "NOTE:      2   800  0.00041           0.8803     0.9114  6.014e+04       5845          0    0.02605     0.00\n",
      "NOTE:      3   800  0.00041           0.8782     0.9387  6.149e+04       4012          0    0.02605     0.00\n",
      "NOTE:      4   800  0.00041           0.8769     0.9438  6.039e+04       3598          0    0.02605     0.00\n",
      "NOTE:      5   800  0.00041           0.8684     0.9428  6.194e+04       3758          0    0.02605     0.00\n",
      "NOTE:      6   800  0.00041           0.9003     0.9122  5.278e+04       5082          0    0.02605     0.00\n",
      "NOTE:      7   800  0.00041           0.8775     0.9321  6.039e+04       4402          0    0.02605     0.00\n",
      "NOTE:      8   800  0.00041           0.7907     0.9315  5.508e+04       4051          0    0.02605     0.00\n",
      "NOTE:      9   800  0.00041            0.964     0.9027  6.322e+04       6814          0    0.02605     0.00\n",
      "NOTE:     10   800  0.00041           0.9133     0.9403  6.458e+04       4100          0    0.02605     0.00\n",
      "NOTE:     11   800  0.00041           0.8466     0.9402  5.835e+04       3709          0    0.02605     0.00\n",
      "NOTE:     12   800  0.00041           0.9697     0.9246  5.988e+04       4882          0    0.02605     0.00\n",
      "NOTE:     13   800  0.00041           0.8094      0.926  5.258e+04       4199          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  80       0.0004          0.8997     0.9267  8.412e+05  6.654e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000328           0.9289     0.9522  6.751e+04       3389          0    0.02605     0.00\n",
      "NOTE:      1   800 0.000328           0.8519     0.9237  6.036e+04       4986          0    0.02605     0.00\n",
      "NOTE:      2   800 0.000328            0.795     0.9514  5.837e+04       2984          0    0.02605     0.00\n",
      "NOTE:      3   800 0.000328           0.9383     0.9193  5.864e+04       5145          0    0.02605     0.00\n",
      "NOTE:      4   800 0.000328           0.8022     0.9192  5.772e+04       5073          0    0.02605     0.00\n",
      "NOTE:      5   800 0.000328            1.003     0.9176  6.635e+04       5959          0    0.02605     0.00\n",
      "NOTE:      6   800 0.000328           0.8657     0.8945  5.345e+04       6302          0    0.02605     0.00\n",
      "NOTE:      7   800 0.000328           0.8547     0.9476  5.847e+04       3236          0    0.02605     0.00\n",
      "NOTE:      8   800 0.000328           0.8036     0.9286  5.424e+04       4168          0    0.02605     0.00\n",
      "NOTE:      9   800 0.000328           0.7655     0.9268  5.772e+04       4561          0    0.02605     0.00\n",
      "NOTE:     10   800 0.000328            0.971     0.9316  6.214e+04       4563          0    0.02605     0.00\n",
      "NOTE:     11   800 0.000328            0.949     0.9053  5.881e+04       6153          0    0.02605     0.00\n",
      "NOTE:     12   800 0.000328           0.7507     0.9206  5.251e+04       4530          0    0.02605     0.00\n",
      "NOTE:     13   800 0.000328           0.9166     0.9447  6.354e+04       3717          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  81       0.0003          0.8711     0.9276  8.298e+05  6.477e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000328           0.9268     0.9352  5.913e+04       4097          0    0.02605     0.00\n",
      "NOTE:      1   800 0.000328           0.9125     0.8921  5.766e+04       6971          0    0.02605     0.00\n",
      "NOTE:      2   800 0.000328            0.961     0.8903  6.182e+04       7615          0    0.02605     0.00\n",
      "NOTE:      3   800 0.000328           0.9282     0.9282  6.515e+04       5039          0    0.02605     0.00\n",
      "NOTE:      4   800 0.000328           0.7913     0.9367  5.325e+04       3600          0    0.02605     0.00\n",
      "NOTE:      5   800 0.000328           0.8839     0.9001  5.181e+04       5749          0    0.02605     0.00\n",
      "NOTE:      6   800 0.000328           0.8713     0.9212  5.755e+04       4925          0    0.02605     0.00\n",
      "NOTE:      7   800 0.000328           0.8864     0.9394  5.629e+04       3631          0    0.02605     0.00\n",
      "NOTE:      8   800 0.000328           0.8866     0.9122  5.993e+04       5770          0    0.02605     0.00\n",
      "NOTE:      9   800 0.000328            1.007     0.8977  6.058e+04       6902          0    0.02605     0.00\n",
      "NOTE:     10   800 0.000328            0.834     0.9442  5.869e+04       3470          0    0.02605     0.00\n",
      "NOTE:     11   800 0.000328           0.8888     0.9215  5.786e+04       4927          0    0.02605     0.00\n",
      "NOTE:     12   800 0.000328           0.8452     0.9419  5.856e+04       3614          0    0.02605     0.00\n",
      "NOTE:     13   800 0.000328           0.8309     0.9558  6.009e+04       2782          0    0.02605     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  82       0.0003          0.8896     0.9221  8.184e+05  6.909e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000328           0.9484     0.8989  5.855e+04       6584          0    0.02605     0.00\n",
      "NOTE:      1   800 0.000328           0.8362     0.9234  5.809e+04       4820          0    0.02605     0.00\n",
      "NOTE:      2   800 0.000328           0.7743      0.953  5.747e+04       2837          0    0.02605     0.00\n",
      "NOTE:      3   800 0.000328           0.9099     0.9308  5.809e+04       4317          0    0.02605     0.00\n",
      "NOTE:      4   800 0.000328           0.7624     0.8873  4.914e+04       6240          0    0.02604     0.00\n",
      "NOTE:      5   800 0.000328           0.9784     0.9322  6.513e+04       4735          0    0.02604     0.00\n",
      "NOTE:      6   800 0.000328           0.8177     0.9422  5.868e+04       3598          0    0.02604     0.00\n",
      "NOTE:      7   800 0.000328           0.9125     0.8988  5.636e+04       6349          0    0.02604     0.00\n",
      "NOTE:      8   800 0.000328           0.8622     0.9337  6.106e+04       4335          0    0.02604     0.00\n",
      "NOTE:      9   800 0.000328           0.9503     0.9137  5.806e+04       5487          0    0.02604     0.00\n",
      "NOTE:     10   800 0.000328           0.8923     0.9128  6.166e+04       5893          0    0.02604     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     11   800 0.000328           0.8682     0.9202  5.493e+04       4761          0    0.02604     0.00\n",
      "NOTE:     12   800 0.000328            1.019     0.9067  6.883e+04       7087          0    0.02604     0.00\n",
      "NOTE:     13   800 0.000328           0.8551     0.9059  5.239e+04       5445          0    0.02604     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  83       0.0003          0.8848     0.9186  8.184e+05  7.249e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000328           0.8844     0.9047  5.445e+04       5734          0    0.02604     0.00\n",
      "NOTE:      1   800 0.000328           0.9232     0.9183  6.173e+04       5491          0    0.02604     0.00\n",
      "NOTE:      2   800 0.000328           0.8373     0.9517  6.083e+04       3085          0    0.02604     0.00\n",
      "NOTE:      3   800 0.000328           0.9459     0.9308  6.404e+04       4764          0    0.02604     0.00\n",
      "NOTE:      4   800 0.000328            0.883     0.9275  5.823e+04       4549          0    0.02604     0.00\n",
      "NOTE:      5   800 0.000328           0.8595     0.9318  5.688e+04       4165          0    0.02604     0.00\n",
      "NOTE:      6   800 0.000328           0.9694     0.9177  6.518e+04       5842          0    0.02604     0.00\n",
      "NOTE:      7   800 0.000328            0.916     0.9261  5.496e+04       4383          0    0.02604     0.00\n",
      "NOTE:      8   800 0.000328           0.7492     0.9351  5.151e+04       3574          0    0.02604     0.00\n",
      "NOTE:      9   800 0.000328           0.9009     0.9036  5.865e+04       6255          0    0.02604     0.00\n",
      "NOTE:     10   800 0.000328            1.051     0.9488   7.32e+04       3946          0    0.02604     0.00\n",
      "NOTE:     11   800 0.000328           0.9589     0.9248  6.466e+04       5261          0    0.02604     0.00\n",
      "NOTE:     12   800 0.000328           0.8775     0.9204  5.874e+04       5079          0    0.02604     0.00\n",
      "NOTE:     13   800 0.000328            1.007      0.919  6.597e+04       5817          0    0.02604     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  84       0.0003          0.9117     0.9259   8.49e+05  6.795e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000328           0.9395     0.9292  6.415e+04       4887          0    0.02604     0.00\n",
      "NOTE:      1   800 0.000328           0.9674     0.9187  6.279e+04       5554          0    0.02604     0.00\n",
      "NOTE:      2   800 0.000328            1.013     0.9235  6.755e+04       5595          0    0.02604     0.00\n",
      "NOTE:      3   800 0.000328           0.9389     0.9493   6.24e+04       3330          0    0.02604     0.00\n",
      "NOTE:      4   800 0.000328           0.9526     0.9271  6.345e+04       4986          0    0.02604     0.00\n",
      "NOTE:      5   800 0.000328           0.8811     0.9218  5.874e+04       4984          0    0.02604     0.00\n",
      "NOTE:      6   800 0.000328            1.005     0.9131  5.783e+04       5503          0    0.02604     0.00\n",
      "NOTE:      7   800 0.000328            0.856     0.9132  5.507e+04       5231          0    0.02604     0.00\n",
      "NOTE:      8   800 0.000328           0.7726     0.9377  5.268e+04       3499          0    0.02604     0.00\n",
      "NOTE:      9   800 0.000328           0.9013     0.9255  6.011e+04       4836          0    0.02604     0.00\n",
      "NOTE:     10   800 0.000328           0.9062     0.9297  5.921e+04       4478          0    0.02604     0.00\n",
      "NOTE:     11   800 0.000328           0.8621     0.9059  5.788e+04       6016          0    0.02604     0.00\n",
      "NOTE:     12   800 0.000328           0.8985     0.9185  5.873e+04       5211          0    0.02604     0.00\n",
      "NOTE:     13   800 0.000328           0.8818     0.9199  5.438e+04       4735          0    0.02604     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  85       0.0003          0.9125     0.9238   8.35e+05  6.885e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000328           0.8777     0.9159  5.816e+04       5339          0    0.02604     0.00\n",
      "NOTE:      1   800 0.000328           0.9833     0.9176   6.17e+04       5540          0    0.02604     0.00\n",
      "NOTE:      2   800 0.000328            0.845     0.9134   5.51e+04       5223          0    0.02604     0.00\n",
      "NOTE:      3   800 0.000328           0.8735     0.9304  5.682e+04       4253          0    0.02604     0.00\n",
      "NOTE:      4   800 0.000328           0.9758     0.8918  5.706e+04       6920          0    0.02604     0.00\n",
      "NOTE:      5   800 0.000328           0.9214     0.9165   5.95e+04       5421          0    0.02604     0.00\n",
      "NOTE:      6   800 0.000328           0.8419     0.9161  5.522e+04       5060          0    0.02604     0.00\n",
      "NOTE:      7   800 0.000328           0.9641     0.9338  6.551e+04       4647          0    0.02604     0.00\n",
      "NOTE:      8   800 0.000328           0.8511     0.9142  5.361e+04       5032          0    0.02604     0.00\n",
      "NOTE:      9   800 0.000328           0.8651     0.9054  5.471e+04       5718          0    0.02604     0.00\n",
      "NOTE:     10   800 0.000328           0.8991     0.9238  5.857e+04       4828          0    0.02604     0.00\n",
      "NOTE:     11   800 0.000328           0.8267     0.9277  5.601e+04       4366          0    0.02604     0.00\n",
      "NOTE:     12   800 0.000328            1.014     0.9347  6.534e+04       4562          0    0.02604     0.00\n",
      "NOTE:     13   800 0.000328           0.9704     0.9279  6.125e+04       4757          0    0.02604     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  86       0.0003          0.9078     0.9195  8.186e+05  7.167e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000328           0.8868     0.9271  5.965e+04       4690          0    0.02604     0.00\n",
      "NOTE:      1   800 0.000328            0.835      0.924  5.751e+04       4730          0    0.02604     0.00\n",
      "NOTE:      2   800 0.000328           0.8925     0.9273  5.849e+04       4584          0    0.02604     0.00\n",
      "NOTE:      3   800 0.000328           0.8475     0.9507  6.297e+04       3265          0    0.02604     0.00\n",
      "NOTE:      4   800 0.000328           0.9247     0.9335  6.363e+04       4530          0    0.02604     0.00\n",
      "NOTE:      5   800 0.000328           0.9852      0.923  6.542e+04       5457          0    0.02604     0.00\n",
      "NOTE:      6   800 0.000328           0.9935     0.9306   6.48e+04       4833          0    0.02604     0.00\n",
      "NOTE:      7   800 0.000328           0.8855      0.946   6.37e+04       3633          0    0.02604     0.00\n",
      "NOTE:      8   800 0.000328           0.8191     0.9271  5.202e+04       4093          0    0.02604     0.00\n",
      "NOTE:      9   800 0.000328           0.8775     0.9046  5.346e+04       5636          0    0.02604     0.00\n",
      "NOTE:     10   800 0.000328           0.9285     0.9299  6.436e+04       4854          0    0.02604     0.00\n",
      "NOTE:     11   800 0.000328           0.8463     0.9092    5.7e+04       5692          0    0.02604     0.00\n",
      "NOTE:     12   800 0.000328           0.8907     0.9314  5.971e+04       4399          0    0.02604     0.00\n",
      "NOTE:     13   800 0.000328           0.8578     0.8982  5.721e+04       6485          0    0.02604     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  87       0.0003          0.8908     0.9262  8.399e+05  6.688e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000328           0.8806     0.9235  5.773e+04       4784          0    0.02604     0.00\n",
      "NOTE:      1   800 0.000328           0.8807     0.9246  6.256e+04       5104          0    0.02604     0.00\n",
      "NOTE:      2   800 0.000328             1.04     0.9127  6.733e+04       6444          0    0.02604     0.00\n",
      "NOTE:      3   800 0.000328           0.9619     0.9442  6.292e+04       3719          0    0.02604     0.00\n",
      "NOTE:      4   800 0.000328           0.9865     0.9362  6.987e+04       4762          0    0.02604     0.00\n",
      "NOTE:      5   800 0.000328           0.8747     0.9157  5.925e+04       5455          0    0.02604     0.00\n",
      "NOTE:      6   800 0.000328           0.8738     0.9435  6.307e+04       3776          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000328            1.047     0.9276  7.056e+04       5507          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000328           0.8736     0.9115  5.752e+04       5588          0    0.02603     0.00\n",
      "NOTE:      9   800 0.000328           0.8804     0.9125  6.027e+04       5781          0    0.02603     0.00\n",
      "NOTE:     10   800 0.000328           0.9199     0.9379  6.348e+04       4203          0    0.02603     0.00\n",
      "NOTE:     11   800 0.000328           0.9444     0.9327  6.087e+04       4389          0    0.02603     0.00\n",
      "NOTE:     12   800 0.000328           0.9201     0.9196  6.004e+04       5251          0    0.02603     0.00\n",
      "NOTE:     13   800 0.000328           0.9371     0.9171  5.563e+04       5031          0    0.02603     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  88       0.0003          0.9301     0.9258  8.711e+05  6.979e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000328            1.015     0.9138  6.421e+04       6059          0    0.02603     0.00\n",
      "NOTE:      1   800 0.000328           0.8816     0.9421  5.988e+04       3678          0    0.02603     0.00\n",
      "NOTE:      2   800 0.000328           0.9673     0.9315  6.521e+04       4792          0    0.02603     0.00\n",
      "NOTE:      3   800 0.000328           0.8774     0.9325  5.684e+04       4115          0    0.02603     0.00\n",
      "NOTE:      4   800 0.000328           0.9585     0.9355  6.044e+04       4170          0    0.02603     0.00\n",
      "NOTE:      5   800 0.000328            0.923     0.9226  6.532e+04       5478          0    0.02603     0.00\n",
      "NOTE:      6   800 0.000328           0.8539     0.9298  5.686e+04       4296          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000328           0.9043      0.915  6.164e+04       5723          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000328            0.945     0.9089   6.25e+04       6266          0    0.02603     0.00\n",
      "NOTE:      9   800 0.000328           0.9368     0.9397  6.187e+04       3968          0    0.02603     0.00\n",
      "NOTE:     10   800 0.000328           0.8403      0.928  5.743e+04       4459          0    0.02603     0.00\n",
      "NOTE:     11   800 0.000328           0.8136     0.9384  5.882e+04       3859          0    0.02603     0.00\n",
      "NOTE:     12   800 0.000328           0.8399     0.9453  5.922e+04       3430          0    0.02603     0.00\n",
      "NOTE:     13   800 0.000328           0.9468     0.9411  6.507e+04       4073          0    0.02603     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  89       0.0003          0.9074       0.93  8.553e+05  6.437e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000262             0.88     0.9129  5.905e+04       5631          0    0.02603     0.00\n",
      "NOTE:      1   800 0.000262           0.7991     0.9348  5.416e+04       3775          0    0.02603     0.00\n",
      "NOTE:      2   800 0.000262           0.9198     0.9257  5.993e+04       4813          0    0.02603     0.00\n",
      "NOTE:      3   800 0.000262           0.8399     0.9039  5.282e+04       5617          0    0.02603     0.00\n",
      "NOTE:      4   800 0.000262           0.8274     0.9406  5.963e+04       3763          0    0.02603     0.00\n",
      "NOTE:      5   800 0.000262           0.8663      0.882  5.466e+04       7312          0    0.02603     0.00\n",
      "NOTE:      6   800 0.000262           0.8209     0.9303  5.841e+04       4374          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000262           0.8379      0.909  5.552e+04       5558          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000262           0.8872     0.9369  5.794e+04       3901          0    0.02603     0.00\n",
      "NOTE:      9   800 0.000262           0.9439     0.9385  6.101e+04       4000          0    0.02603     0.00\n",
      "NOTE:     10   800 0.000262           0.6869     0.9418  4.986e+04       3079          0    0.02603     0.00\n",
      "NOTE:     11   800 0.000262           0.9592     0.9233  5.854e+04       4860          0    0.02603     0.00\n",
      "NOTE:     12   800 0.000262           0.8109     0.9406  5.216e+04       3294          0    0.02603     0.00\n",
      "NOTE:     13   800 0.000262           0.8393      0.946  6.299e+04       3594          0    0.02603     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  90       0.0003          0.8513     0.9261  7.967e+05  6.357e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000262           0.8691     0.9254  5.602e+04       4518          0    0.02603     0.00\n",
      "NOTE:      1   800 0.000262           0.8856     0.9396  6.573e+04       4228          0    0.02603     0.00\n",
      "NOTE:      2   800 0.000262           0.8176     0.9421  5.843e+04       3593          0    0.02603     0.00\n",
      "NOTE:      3   800 0.000262           0.8588     0.9441  5.929e+04       3509          0    0.02603     0.00\n",
      "NOTE:      4   800 0.000262            0.786     0.9385  5.488e+04       3597          0    0.02603     0.00\n",
      "NOTE:      5   800 0.000262           0.8539     0.9495  5.833e+04       3101          0    0.02603     0.00\n",
      "NOTE:      6   800 0.000262           0.8121     0.9451  5.966e+04       3468          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000262           0.9652     0.9313   6.19e+04       4569          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000262           0.8536     0.8992  5.479e+04       6145          0    0.02603     0.00\n",
      "NOTE:      9   800 0.000262           0.8902     0.8606  5.564e+04       9013          0    0.02603     0.00\n",
      "NOTE:     10   800 0.000262           0.8848     0.9344  6.202e+04       4355          0    0.02603     0.00\n",
      "NOTE:     11   800 0.000262           0.9014     0.9393  5.935e+04       3837          0    0.02603     0.00\n",
      "NOTE:     12   800 0.000262           0.9118     0.8869  5.396e+04       6882          0    0.02603     0.00\n",
      "NOTE:     13   800 0.000262           0.8339     0.9503  5.212e+04       2726          0    0.02603     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  91       0.0003           0.866     0.9274  8.121e+05  6.354e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000262           0.8273     0.9282   5.88e+04       4550          0    0.02603     0.00\n",
      "NOTE:      1   800 0.000262            0.865     0.9029  5.431e+04       5840          0    0.02603     0.00\n",
      "NOTE:      2   800 0.000262           0.9574      0.907  6.324e+04       6482          0    0.02603     0.00\n",
      "NOTE:      3   800 0.000262           0.8227     0.9243  5.689e+04       4661          0    0.02603     0.00\n",
      "NOTE:      4   800 0.000262           0.8399     0.9352  5.191e+04       3597          0    0.02603     0.00\n",
      "NOTE:      5   800 0.000262           0.8883      0.922  5.707e+04       4830          0    0.02603     0.00\n",
      "NOTE:      6   800 0.000262           0.9045     0.9329  6.152e+04       4424          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000262           0.8494     0.9389  6.086e+04       3960          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000262           0.8374     0.9289  5.491e+04       4200          0    0.02603     0.00\n",
      "NOTE:      9   800 0.000262            0.892     0.9461  6.246e+04       3558          0    0.02603     0.00\n",
      "NOTE:     10   800 0.000262           0.8398     0.9216  5.717e+04       4864          0    0.02603     0.00\n",
      "NOTE:     11   800 0.000262            0.887     0.9407  5.829e+04       3674          0    0.02603     0.00\n",
      "NOTE:     12   800 0.000262           0.9158     0.9358  6.067e+04       4163          0    0.02603     0.00\n",
      "NOTE:     13   800 0.000262           0.8195     0.9334  5.543e+04       3954          0    0.02603     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  92       0.0003          0.8676     0.9284  8.135e+05  6.276e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000262           0.7746     0.9183  5.346e+04       4754          0    0.02603     0.00\n",
      "NOTE:      1   800 0.000262           0.8953     0.9192  5.861e+04       5150          0    0.02603     0.00\n",
      "NOTE:      2   800 0.000262           0.8065     0.9222  5.521e+04       4655          0    0.02603     0.00\n",
      "NOTE:      3   800 0.000262           0.8899     0.9306  5.901e+04       4398          0    0.02603     0.00\n",
      "NOTE:      4   800 0.000262           0.9169     0.9482  6.252e+04       3418          0    0.02603     0.00\n",
      "NOTE:      5   800 0.000262            0.853     0.9243  5.746e+04       4709          0    0.02603     0.00\n",
      "NOTE:      6   800 0.000262           0.8242     0.9294   5.22e+04       3968          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000262             0.96     0.9283  6.576e+04       5078          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000262           0.8326     0.9048  5.105e+04       5372          0    0.02603     0.00\n",
      "NOTE:      9   800 0.000262           0.7785     0.9165   5.46e+04       4973          0    0.02603     0.00\n",
      "NOTE:     10   800 0.000262           0.8618     0.9463  5.955e+04       3381          0    0.02603     0.00\n",
      "NOTE:     11   800 0.000262           0.8552     0.9154   5.13e+04       4741          0    0.02603     0.00\n",
      "NOTE:     12   800 0.000262           0.9398     0.9334  6.362e+04       4539          0    0.02603     0.00\n",
      "NOTE:     13   800 0.000262           0.8956     0.9106  5.922e+04       5816          0    0.02603     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  93       0.0003          0.8631     0.9252  8.036e+05  6.495e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000262            0.884     0.9469  6.629e+04       3714          0    0.02603     0.00\n",
      "NOTE:      1   800 0.000262            1.002     0.9148  5.986e+04       5574          0    0.02603     0.00\n",
      "NOTE:      2   800 0.000262           0.9298     0.8984  5.487e+04       6207          0    0.02603     0.00\n",
      "NOTE:      3   800 0.000262           0.9542     0.9279  6.173e+04       4798          0    0.02603     0.00\n",
      "NOTE:      4   800 0.000262             1.01     0.9285   6.37e+04       4902          0    0.02603     0.00\n",
      "NOTE:      5   800 0.000262           0.9671     0.9102   5.92e+04       5842          0    0.02603     0.00\n",
      "NOTE:      6   800 0.000262           0.9319     0.9309  6.134e+04       4556          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000262           0.9476     0.9181  5.881e+04       5249          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000262           0.8793      0.919  5.733e+04       5055          0    0.02603     0.00\n",
      "NOTE:      9   800 0.000262           0.9066     0.8901  5.905e+04       7294          0    0.02603     0.00\n",
      "NOTE:     10   800 0.000262           0.9886     0.9147  6.329e+04       5899          0    0.02603     0.00\n",
      "NOTE:     11   800 0.000262           0.9362      0.922  6.503e+04       5500          0    0.02603     0.00\n",
      "NOTE:     12   800 0.000262            1.002      0.925  6.719e+04       5447          0    0.02603     0.00\n",
      "NOTE:     13   800 0.000262           0.9179     0.9208  6.128e+04       5272          0    0.02603     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  94       0.0003          0.9469     0.9194   8.59e+05  7.531e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000262           0.8877      0.922  5.969e+04       5051          0    0.02603     0.00\n",
      "NOTE:      1   800 0.000262           0.8721     0.9172  5.934e+04       5359          0    0.02603     0.00\n",
      "NOTE:      2   800 0.000262           0.9421      0.928   6.49e+04       5034          0    0.02603     0.00\n",
      "NOTE:      3   800 0.000262           0.7608     0.9527  5.434e+04       2696          0    0.02603     0.00\n",
      "NOTE:      4   800 0.000262           0.9689     0.9325  6.625e+04       4795          0    0.02603     0.00\n",
      "NOTE:      5   800 0.000262           0.8648     0.9273  6.023e+04       4723          0    0.02603     0.00\n",
      "NOTE:      6   800 0.000262           0.8936     0.9313  6.365e+04       4696          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000262           0.9239     0.9221   6.09e+04       5147          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000262           0.8403     0.9537  5.952e+04       2890          0    0.02603     0.00\n",
      "NOTE:      9   800 0.000262           0.9506     0.9196  6.394e+04       5591          0    0.02603     0.00\n",
      "NOTE:     10   800 0.000262            1.021     0.9337   6.68e+04       4740          0    0.02603     0.00\n",
      "NOTE:     11   800 0.000262            1.014     0.9443  6.635e+04       3911          0    0.02603     0.00\n",
      "NOTE:     12   800 0.000262            0.909     0.9135  6.087e+04       5760          0    0.02603     0.00\n",
      "NOTE:     13   800 0.000262           0.8271     0.9268  5.368e+04       4238          0    0.02603     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  95       0.0003          0.9054     0.9301  8.604e+05  6.463e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000262            0.835     0.9425  5.918e+04       3614          0    0.02603     0.00\n",
      "NOTE:      1   800 0.000262           0.8055     0.9195  5.351e+04       4685          0    0.02603     0.00\n",
      "NOTE:      2   800 0.000262           0.8453     0.9396  5.809e+04       3733          0    0.02603     0.00\n",
      "NOTE:      3   800 0.000262           0.9107     0.9088  5.878e+04       5896          0    0.02603     0.00\n",
      "NOTE:      4   800 0.000262            1.103     0.9224  6.831e+04       5751          0    0.02603     0.00\n",
      "NOTE:      5   800 0.000262           0.7859     0.9553  5.805e+04       2713          0    0.02603     0.00\n",
      "NOTE:      6   800 0.000262           0.7752     0.8993  5.247e+04       5873          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000262           0.9422     0.9272  6.388e+04       5013          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000262           0.9192     0.9376  6.577e+04       4376          0    0.02603     0.00\n",
      "NOTE:      9   800 0.000262           0.8867     0.9326  6.094e+04       4407          0    0.02603     0.00\n",
      "NOTE:     10   800 0.000262           0.8955     0.9361  6.284e+04       4293          0    0.02603     0.00\n",
      "NOTE:     11   800 0.000262           0.9626      0.944  6.999e+04       4150          0    0.02603     0.00\n",
      "NOTE:     12   800 0.000262           0.9338     0.9416   6.41e+04       3972          0    0.02603     0.00\n",
      "NOTE:     13   800 0.000262           0.9276     0.9177  6.198e+04       5556          0    0.02603     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  96       0.0003          0.8949     0.9305  8.579e+05  6.403e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000262           0.9053      0.907  5.867e+04       6015          0    0.02603     0.00\n",
      "NOTE:      1   800 0.000262           0.8397     0.9227  5.495e+04       4601          0    0.02603     0.00\n",
      "NOTE:      2   800 0.000262           0.9828     0.9039  5.948e+04       6325          0    0.02603     0.00\n",
      "NOTE:      3   800 0.000262           0.8941     0.9175   5.69e+04       5117          0    0.02603     0.00\n",
      "NOTE:      4   800 0.000262           0.7887     0.9418  5.352e+04       3307          0    0.02603     0.00\n",
      "NOTE:      5   800 0.000262           0.8063      0.963  5.769e+04       2216          0    0.02603     0.00\n",
      "NOTE:      6   800 0.000262           0.9123     0.9129  5.992e+04       5719          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000262           0.9405     0.9156  5.955e+04       5492          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000262           0.9175     0.9045  5.472e+04       5779          0    0.02603     0.00\n",
      "NOTE:      9   800 0.000262           0.8244     0.9297  5.587e+04       4225          0    0.02603     0.00\n",
      "NOTE:     10   800 0.000262            0.915     0.9396  5.835e+04       3752          0    0.02603     0.00\n",
      "NOTE:     11   800 0.000262           0.8395     0.9227  5.409e+04       4530          0    0.02603     0.00\n",
      "NOTE:     12   800 0.000262           0.8587     0.9289  5.757e+04       4406          0    0.02603     0.00\n",
      "NOTE:     13   800 0.000262           0.9455     0.9096  5.883e+04       5846          0    0.02603     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  97       0.0003          0.8836     0.9224  8.001e+05  6.733e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800 0.000262            0.922     0.9274   6.33e+04       4957          0    0.02603     0.00\n",
      "NOTE:      1   800 0.000262           0.7838      0.921   5.39e+04       4623          0    0.02603     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      2   800 0.000262           0.8504     0.9086  5.748e+04       5779          0    0.02603     0.00\n",
      "NOTE:      3   800 0.000262           0.9184     0.9301   5.94e+04       4467          0    0.02603     0.00\n",
      "NOTE:      4   800 0.000262           0.8713     0.9436  6.168e+04       3687          0    0.02603     0.00\n",
      "NOTE:      5   800 0.000262           0.9736     0.9437  6.559e+04       3916          0    0.02603     0.00\n",
      "NOTE:      6   800 0.000262            1.091     0.9227  6.628e+04       5555          0    0.02603     0.00\n",
      "NOTE:      7   800 0.000262           0.8913     0.9244  5.855e+04       4790          0    0.02603     0.00\n",
      "NOTE:      8   800 0.000262           0.8802     0.9211  5.891e+04       5049          0    0.02602     0.00\n",
      "NOTE:      9   800 0.000262            1.011     0.8718  5.404e+04       7946          0    0.02602     0.00\n",
      "NOTE:     10   800 0.000262           0.7658     0.9413  5.368e+04       3345          0    0.02602     0.00\n",
      "NOTE:     11   800 0.000262           0.9861     0.9228  6.508e+04       5446          0    0.02602     0.00\n",
      "NOTE:     12   800 0.000262           0.9371     0.9353  6.335e+04       4386          0    0.02602     0.00\n",
      "NOTE:     13   800 0.000262            0.807     0.9432   5.49e+04       3305          0    0.02602     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  98       0.0003          0.9063     0.9256  8.361e+05  6.725e+04          0     0.03\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   800  0.00021           0.9413      0.943  6.624e+04       4006          0    0.02602     0.00\n",
      "NOTE:      1   800  0.00021           0.9865     0.8978  5.795e+04       6596          0    0.02602     0.00\n",
      "NOTE:      2   800  0.00021            1.017     0.9251  6.872e+04       5561          0    0.02602     0.00\n",
      "NOTE:      3   800  0.00021           0.8585     0.9153  5.608e+04       5190          0    0.02602     0.00\n",
      "NOTE:      4   800  0.00021           0.9471     0.9284  6.404e+04       4936          0    0.02602     0.00\n",
      "NOTE:      5   800  0.00021           0.8086     0.9235  5.525e+04       4575          0    0.02602     0.00\n",
      "NOTE:      6   800  0.00021            0.835     0.9133  5.615e+04       5332          0    0.02602     0.00\n",
      "NOTE:      7   800  0.00021           0.9395     0.9368  6.355e+04       4291          0    0.02602     0.00\n",
      "NOTE:      8   800  0.00021           0.8851     0.9371  6.106e+04       4101          0    0.02602     0.00\n",
      "NOTE:      9   800  0.00021           0.8369     0.9437  5.814e+04       3466          0    0.02602     0.00\n",
      "NOTE:     10   800  0.00021           0.8793     0.9246  6.143e+04       5012          0    0.02602     0.00\n",
      "NOTE:     11   800  0.00021           0.8114     0.9233  5.273e+04       4379          0    0.02602     0.00\n",
      "NOTE:     12   800  0.00021            0.952     0.9482  6.386e+04       3489          0    0.02602     0.00\n",
      "NOTE:     13   800  0.00021           0.8439     0.9147  6.049e+04       5642          0    0.02602     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  99       0.0002          0.8959      0.927  8.457e+05  6.658e+04          0     0.03\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       2.79 (s).\n"
     ]
    }
   ],
   "source": [
    "print(deepSurvModel.model_name)\n",
    "s.droptable(deepSurvModel.model_weights.name)\n",
    "deepsurvTrain_res = deepSurvModel.fit(\n",
    "                      data=trainTbl,\n",
    "                      data_specs=[\n",
    "                      dict(layer='input1',type='numnom', data=inputVars,nominals=nominals),\n",
    "                      dict(layer='survival1',type='numnom',data='y')],                      \n",
    "                      n_threads= 2, \n",
    "                      record_seed=13309, \n",
    "                      optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize training loss history to check model convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x28b66d7b548>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(deepsurvTrain_res[\"OptIterHistory\"][[\"Loss\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Build [Cox proportional hazards model](https://en.wikipedia.org/wiki/Proportional_hazards_model)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Input layer added.\n",
      "NOTE: Fully-connected layer added.\n",
      "NOTE: Survival layer added.\n",
      "NOTE: Model compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "modelName='cox_model';\n",
    "coxModel = Sequential(s, model_table=modelName)\n",
    "coxModel.add(InputLayer(std='STD'))\n",
    "coxModel.add(Dense(n=1, act='identity', include_bias=False))\n",
    "coxModel.add(Survival())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\r\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n",
       "<!-- Generated by graphviz version 2.38.0 (20140413.2041)\r\n",
       " -->\r\n",
       "<!-- Title: cox_model Pages: 1 -->\r\n",
       "<svg width=\"161pt\" height=\"171pt\"\r\n",
       " viewBox=\"0.00 0.00 161.00 171.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 167)\">\r\n",
       "<title>cox_model</title>\r\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-167 157,-167 157,4 -4,4\"/>\r\n",
       "<!-- Input1 -->\r\n",
       "<g id=\"node1\" class=\"node\"><title>Input1</title>\r\n",
       "<polygon fill=\"#3288bd\" fill-opacity=\"0.250980\" stroke=\"#3288bd\" points=\"23,-140.5 23,-162.5 130,-162.5 130,-140.5 23,-140.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"76.5\" y=\"-147.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">Input1(input)</text>\r\n",
       "</g>\r\n",
       "<!-- F.C.1 -->\r\n",
       "<g id=\"node2\" class=\"node\"><title>F.C.1</title>\r\n",
       "<polygon fill=\"#ffffbf\" fill-opacity=\"0.250980\" stroke=\"#aeae82\" points=\"20.5,-70.5 20.5,-92.5 132.5,-92.5 132.5,-70.5 20.5,-70.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"76.5\" y=\"-77.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">0x1 F.C.1(fc)</text>\r\n",
       "</g>\r\n",
       "<!-- Input1&#45;&gt;F.C.1 -->\r\n",
       "<g id=\"edge1\" class=\"edge\"><title>Input1&#45;&gt;F.C.1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M76.5,-140.466C76.5,-130.623 76.5,-115.327 76.5,-102.919\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"80.0001,-102.575 76.5,-92.5748 73.0001,-102.575 80.0001,-102.575\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"82.5\" y=\"-114\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 0 </text>\r\n",
       "</g>\r\n",
       "<!-- Survival1 -->\r\n",
       "<g id=\"node3\" class=\"node\"><title>Survival1</title>\r\n",
       "<polygon fill=\"#9e0142\" fill-opacity=\"0.250980\" stroke=\"#9e0142\" points=\"0,-0.5 0,-22.5 153,-22.5 153,-0.5 0,-0.5\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"76.5\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">1 Survival1(survival)</text>\r\n",
       "</g>\r\n",
       "<!-- F.C.1&#45;&gt;Survival1 -->\r\n",
       "<g id=\"edge2\" class=\"edge\"><title>F.C.1&#45;&gt;Survival1</title>\r\n",
       "<path fill=\"none\" stroke=\"#5677f3\" d=\"M76.5,-70.4664C76.5,-60.6231 76.5,-45.327 76.5,-32.9189\"/>\r\n",
       "<polygon fill=\"#5677f3\" stroke=\"#5677f3\" points=\"80.0001,-32.5748 76.5,-22.5748 73.0001,-32.5748 80.0001,-32.5748\"/>\r\n",
       "<text text-anchor=\"middle\" x=\"82.5\" y=\"-44\" font-family=\"Helvetica,sans-Serif\" font-size=\"10.00\"> 1 </text>\r\n",
       "</g>\r\n",
       "</g>\r\n",
       "</svg>\r\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x28b667d3948>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coxModel.plot_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer Id</th>\n",
       "      <th>Layer</th>\n",
       "      <th>Type</th>\n",
       "      <th>Kernel Size</th>\n",
       "      <th>Stride</th>\n",
       "      <th>Activation</th>\n",
       "      <th>Output Size</th>\n",
       "      <th>Number of Parameters</th>\n",
       "      <th>FLOPS(forward pass)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Input1</td>\n",
       "      <td>input</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>F.C.1</td>\n",
       "      <td>fc</td>\n",
       "      <td>(0, 1)</td>\n",
       "      <td></td>\n",
       "      <td>Identity</td>\n",
       "      <td>1</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Survival1</td>\n",
       "      <td>survival</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Total number of parameters</td>\n",
       "      <td>Total FLOPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Summary</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Layer Id      Layer      Type Kernel Size Stride Activation Output Size  \\\n",
       "0        0     Input1     input                          None           0   \n",
       "1        1      F.C.1        fc      (0, 1)          Identity           1   \n",
       "2        2  Survival1  survival                          None           1   \n",
       "3                                                                           \n",
       "4  Summary                                                                  \n",
       "\n",
       "         Number of Parameters FLOPS(forward pass)  \n",
       "0                      (0, 0)                   0  \n",
       "1                      (0, 0)                   0  \n",
       "2                      (0, 0)                   0  \n",
       "3  Total number of parameters         Total FLOPS  \n",
       "4                           0                   0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coxModel.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4Train Cox model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cox_model\n",
      "NOTE: Training from scratch.\n",
      "NOTE:  Synchronous mode is enabled.\n",
      "NOTE:  The total number of parameters is 19.\n",
      "NOTE:  The approximate memory cost is 1.00 MB.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       0.01 (s).\n",
      "NOTE:  The total number of threads on each worker is 2.\n",
      "NOTE:  The total mini-batch size per thread on each worker is 200.\n",
      "NOTE:  The maximum mini-batch size across all workers for the synchronous mode is 400.\n",
      "NOTE:  Target variable: y\n",
      "NOTE:  Number of input variables:     8\n",
      "NOTE:  Number of nominal input variables:      2\n",
      "NOTE:  Number of numeric input variables:      6\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001            1.077     0.8081  1.473e+04       3497          0   0.001145     0.00\n",
      "NOTE:      1   400    0.001            1.148     0.7558  1.378e+04       4454          0   0.001145     0.00\n",
      "NOTE:      2   400    0.001            1.072     0.7542   1.31e+04       4268          0   0.001144     0.00\n",
      "NOTE:      3   400    0.001           0.9418     0.7464  1.182e+04       4017          0   0.001144     0.00\n",
      "NOTE:      4   400    0.001           0.8748     0.8092   1.16e+04       2734          0   0.001143     0.00\n",
      "NOTE:      5   400    0.001           0.9482     0.7534  1.149e+04       3762          0   0.001142     0.00\n",
      "NOTE:      6   400    0.001           0.8932     0.7171       9938       3921          0   0.001141     0.00\n",
      "NOTE:      7   400    0.001           0.8946     0.7681  1.154e+04       3484          0    0.00114     0.00\n",
      "NOTE:      8   400    0.001            1.011     0.7625  1.152e+04       3588          0   0.001138     0.00\n",
      "NOTE:      9   400    0.001            1.016     0.7874   1.34e+04       3619          0   0.001137     0.00\n",
      "NOTE:     10   400    0.001            1.107     0.7357  1.387e+04       4985          0   0.001135     0.00\n",
      "NOTE:     11   400    0.001           0.9311     0.7907  1.307e+04       3459          0   0.001134     0.00\n",
      "NOTE:     12   400    0.001           0.8587     0.7249  1.056e+04       4007          0   0.001132     0.00\n",
      "NOTE:     13   400    0.001            1.094     0.7832  1.395e+04       3861          0    0.00113     0.00\n",
      "NOTE:     14   400    0.001           0.9235     0.7601  1.238e+04       3908          0   0.001128     0.00\n",
      "NOTE:     15   400    0.001            1.034     0.7281  1.214e+04       4532          0   0.001126     0.00\n",
      "NOTE:     16   400    0.001           0.9859     0.7576  1.219e+04       3899          0   0.001125     0.00\n",
      "NOTE:     17   400    0.001            1.083     0.7922  1.476e+04       3871          0   0.001123     0.00\n",
      "NOTE:     18   400    0.001            1.061     0.7554  1.297e+04       4200          0   0.001121     0.00\n",
      "NOTE:     19   400    0.001            1.184     0.7781   1.59e+04       4535          0    0.00112     0.00\n",
      "NOTE:     20   400    0.001            1.044     0.7483  1.191e+04       4005          0   0.001118     0.00\n",
      "NOTE:     21   400    0.001            1.037     0.7151  1.141e+04       4548          0   0.001116     0.00\n",
      "NOTE:     22   400    0.001           0.9819     0.7757  1.102e+04       3186          0   0.001114     0.00\n",
      "NOTE:     23   400    0.001           0.9893     0.7477  1.181e+04       3984          0   0.001112     0.00\n",
      "NOTE:     24   400    0.001            1.121     0.8172  1.545e+04       3456          0    0.00111     0.00\n",
      "NOTE:     25   400    0.001            1.103     0.7807  1.339e+04       3761          0   0.001109     0.00\n",
      "NOTE:     26   400    0.001             1.09     0.7694  1.319e+04       3954          0   0.001107     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  0         0.001           1.019     0.7647  3.429e+05  1.055e+05          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001            1.052      0.765  1.297e+04       3983          0   0.001105     0.00\n",
      "NOTE:      1   400    0.001           0.8496     0.8055  1.192e+04       2878          0   0.001103     0.00\n",
      "NOTE:      2   400    0.001           0.8525     0.8039  1.133e+04       2763          0   0.001101     0.00\n",
      "NOTE:      3   400    0.001           0.9514     0.7361  1.062e+04       3808          0     0.0011     0.00\n",
      "NOTE:      4   400    0.001            1.071     0.7747  1.291e+04       3753          0   0.001098     0.00\n",
      "NOTE:      5   400    0.001            0.969     0.8054  1.372e+04       3316          0   0.001097     0.00\n",
      "NOTE:      6   400    0.001           0.8333     0.7337       8523       3093          0   0.001095     0.00\n",
      "NOTE:      7   400    0.001           0.9952     0.7952  1.444e+04       3719          0   0.001094     0.00\n",
      "NOTE:      8   400    0.001           0.9115     0.8024  1.243e+04       3061          0   0.001092     0.00\n",
      "NOTE:      9   400    0.001            1.066      0.784  1.388e+04       3826          0   0.001091     0.00\n",
      "NOTE:     10   400    0.001           0.7982      0.748       9744       3283          0   0.001089     0.00\n",
      "NOTE:     11   400    0.001           0.8523     0.7686  1.085e+04       3266          0   0.001088     0.00\n",
      "NOTE:     12   400    0.001             1.03       0.74  1.176e+04       4133          0   0.001086     0.00\n",
      "NOTE:     13   400    0.001           0.9596     0.7703  1.283e+04       3827          0   0.001085     0.00\n",
      "NOTE:     14   400    0.001            1.051     0.7738  1.342e+04       3923          0   0.001083     0.00\n",
      "NOTE:     15   400    0.001           0.9616     0.7708   1.24e+04       3685          0   0.001082     0.00\n",
      "NOTE:     16   400    0.001           0.8799     0.8042  1.307e+04       3182          0    0.00108     0.00\n",
      "NOTE:     17   400    0.001           0.9482     0.7621  1.311e+04       4094          0   0.001079     0.00\n",
      "NOTE:     18   400    0.001           0.9041     0.8086  1.321e+04       3127          0   0.001078     0.00\n",
      "NOTE:     19   400    0.001           0.9328     0.7595  1.109e+04       3513          0   0.001077     0.00\n",
      "NOTE:     20   400    0.001           0.8147     0.7678  1.028e+04       3108          0   0.001076     0.00\n",
      "NOTE:     21   400    0.001           0.9426     0.7768  1.254e+04       3603          0   0.001074     0.00\n",
      "NOTE:     22   400    0.001           0.9534     0.7733  1.301e+04       3815          0   0.001073     0.00\n",
      "NOTE:     23   400    0.001            1.067     0.7862  1.457e+04       3963          0   0.001072     0.00\n",
      "NOTE:     24   400    0.001           0.8742      0.826  1.186e+04       2499          0   0.001071     0.00\n",
      "NOTE:     25   400    0.001           0.9668     0.8137  1.407e+04       3221          0    0.00107     0.00\n",
      "NOTE:     26   400    0.001            1.025     0.7859  1.403e+04       3821          0   0.001069     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  1         0.001          0.9449     0.7802  3.346e+05  9.426e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001            1.068     0.7486  1.233e+04       4142          0   0.001068     0.00\n",
      "NOTE:      1   400    0.001           0.9232     0.7956  1.071e+04       2751          0   0.001067     0.00\n",
      "NOTE:      2   400    0.001           0.7882     0.7403       9631       3379          0   0.001065     0.00\n",
      "NOTE:      3   400    0.001            1.019     0.7907  1.421e+04       3761          0   0.001064     0.00\n",
      "NOTE:      4   400    0.001           0.8709     0.8037   1.27e+04       3102          0   0.001063     0.00\n",
      "NOTE:      5   400    0.001           0.8968      0.757  1.112e+04       3570          0   0.001062     0.00\n",
      "NOTE:      6   400    0.001           0.9294     0.7966  1.272e+04       3247          0   0.001061     0.00\n",
      "NOTE:      7   400    0.001           0.8867     0.7761  1.073e+04       3096          0    0.00106     0.00\n",
      "NOTE:      8   400    0.001           0.9499     0.7936  1.252e+04       3256          0   0.001059     0.00\n",
      "NOTE:      9   400    0.001           0.9519     0.7847  1.197e+04       3284          0   0.001059     0.00\n",
      "NOTE:     10   400    0.001           0.8369     0.7874  1.046e+04       2824          0   0.001058     0.00\n",
      "NOTE:     11   400    0.001             0.93     0.8101  1.506e+04       3528          0   0.001057     0.00\n",
      "NOTE:     12   400    0.001           0.8651     0.8033  1.182e+04       2893          0   0.001056     0.00\n",
      "NOTE:     13   400    0.001           0.9193     0.8003  1.251e+04       3122          0   0.001055     0.00\n",
      "NOTE:     14   400    0.001           0.9232     0.7916  1.246e+04       3280          0   0.001054     0.00\n",
      "NOTE:     15   400    0.001           0.8637       0.77  1.043e+04       3115          0   0.001053     0.00\n",
      "NOTE:     16   400    0.001            0.843     0.7868  1.137e+04       3079          0   0.001053     0.00\n",
      "NOTE:     17   400    0.001           0.7261     0.7976       9216       2339          0   0.001052     0.00\n",
      "NOTE:     18   400    0.001           0.9515      0.808  1.319e+04       3134          0   0.001051     0.00\n",
      "NOTE:     19   400    0.001            1.046     0.8167  1.458e+04       3273          0    0.00105     0.00\n",
      "NOTE:     20   400    0.001            1.032     0.7714  1.365e+04       4047          0   0.001049     0.00\n",
      "NOTE:     21   400    0.001           0.8944     0.7875  1.142e+04       3082          0   0.001049     0.00\n",
      "NOTE:     22   400    0.001            1.076     0.7926  1.406e+04       3679          0   0.001048     0.00\n",
      "NOTE:     23   400    0.001            0.747     0.8175  1.056e+04       2357          0   0.001047     0.00\n",
      "NOTE:     24   400    0.001            0.791      0.812   1.16e+04       2686          0   0.001046     0.00\n",
      "NOTE:     25   400    0.001           0.9632     0.8057  1.403e+04       3382          0   0.001045     0.00\n",
      "NOTE:     26   400    0.001           0.9943      0.791  1.297e+04       3428          0   0.001045     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  2         0.001          0.9143     0.7907   3.28e+05  8.684e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001           0.9081     0.7875  1.182e+04       3189          0   0.001044     0.00\n",
      "NOTE:      1   400    0.001            1.058     0.8173  1.491e+04       3335          0   0.001043     0.00\n",
      "NOTE:      2   400    0.001           0.8643     0.7742  1.023e+04       2984          0   0.001043     0.00\n",
      "NOTE:      3   400    0.001           0.9432       0.79  1.302e+04       3462          0   0.001042     0.00\n",
      "NOTE:      4   400    0.001           0.8345     0.7648  1.047e+04       3219          0   0.001041     0.00\n",
      "NOTE:      5   400    0.001           0.9918     0.7896  1.242e+04       3310          0   0.001041     0.00\n",
      "NOTE:      6   400    0.001           0.8037     0.7409  1.028e+04       3594          0    0.00104     0.00\n",
      "NOTE:      7   400    0.001           0.7611     0.7984  1.106e+04       2794          0   0.001039     0.00\n",
      "NOTE:      8   400    0.001           0.9091      0.788   1.19e+04       3202          0   0.001039     0.00\n",
      "NOTE:      9   400    0.001           0.8549     0.7887  1.285e+04       3441          0   0.001038     0.00\n",
      "NOTE:     10   400    0.001           0.9439     0.8608  1.531e+04       2476          0   0.001038     0.00\n",
      "NOTE:     11   400    0.001           0.7432     0.7885  1.055e+04       2829          0   0.001037     0.00\n",
      "NOTE:     12   400    0.001           0.9281     0.7962  1.262e+04       3229          0   0.001037     0.00\n",
      "NOTE:     13   400    0.001           0.9247     0.7869   1.23e+04       3329          0   0.001036     0.00\n",
      "NOTE:     14   400    0.001           0.8401     0.7879   1.19e+04       3203          0   0.001036     0.00\n",
      "NOTE:     15   400    0.001           0.9578     0.7951  1.299e+04       3347          0   0.001036     0.00\n",
      "NOTE:     16   400    0.001             1.01     0.7766  1.223e+04       3519          0   0.001035     0.00\n",
      "NOTE:     17   400    0.001           0.9605     0.7716  1.268e+04       3752          0   0.001035     0.00\n",
      "NOTE:     18   400    0.001            1.043     0.8179  1.464e+04       3260          0   0.001034     0.00\n",
      "NOTE:     19   400    0.001           0.8309     0.7848    1.1e+04       3016          0   0.001034     0.00\n",
      "NOTE:     20   400    0.001           0.8935     0.7655  1.164e+04       3565          0   0.001033     0.00\n",
      "NOTE:     21   400    0.001           0.9387     0.7833  1.282e+04       3545          0   0.001032     0.00\n",
      "NOTE:     22   400    0.001            1.026     0.7776  1.325e+04       3789          0   0.001032     0.00\n",
      "NOTE:     23   400    0.001           0.8384     0.8663  1.309e+04       2021          0   0.001031     0.00\n",
      "NOTE:     24   400    0.001           0.7565     0.7709       9725       2890          0   0.001031     0.00\n",
      "NOTE:     25   400    0.001            1.028     0.8157  1.523e+04       3442          0    0.00103     0.00\n",
      "NOTE:     26   400    0.001           0.9498     0.8264   1.37e+04       2877          0    0.00103     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  3         0.001           0.909     0.7944  3.346e+05  8.662e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001            0.725     0.8048       9626       2334          0    0.00103     0.00\n",
      "NOTE:      1   400    0.001           0.9595     0.8165  1.418e+04       3187          0   0.001029     0.00\n",
      "NOTE:      2   400    0.001           0.9801     0.8594  1.544e+04       2525          0   0.001029     0.00\n",
      "NOTE:      3   400    0.001           0.7714     0.8147   1.09e+04       2479          0   0.001029     0.00\n",
      "NOTE:      4   400    0.001           0.9539     0.8161  1.421e+04       3200          0   0.001028     0.00\n",
      "NOTE:      5   400    0.001           0.8746     0.8037  1.143e+04       2791          0   0.001028     0.00\n",
      "NOTE:      6   400    0.001            1.002      0.768   1.39e+04       4197          0   0.001028     0.00\n",
      "NOTE:      7   400    0.001            1.079     0.7974  1.601e+04       4069          0   0.001027     0.00\n",
      "NOTE:      8   400    0.001           0.9186     0.8448  1.399e+04       2571          0   0.001027     0.00\n",
      "NOTE:      9   400    0.001           0.8662     0.7879  1.208e+04       3251          0   0.001027     0.00\n",
      "NOTE:     10   400    0.001            0.908      0.821  1.291e+04       2816          0   0.001026     0.00\n",
      "NOTE:     11   400    0.001           0.8176     0.8136  1.303e+04       2986          0   0.001026     0.00\n",
      "NOTE:     12   400    0.001           0.9217     0.7559  1.164e+04       3757          0   0.001026     0.00\n",
      "NOTE:     13   400    0.001            0.911     0.8429  1.411e+04       2629          0   0.001025     0.00\n",
      "NOTE:     14   400    0.001            1.003     0.7975  1.425e+04       3616          0   0.001025     0.00\n",
      "NOTE:     15   400    0.001            1.011     0.7916  1.309e+04       3448          0   0.001025     0.00\n",
      "NOTE:     16   400    0.001            1.056     0.8235  1.597e+04       3424          0   0.001025     0.00\n",
      "NOTE:     17   400    0.001           0.9093     0.8651  1.454e+04       2268          0   0.001024     0.00\n",
      "NOTE:     18   400    0.001           0.8722     0.8152  1.183e+04       2680          0   0.001024     0.00\n",
      "NOTE:     19   400    0.001            1.102     0.7907  1.296e+04       3430          0   0.001024     0.00\n",
      "NOTE:     20   400    0.001           0.9362      0.834  1.488e+04       2961          0   0.001024     0.00\n",
      "NOTE:     21   400    0.001            1.079     0.7878   1.39e+04       3743          0   0.001023     0.00\n",
      "NOTE:     22   400    0.001             0.94     0.8403  1.364e+04       2591          0   0.001023     0.00\n",
      "NOTE:     23   400    0.001           0.9706     0.7709  1.117e+04       3320          0   0.001023     0.00\n",
      "NOTE:     24   400    0.001           0.9074     0.7696  1.101e+04       3297          0   0.001023     0.00\n",
      "NOTE:     25   400    0.001            1.002     0.8211  1.394e+04       3038          0   0.001022     0.00\n",
      "NOTE:     26   400    0.001           0.9972     0.7816  1.319e+04       3686          0   0.001022     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  4         0.001          0.9435     0.8093  3.578e+05  8.429e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001           0.8652     0.7969  1.219e+04       3107          0   0.001022     0.00\n",
      "NOTE:      1   400    0.001            0.939     0.7794  1.376e+04       3894          0   0.001022     0.00\n",
      "NOTE:      2   400    0.001           0.8497      0.818   1.13e+04       2514          0   0.001021     0.00\n",
      "NOTE:      3   400    0.001            1.016     0.8279  1.418e+04       2947          0   0.001021     0.00\n",
      "NOTE:      4   400    0.001           0.9243     0.8089  1.331e+04       3143          0   0.001021     0.00\n",
      "NOTE:      5   400    0.001           0.9469     0.8053   1.36e+04       3288          0    0.00102     0.00\n",
      "NOTE:      6   400    0.001           0.7936      0.814  1.176e+04       2688          0    0.00102     0.00\n",
      "NOTE:      7   400    0.001           0.8602     0.8444  1.319e+04       2430          0    0.00102     0.00\n",
      "NOTE:      8   400    0.001           0.9796     0.7654  1.213e+04       3716          0    0.00102     0.00\n",
      "NOTE:      9   400    0.001           0.9046      0.832  1.334e+04       2692          0    0.00102     0.00\n",
      "NOTE:     10   400    0.001           0.8751      0.828  1.269e+04       2637          0   0.001019     0.00\n",
      "NOTE:     11   400    0.001           0.8839     0.8629   1.43e+04       2272          0   0.001019     0.00\n",
      "NOTE:     12   400    0.001           0.8462     0.8441  1.311e+04       2420          0   0.001019     0.00\n",
      "NOTE:     13   400    0.001           0.8854     0.7851   1.25e+04       3420          0   0.001019     0.00\n",
      "NOTE:     14   400    0.001           0.9657     0.8289  1.413e+04       2917          0   0.001019     0.00\n",
      "NOTE:     15   400    0.001           0.9933     0.8153  1.401e+04       3173          0   0.001019     0.00\n",
      "NOTE:     16   400    0.001            0.815     0.8447  1.221e+04       2245          0   0.001019     0.00\n",
      "NOTE:     17   400    0.001           0.8599     0.7959  1.159e+04       2971          0   0.001019     0.00\n",
      "NOTE:     18   400    0.001           0.9025     0.7756  1.133e+04       3278          0   0.001019     0.00\n",
      "NOTE:     19   400    0.001           0.7751     0.8086  1.111e+04       2630          0   0.001019     0.00\n",
      "NOTE:     20   400    0.001            0.903     0.8109  1.282e+04       2988          0   0.001019     0.00\n",
      "NOTE:     21   400    0.001           0.9131      0.806  1.364e+04       3283          0   0.001019     0.00\n",
      "NOTE:     22   400    0.001           0.9413     0.8561  1.468e+04       2468          0   0.001018     0.00\n",
      "NOTE:     23   400    0.001            1.086     0.8302  1.569e+04       3208          0   0.001018     0.00\n",
      "NOTE:     24   400    0.001           0.9801     0.8283  1.344e+04       2787          0   0.001018     0.00\n",
      "NOTE:     25   400    0.001           0.9417     0.8073  1.372e+04       3274          0   0.001018     0.00\n",
      "NOTE:     26   400    0.001            1.036     0.8336  1.513e+04       3020          0   0.001018     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  5         0.001          0.9141     0.8171  3.548e+05  7.941e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001            1.031     0.8147  1.402e+04       3190          0   0.001018     0.00\n",
      "NOTE:      1   400    0.001            0.819     0.8169  1.188e+04       2664          0   0.001018     0.00\n",
      "NOTE:      2   400    0.001            1.016     0.8131  1.412e+04       3246          0   0.001018     0.00\n",
      "NOTE:      3   400    0.001           0.8462     0.8186  1.249e+04       2768          0   0.001018     0.00\n",
      "NOTE:      4   400    0.001           0.7915     0.8446  1.223e+04       2250          0   0.001018     0.00\n",
      "NOTE:      5   400    0.001           0.9861     0.8139  1.428e+04       3264          0   0.001018     0.00\n",
      "NOTE:      6   400    0.001           0.7243     0.8238  1.051e+04       2249          0   0.001018     0.00\n",
      "NOTE:      7   400    0.001           0.8437     0.7855  1.173e+04       3202          0   0.001018     0.00\n",
      "NOTE:      8   400    0.001            1.002     0.8015  1.455e+04       3603          0   0.001018     0.00\n",
      "NOTE:      9   400    0.001           0.8898     0.8085  1.213e+04       2873          0   0.001018     0.00\n",
      "NOTE:     10   400    0.001           0.9689     0.8227  1.434e+04       3090          0   0.001018     0.00\n",
      "NOTE:     11   400    0.001           0.8933     0.8127  1.238e+04       2854          0   0.001018     0.00\n",
      "NOTE:     12   400    0.001            1.029     0.8188  1.575e+04       3485          0   0.001018     0.00\n",
      "NOTE:     13   400    0.001            1.046      0.841   1.57e+04       2969          0   0.001017     0.00\n",
      "NOTE:     14   400    0.001            0.875     0.7967  1.348e+04       3440          0   0.001017     0.00\n",
      "NOTE:     15   400    0.001           0.8899     0.7979  1.181e+04       2991          0   0.001017     0.00\n",
      "NOTE:     16   400    0.001            1.002     0.8101  1.486e+04       3483          0   0.001017     0.00\n",
      "NOTE:     17   400    0.001            1.075     0.8315   1.55e+04       3141          0   0.001017     0.00\n",
      "NOTE:     18   400    0.001           0.8313     0.8084  1.231e+04       2916          0   0.001017     0.00\n",
      "NOTE:     19   400    0.001           0.9615     0.8371  1.419e+04       2761          0   0.001017     0.00\n",
      "NOTE:     20   400    0.001           0.9195     0.8186  1.392e+04       3083          0   0.001017     0.00\n",
      "NOTE:     21   400    0.001           0.8575     0.8162  1.277e+04       2877          0   0.001017     0.00\n",
      "NOTE:     22   400    0.001           0.9154     0.8032   1.37e+04       3357          0   0.001017     0.00\n",
      "NOTE:     23   400    0.001           0.8753     0.8078  1.176e+04       2799          0   0.001017     0.00\n",
      "NOTE:     24   400    0.001             1.04     0.8062  1.392e+04       3346          0   0.001017     0.00\n",
      "NOTE:     25   400    0.001           0.8875     0.7952  1.227e+04       3161          0   0.001017     0.00\n",
      "NOTE:     26   400    0.001           0.9764     0.8353  1.397e+04       2754          0   0.001017     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  6         0.001          0.9257     0.8151  3.606e+05  8.182e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001            0.941     0.8191  1.396e+04       3083          0   0.001017     0.00\n",
      "NOTE:      1   400    0.001           0.9241     0.8186  1.291e+04       2860          0   0.001017     0.00\n",
      "NOTE:      2   400    0.001           0.8802     0.8007  1.224e+04       3048          0   0.001017     0.00\n",
      "NOTE:      3   400    0.001           0.7727     0.8673  1.177e+04       1801          0   0.001017     0.00\n",
      "NOTE:      4   400    0.001           0.9172     0.8594  1.473e+04       2410          0   0.001017     0.00\n",
      "NOTE:      5   400    0.001            0.741      0.833  1.215e+04       2436          0   0.001017     0.00\n",
      "NOTE:      6   400    0.001            0.948     0.7982  1.252e+04       3164          0   0.001017     0.00\n",
      "NOTE:      7   400    0.001           0.8288     0.8458  1.298e+04       2365          0   0.001017     0.00\n",
      "NOTE:      8   400    0.001           0.9607     0.8156  1.405e+04       3176          0   0.001017     0.00\n",
      "NOTE:      9   400    0.001            1.077     0.8071  1.554e+04       3714          0   0.001017     0.00\n",
      "NOTE:     10   400    0.001            1.033     0.8306  1.376e+04       2806          0   0.001017     0.00\n",
      "NOTE:     11   400    0.001            1.072     0.8222  1.496e+04       3236          0   0.001017     0.00\n",
      "NOTE:     12   400    0.001           0.7878     0.8551  1.225e+04       2076          0   0.001017     0.00\n",
      "NOTE:     13   400    0.001            1.012     0.8194  1.438e+04       3169          0   0.001017     0.00\n",
      "NOTE:     14   400    0.001           0.9058     0.8328  1.278e+04       2567          0   0.001017     0.00\n",
      "NOTE:     15   400    0.001           0.8974     0.7984  1.399e+04       3532          0   0.001018     0.00\n",
      "NOTE:     16   400    0.001           0.8925     0.8354  1.269e+04       2500          0   0.001018     0.00\n",
      "NOTE:     17   400    0.001           0.8878     0.7995  1.228e+04       3080          0   0.001018     0.00\n",
      "NOTE:     18   400    0.001           0.8213     0.8749  1.352e+04       1933          0   0.001018     0.00\n",
      "NOTE:     19   400    0.001           0.9211     0.8188  1.354e+04       2996          0   0.001018     0.00\n",
      "NOTE:     20   400    0.001           0.9119     0.8547  1.357e+04       2308          0   0.001018     0.00\n",
      "NOTE:     21   400    0.001           0.7452     0.8493  1.142e+04       2026          0   0.001019     0.00\n",
      "NOTE:     22   400    0.001            0.815     0.8215  1.111e+04       2415          0   0.001019     0.00\n",
      "NOTE:     23   400    0.001           0.8216      0.838  1.324e+04       2559          0   0.001019     0.00\n",
      "NOTE:     24   400    0.001           0.8371     0.8509  1.274e+04       2232          0   0.001019     0.00\n",
      "NOTE:     25   400    0.001            1.122     0.8591  1.724e+04       2828          0   0.001019     0.00\n",
      "NOTE:     26   400    0.001           0.8578     0.8581  1.306e+04       2161          0   0.001019     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  7         0.001          0.9012     0.8322  3.594e+05  7.248e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001           0.8961     0.8293  1.208e+04       2487          0    0.00102     0.00\n",
      "NOTE:      1   400    0.001           0.9535     0.8364  1.361e+04       2663          0    0.00102     0.00\n",
      "NOTE:      2   400    0.001           0.9064     0.8234  1.219e+04       2613          0    0.00102     0.00\n",
      "NOTE:      3   400    0.001           0.8311     0.8395  1.223e+04       2339          0    0.00102     0.00\n",
      "NOTE:      4   400    0.001             0.88     0.7565  1.027e+04       3304          0   0.001021     0.00\n",
      "NOTE:      5   400    0.001           0.7816      0.826   1.17e+04       2465          0   0.001021     0.00\n",
      "NOTE:      6   400    0.001           0.8277     0.8472   1.21e+04       2183          0   0.001021     0.00\n",
      "NOTE:      7   400    0.001            0.821     0.8265  1.192e+04       2501          0   0.001021     0.00\n",
      "NOTE:      8   400    0.001            1.026     0.8187   1.49e+04       3300          0   0.001021     0.00\n",
      "NOTE:      9   400    0.001            1.003     0.8441  1.557e+04       2876          0   0.001021     0.00\n",
      "NOTE:     10   400    0.001           0.7999     0.8491  1.239e+04       2201          0   0.001021     0.00\n",
      "NOTE:     11   400    0.001            0.723     0.8181  1.031e+04       2292          0   0.001021     0.00\n",
      "NOTE:     12   400    0.001           0.9099     0.8663  1.396e+04       2153          0   0.001021     0.00\n",
      "NOTE:     13   400    0.001           0.9187     0.7932  1.199e+04       3126          0   0.001022     0.00\n",
      "NOTE:     14   400    0.001           0.8522     0.8318  1.151e+04       2328          0   0.001022     0.00\n",
      "NOTE:     15   400    0.001           0.8467     0.7872  1.113e+04       3009          0   0.001022     0.00\n",
      "NOTE:     16   400    0.001           0.9811     0.8224  1.424e+04       3074          0   0.001022     0.00\n",
      "NOTE:     17   400    0.001           0.9258     0.8722  1.537e+04       2253          0   0.001022     0.00\n",
      "NOTE:     18   400    0.001           0.7962     0.8759  1.159e+04       1642          0   0.001022     0.00\n",
      "NOTE:     19   400    0.001           0.9399      0.807   1.23e+04       2941          0   0.001022     0.00\n",
      "NOTE:     20   400    0.001           0.9368     0.8521  1.489e+04       2585          0   0.001022     0.00\n",
      "NOTE:     21   400    0.001           0.9299     0.8551  1.401e+04       2375          0   0.001022     0.00\n",
      "NOTE:     22   400    0.001           0.8387     0.8437  1.359e+04       2517          0   0.001022     0.00\n",
      "NOTE:     23   400    0.001           0.7374     0.8266  1.086e+04       2277          1   0.001022     0.00\n",
      "NOTE:     24   400    0.001            1.038     0.8607  1.684e+04       2724          0   0.001022     0.00\n",
      "NOTE:     25   400    0.001           0.9075     0.8602  1.352e+04       2198          0   0.001022     0.00\n",
      "NOTE:     26   400    0.001           0.7481     0.8786  1.231e+04       1700          0   0.001022     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  8         0.001          0.8799      0.836  3.474e+05  6.813e+04          1     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001           0.9203     0.7715   1.23e+04       3643          0   0.001023     0.00\n",
      "NOTE:      1   400    0.001           0.7245     0.8128  1.019e+04       2347          0   0.001023     0.00\n",
      "NOTE:      2   400    0.001           0.9669     0.8476   1.53e+04       2751          0   0.001023     0.00\n",
      "NOTE:      3   400    0.001             1.01     0.8606  1.566e+04       2537          0   0.001023     0.00\n",
      "NOTE:      4   400    0.001           0.9795      0.791  1.226e+04       3241          0   0.001023     0.00\n",
      "NOTE:      5   400    0.001           0.9624     0.7918  1.319e+04       3468          0   0.001024     0.00\n",
      "NOTE:      6   400    0.001           0.7656     0.8546  1.191e+04       2026          0   0.001024     0.00\n",
      "NOTE:      7   400    0.001           0.8146     0.8229  1.205e+04       2593          0   0.001024     0.00\n",
      "NOTE:      8   400    0.001           0.8809     0.8279  1.302e+04       2706          0   0.001024     0.00\n",
      "NOTE:      9   400    0.001           0.7569     0.8754  1.339e+04       1906          0   0.001024     0.00\n",
      "NOTE:     10   400    0.001           0.8898     0.8637  1.435e+04       2265          0   0.001024     0.00\n",
      "NOTE:     11   400    0.001           0.6762     0.8387       9330       1794          0   0.001024     0.00\n",
      "NOTE:     12   400    0.001           0.9797     0.8085  1.449e+04       3432          0   0.001025     0.00\n",
      "NOTE:     13   400    0.001            1.002     0.7661  1.325e+04       4044          0   0.001025     0.00\n",
      "NOTE:     14   400    0.001           0.8429     0.8591   1.33e+04       2182          0   0.001025     0.00\n",
      "NOTE:     15   400    0.001           0.9592     0.8509  1.495e+04       2621          0   0.001025     0.00\n",
      "NOTE:     16   400    0.001             1.02     0.8617  1.535e+04       2464          0   0.001025     0.00\n",
      "NOTE:     17   400    0.001           0.9787     0.8081  1.452e+04       3448          0   0.001026     0.00\n",
      "NOTE:     18   400    0.001           0.9008     0.8275  1.359e+04       2833          0   0.001026     0.00\n",
      "NOTE:     19   400    0.001           0.7663     0.7777       9809       2804          0   0.001026     0.00\n",
      "NOTE:     20   400    0.001           0.9372     0.7917  1.264e+04       3325          0   0.001026     0.00\n",
      "NOTE:     21   400    0.001           0.9145     0.8563  1.521e+04       2554          0   0.001026     0.00\n",
      "NOTE:     22   400    0.001            1.119     0.7727  1.435e+04       4223          0   0.001026     0.00\n",
      "NOTE:     23   400    0.001           0.8267     0.8306  1.212e+04       2471          0   0.001026     0.00\n",
      "NOTE:     24   400    0.001           0.9205     0.8165  1.383e+04       3108          0   0.001026     0.00\n",
      "NOTE:     25   400    0.001            1.084      0.816  1.515e+04       3415          0   0.001026     0.00\n",
      "NOTE:     26   400    0.001           0.9413     0.8199  1.323e+04       2905          0   0.001026     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  9         0.001          0.9089     0.8231  3.587e+05  7.711e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001            1.061     0.8444  1.562e+04       2877          0   0.001026     0.00\n",
      "NOTE:      1   400    0.001           0.9061     0.8498  1.375e+04       2431          0   0.001026     0.00\n",
      "NOTE:      2   400    0.001             0.84     0.8021  1.135e+04       2801          0   0.001026     0.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The table cox_model_weights could not be located in caslib CASUSER(guilin) of Cloud Analytic Services.\n",
      "ERROR: The action stopped due to errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      3   400    0.001            1.054     0.8315  1.585e+04       3213          0   0.001026     0.00\n",
      "NOTE:      4   400    0.001           0.9805     0.8681  1.504e+04       2284          0   0.001026     0.00\n",
      "NOTE:      5   400    0.001           0.8555     0.8305  1.202e+04       2454          0   0.001026     0.00\n",
      "NOTE:      6   400    0.001           0.8305     0.8355  1.306e+04       2571          0   0.001026     0.00\n",
      "NOTE:      7   400    0.001           0.9513     0.8182  1.335e+04       2965          0   0.001027     0.00\n",
      "NOTE:      8   400    0.001            1.044     0.8469  1.472e+04       2661          0   0.001027     0.00\n",
      "NOTE:      9   400    0.001            1.007     0.8275  1.478e+04       3080          0   0.001027     0.00\n",
      "NOTE:     10   400    0.001           0.8482     0.7781  1.177e+04       3355          0   0.001027     0.00\n",
      "NOTE:     11   400    0.001           0.9796     0.8544  1.475e+04       2513          0   0.001027     0.00\n",
      "NOTE:     12   400    0.001           0.8783     0.8387  1.316e+04       2530          0   0.001027     0.00\n",
      "NOTE:     13   400    0.001           0.9863     0.8672  1.631e+04       2499          0   0.001027     0.00\n",
      "NOTE:     14   400    0.001           0.9819     0.8357  1.453e+04       2857          0   0.001027     0.00\n",
      "NOTE:     15   400    0.001            0.985     0.8064  1.348e+04       3236          0   0.001027     0.00\n",
      "NOTE:     16   400    0.001           0.8455     0.8366   1.38e+04       2696          0   0.001027     0.00\n",
      "NOTE:     17   400    0.001            0.878     0.8024  1.274e+04       3136          0   0.001028     0.00\n",
      "NOTE:     18   400    0.001            1.034     0.8148  1.394e+04       3168          0   0.001028     0.00\n",
      "NOTE:     19   400    0.001           0.8107      0.805  1.161e+04       2811          0   0.001028     0.00\n",
      "NOTE:     20   400    0.001            1.034     0.8127  1.313e+04       3026          0   0.001028     0.00\n",
      "NOTE:     21   400    0.001           0.9722     0.8297  1.482e+04       3041          0   0.001028     0.00\n",
      "NOTE:     22   400    0.001           0.9697     0.8391  1.455e+04       2789          0   0.001028     0.00\n",
      "NOTE:     23   400    0.001           0.7601     0.7883       9395       2523          0   0.001028     0.00\n",
      "NOTE:     24   400    0.001           0.9291     0.8685  1.467e+04       2222          0   0.001028     0.00\n",
      "NOTE:     25   400    0.001           0.9789     0.8533  1.468e+04       2523          0   0.001028     0.00\n",
      "NOTE:     26   400    0.001           0.8538     0.7871  1.079e+04       2919          0   0.001028     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  10        0.001          0.9354     0.8302  3.676e+05  7.518e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001           0.9587     0.8222  1.414e+04       3056          0   0.001028     0.00\n",
      "NOTE:      1   400    0.001           0.9091     0.7792  1.245e+04       3527          0   0.001028     0.00\n",
      "NOTE:      2   400    0.001            1.013     0.8197  1.421e+04       3126          0   0.001028     0.00\n",
      "NOTE:      3   400    0.001           0.7914     0.8405  1.037e+04       1967          0   0.001029     0.00\n",
      "NOTE:      4   400    0.001           0.8168     0.8415  1.372e+04       2585          0   0.001029     0.00\n",
      "NOTE:      5   400    0.001            1.013     0.8209  1.427e+04       3114          0   0.001029     0.00\n",
      "NOTE:      6   400    0.001            0.974     0.8363  1.433e+04       2805          0   0.001029     0.00\n",
      "NOTE:      7   400    0.001           0.8507     0.8262  1.145e+04       2410          0   0.001029     0.00\n",
      "NOTE:      8   400    0.001           0.9544     0.8152  1.328e+04       3010          0   0.001028     0.00\n",
      "NOTE:      9   400    0.001           0.8227     0.8795  1.317e+04       1805          0   0.001028     0.00\n",
      "NOTE:     10   400    0.001           0.8633     0.8109  1.187e+04       2768          0   0.001029     0.00\n",
      "NOTE:     11   400    0.001           0.8988     0.8039  1.327e+04       3238          0   0.001029     0.00\n",
      "NOTE:     12   400    0.001           0.8595     0.8692  1.266e+04       1906          0   0.001029     0.00\n",
      "NOTE:     13   400    0.001           0.7669     0.8371  1.118e+04       2177          0   0.001029     0.00\n",
      "NOTE:     14   400    0.001           0.8483     0.8261  1.246e+04       2621          0   0.001029     0.00\n",
      "NOTE:     15   400    0.001           0.9512     0.8076  1.308e+04       3116          0   0.001029     0.00\n",
      "NOTE:     16   400    0.001           0.9799       0.79  1.387e+04       3686          0   0.001029     0.00\n",
      "NOTE:     17   400    0.001           0.9873     0.8154  1.471e+04       3330          0   0.001029     0.00\n",
      "NOTE:     18   400    0.001           0.9195     0.8286  1.364e+04       2822          0   0.001029     0.00\n",
      "NOTE:     19   400    0.001           0.8749     0.8309  1.236e+04       2515          0   0.001029     0.00\n",
      "NOTE:     20   400    0.001           0.9339     0.8732  1.481e+04       2150          0    0.00103     0.00\n",
      "NOTE:     21   400    0.001            0.805     0.8193  1.124e+04       2479          0    0.00103     0.00\n",
      "NOTE:     22   400    0.001            1.049     0.8584  1.617e+04       2668          0    0.00103     0.00\n",
      "NOTE:     23   400    0.001           0.8745     0.8074  1.294e+04       3085          0    0.00103     0.00\n",
      "NOTE:     24   400    0.001            1.067     0.8596   1.66e+04       2710          0    0.00103     0.00\n",
      "NOTE:     25   400    0.001            1.051     0.8305  1.378e+04       2812          0    0.00103     0.00\n",
      "NOTE:     26   400    0.001           0.9905     0.8438  1.453e+04       2689          0    0.00103     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  11        0.001          0.9194     0.8294  3.605e+05  7.418e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001           0.9734     0.8482  1.513e+04       2707          0   0.001031     0.00\n",
      "NOTE:      1   400    0.001           0.9441     0.8123  1.289e+04       2978          0   0.001031     0.00\n",
      "NOTE:      2   400    0.001           0.7249     0.7661       9383       2865          0   0.001031     0.00\n",
      "NOTE:      3   400    0.001            1.082     0.8844  1.734e+04       2267          0   0.001031     0.00\n",
      "NOTE:      4   400    0.001             1.04     0.8739  1.664e+04       2401          0   0.001031     0.00\n",
      "NOTE:      5   400    0.001           0.8572     0.8642  1.352e+04       2125          0   0.001032     0.00\n",
      "NOTE:      6   400    0.001           0.9488      0.843  1.447e+04       2694          0   0.001032     0.00\n",
      "NOTE:      7   400    0.001           0.8887     0.8492  1.275e+04       2265          0   0.001032     0.00\n",
      "NOTE:      8   400    0.001           0.9261     0.8474  1.377e+04       2479          0   0.001032     0.00\n",
      "NOTE:      9   400    0.001            0.799     0.8646  1.193e+04       1869          0   0.001033     0.00\n",
      "NOTE:     10   400    0.001           0.8464     0.8267  1.228e+04       2574          0   0.001033     0.00\n",
      "NOTE:     11   400    0.001           0.8647     0.8305  1.243e+04       2537          0   0.001033     0.00\n",
      "NOTE:     12   400    0.001             0.86     0.8546  1.406e+04       2392          0   0.001033     0.00\n",
      "NOTE:     13   400    0.001            0.911     0.8282   1.39e+04       2885          0   0.001034     0.00\n",
      "NOTE:     14   400    0.001           0.9224     0.8148  1.414e+04       3214          0   0.001034     0.00\n",
      "NOTE:     15   400    0.001           0.8547     0.8647  1.287e+04       2013          0   0.001034     0.00\n",
      "NOTE:     16   400    0.001            1.044     0.8545  1.629e+04       2774          0   0.001034     0.00\n",
      "NOTE:     17   400    0.001           0.9997     0.8522  1.488e+04       2581          0   0.001034     0.00\n",
      "NOTE:     18   400    0.001           0.9474      0.809  1.368e+04       3231          0   0.001035     0.00\n",
      "NOTE:     19   400    0.001           0.7953     0.8584  1.361e+04       2246          0   0.001035     0.00\n",
      "NOTE:     20   400    0.001            1.011     0.8427  1.474e+04       2750          0   0.001035     0.00\n",
      "NOTE:     21   400    0.001            1.122     0.8373  1.517e+04       2947          0   0.001035     0.00\n",
      "NOTE:     22   400    0.001           0.9276     0.8252  1.225e+04       2594          0   0.001035     0.00\n",
      "NOTE:     23   400    0.001           0.9134     0.8091  1.263e+04       2980          0   0.001035     0.00\n",
      "NOTE:     24   400    0.001           0.8151     0.8433  1.297e+04       2409          0   0.001036     0.00\n",
      "NOTE:     25   400    0.001           0.8361     0.8127   1.17e+04       2697          0   0.001036     0.00\n",
      "NOTE:     26   400    0.001           0.9738     0.8325  1.389e+04       2794          0   0.001036     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  12        0.001          0.9196     0.8401  3.693e+05  7.027e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001           0.7795     0.8222  1.138e+04       2460          0   0.001036     0.00\n",
      "NOTE:      1   400    0.001           0.8698     0.7894  1.218e+04       3249          0   0.001036     0.00\n",
      "NOTE:      2   400    0.001           0.7889     0.8646  1.123e+04       1758          0   0.001036     0.00\n",
      "NOTE:      3   400    0.001            0.904     0.8231  1.304e+04       2801          0   0.001036     0.00\n",
      "NOTE:      4   400    0.001            0.845      0.785  1.185e+04       3247          0   0.001036     0.00\n",
      "NOTE:      5   400    0.001           0.8635     0.8351   1.31e+04       2587          0   0.001036     0.00\n",
      "NOTE:      6   400    0.001           0.9887     0.7998  1.382e+04       3460          0   0.001036     0.00\n",
      "NOTE:      7   400    0.001           0.8946     0.8386  1.338e+04       2575          0   0.001037     0.00\n",
      "NOTE:      8   400    0.001           0.9712     0.8202  1.399e+04       3067          0   0.001037     0.00\n",
      "NOTE:      9   400    0.001           0.7081     0.8181       9743       2166          0   0.001037     0.00\n",
      "NOTE:     10   400    0.001           0.8402      0.863  1.334e+04       2117          0   0.001037     0.00\n",
      "NOTE:     11   400    0.001            1.124     0.8501  1.809e+04       3190          0   0.001037     0.00\n",
      "NOTE:     12   400    0.001           0.8957     0.8238  1.293e+04       2765          0   0.001037     0.00\n",
      "NOTE:     13   400    0.001           0.8086     0.8798   1.32e+04       1803          0   0.001038     0.00\n",
      "NOTE:     14   400    0.001           0.6416     0.8539       9385       1606          0   0.001038     0.00\n",
      "NOTE:     15   400    0.001           0.7453     0.8705  1.205e+04       1793          0   0.001038     0.00\n",
      "NOTE:     16   400    0.001            1.033     0.8707  1.645e+04       2443          0   0.001038     0.00\n",
      "NOTE:     17   400    0.001            1.004     0.8052   1.33e+04       3216          0   0.001039     0.00\n",
      "NOTE:     18   400    0.001           0.8007     0.8478  1.325e+04       2378          0   0.001039     0.00\n",
      "NOTE:     19   400    0.001           0.8147     0.8407  1.256e+04       2379          0   0.001039     0.00\n",
      "NOTE:     20   400    0.001           0.9458       0.84  1.363e+04       2597          0   0.001039     0.00\n",
      "NOTE:     21   400    0.001            1.065     0.8657   1.63e+04       2529          0   0.001039     0.00\n",
      "NOTE:     22   400    0.001            0.841     0.8508  1.269e+04       2226          0    0.00104     0.00\n",
      "NOTE:     23   400    0.001           0.9309     0.8778  1.519e+04       2114          0    0.00104     0.00\n",
      "NOTE:     24   400    0.001            1.035     0.8014  1.409e+04       3493          0    0.00104     0.00\n",
      "NOTE:     25   400    0.001           0.8306     0.8265  1.143e+04       2399          0    0.00104     0.00\n",
      "NOTE:     26   400    0.001            1.019     0.8696  1.543e+04       2313          0   0.001041     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  13        0.001          0.8885     0.8386   3.57e+05  6.873e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400    0.001           0.8909      0.837  1.269e+04       2471          0   0.001041     0.00\n",
      "NOTE:      1   400    0.001           0.9713     0.8397   1.49e+04       2843          0   0.001041     0.00\n",
      "NOTE:      2   400    0.001            0.946     0.8212  1.335e+04       2907          0   0.001041     0.00\n",
      "NOTE:      3   400    0.001            1.089     0.8192  1.533e+04       3382          0   0.001041     0.00\n",
      "NOTE:      4   400    0.001           0.8701     0.8731  1.477e+04       2146          0   0.001041     0.00\n",
      "NOTE:      5   400    0.001           0.9596     0.8584  1.607e+04       2652          0   0.001041     0.00\n",
      "NOTE:      6   400    0.001           0.9011     0.8497  1.291e+04       2283          0   0.001041     0.00\n",
      "NOTE:      7   400    0.001           0.9202     0.8428  1.453e+04       2709          0   0.001042     0.00\n",
      "NOTE:      8   400    0.001           0.7972     0.8885  1.363e+04       1711          0   0.001042     0.00\n",
      "NOTE:      9   400    0.001           0.9243     0.8067  1.223e+04       2931          0   0.001042     0.00\n",
      "NOTE:     10   400    0.001           0.8084     0.8515  1.293e+04       2254          0   0.001042     0.00\n",
      "NOTE:     11   400    0.001           0.9512     0.8329  1.408e+04       2824          0   0.001042     0.00\n",
      "NOTE:     12   400    0.001           0.8979      0.874  1.308e+04       1885          0   0.001043     0.00\n",
      "NOTE:     13   400    0.001           0.9252     0.8713    1.5e+04       2216          0   0.001043     0.00\n",
      "NOTE:     14   400    0.001           0.8096     0.8451  1.194e+04       2188          0   0.001043     0.00\n",
      "NOTE:     15   400    0.001           0.8711     0.8443  1.262e+04       2327          0   0.001044     0.00\n",
      "NOTE:     16   400    0.001           0.8679     0.8448  1.359e+04       2496          0   0.001044     0.00\n",
      "NOTE:     17   400    0.001            0.875     0.8753  1.436e+04       2047          0   0.001044     0.00\n",
      "NOTE:     18   400    0.001            1.102     0.8573  1.723e+04       2867          0   0.001045     0.00\n",
      "NOTE:     19   400    0.001           0.9124     0.8227  1.294e+04       2789          0   0.001045     0.00\n",
      "NOTE:     20   400    0.001           0.9344     0.8689  1.581e+04       2384          0   0.001045     0.00\n",
      "NOTE:     21   400    0.001            0.865     0.8804  1.575e+04       2140          0   0.001046     0.00\n",
      "NOTE:     22   400    0.001            1.165     0.8074  1.703e+04       4061          0   0.001046     0.00\n",
      "NOTE:     23   400    0.001           0.8983     0.8379  1.367e+04       2644          0   0.001046     0.00\n",
      "NOTE:     24   400    0.001           0.8618      0.826  1.217e+04       2563          0   0.001047     0.00\n",
      "NOTE:     25   400    0.001           0.9666     0.8202  1.389e+04       3043          0   0.001047     0.00\n",
      "NOTE:     26   400    0.001           0.9583     0.8453  1.411e+04       2582          0   0.001047     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  14        0.001          0.9237     0.8459  3.806e+05  6.935e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008            1.009     0.8566  1.488e+04       2491          0   0.001048     0.00\n",
      "NOTE:      1   400   0.0008           0.8361     0.8741  1.335e+04       1923          0   0.001048     0.00\n",
      "NOTE:      2   400   0.0008            1.115     0.8375  1.638e+04       3178          0   0.001048     0.00\n",
      "NOTE:      3   400   0.0008           0.8744     0.8798  1.399e+04       1911          0   0.001048     0.00\n",
      "NOTE:      4   400   0.0008           0.9336     0.8598   1.24e+04       2022          0   0.001049     0.00\n",
      "NOTE:      5   400   0.0008           0.8317     0.8495  1.288e+04       2281          0   0.001049     0.00\n",
      "NOTE:      6   400   0.0008           0.8881     0.8288  1.297e+04       2679          0   0.001049     0.00\n",
      "NOTE:      7   400   0.0008           0.8703     0.7798  1.115e+04       3150          0   0.001049     0.00\n",
      "NOTE:      8   400   0.0008           0.9433     0.8378  1.395e+04       2701          0   0.001049     0.00\n",
      "NOTE:      9   400   0.0008           0.9121     0.8432  1.457e+04       2710          0   0.001049     0.00\n",
      "NOTE:     10   400   0.0008           0.8934     0.8313  1.277e+04       2591          0   0.001049     0.00\n",
      "NOTE:     11   400   0.0008           0.9012     0.8837  1.483e+04       1952          0   0.001049     0.00\n",
      "NOTE:     12   400   0.0008           0.8583     0.8486  1.378e+04       2459          0    0.00105     0.00\n",
      "NOTE:     13   400   0.0008           0.9035     0.8735  1.359e+04       1969          0    0.00105     0.00\n",
      "NOTE:     14   400   0.0008           0.8652     0.8047  1.308e+04       3174          0    0.00105     0.00\n",
      "NOTE:     15   400   0.0008           0.8635     0.8466  1.339e+04       2425          0    0.00105     0.00\n",
      "NOTE:     16   400   0.0008           0.8649     0.8652  1.413e+04       2200          0    0.00105     0.00\n",
      "NOTE:     17   400   0.0008            0.812     0.8463  1.165e+04       2117          0    0.00105     0.00\n",
      "NOTE:     18   400   0.0008             0.98     0.7782  1.396e+04       3980          0    0.00105     0.00\n",
      "NOTE:     19   400   0.0008           0.7127     0.8652  1.117e+04       1741          0   0.001051     0.00\n",
      "NOTE:     20   400   0.0008           0.8352     0.8233  1.165e+04       2499          0   0.001051     0.00\n",
      "NOTE:     21   400   0.0008           0.8631     0.8496  1.298e+04       2298          0   0.001051     0.00\n",
      "NOTE:     22   400   0.0008           0.9701     0.8269  1.354e+04       2833          0   0.001051     0.00\n",
      "NOTE:     23   400   0.0008           0.8354     0.8534  1.207e+04       2072          0   0.001051     0.00\n",
      "NOTE:     24   400   0.0008            1.037     0.8204  1.404e+04       3073          0   0.001051     0.00\n",
      "NOTE:     25   400   0.0008           0.8596      0.826  1.273e+04       2683          0   0.001051     0.00\n",
      "NOTE:     26   400   0.0008           0.8357     0.7925  1.061e+04       2777          0   0.001051     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  15       0.0008          0.8928       0.84  3.565e+05  6.789e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.8732     0.8385  1.214e+04       2339          0   0.001051     0.00\n",
      "NOTE:      1   400   0.0008           0.9193     0.8353  1.334e+04       2629          0   0.001051     0.00\n",
      "NOTE:      2   400   0.0008           0.8676     0.8689  1.428e+04       2154          0   0.001051     0.00\n",
      "NOTE:      3   400   0.0008           0.9165     0.8096  1.225e+04       2882          0   0.001051     0.00\n",
      "NOTE:      4   400   0.0008           0.8266     0.8057  1.192e+04       2875          0   0.001051     0.00\n",
      "NOTE:      5   400   0.0008            0.927     0.8109  1.239e+04       2888          0   0.001051     0.00\n",
      "NOTE:      6   400   0.0008           0.9033     0.7912  1.139e+04       3005          0   0.001051     0.00\n",
      "NOTE:      7   400   0.0008            1.146     0.8494  1.746e+04       3096          0   0.001051     0.00\n",
      "NOTE:      8   400   0.0008           0.8825     0.8759  1.504e+04       2130          0   0.001051     0.00\n",
      "NOTE:      9   400   0.0008            0.902     0.8527  1.341e+04       2317          0   0.001051     0.00\n",
      "NOTE:     10   400   0.0008           0.8498     0.8291  1.119e+04       2307          0   0.001051     0.00\n",
      "NOTE:     11   400   0.0008           0.9168     0.8486  1.346e+04       2400          0   0.001051     0.00\n",
      "NOTE:     12   400   0.0008           0.8458     0.8148  1.217e+04       2767          0   0.001051     0.00\n",
      "NOTE:     13   400   0.0008           0.8387     0.8469   1.32e+04       2385          0   0.001052     0.00\n",
      "NOTE:     14   400   0.0008           0.7682     0.8362   1.15e+04       2252          0   0.001052     0.00\n",
      "NOTE:     15   400   0.0008           0.8242     0.8346  1.207e+04       2393          0   0.001052     0.00\n",
      "NOTE:     16   400   0.0008           0.9636     0.8309   1.32e+04       2687          0   0.001052     0.00\n",
      "NOTE:     17   400   0.0008           0.9058     0.8628  1.408e+04       2239          0   0.001052     0.00\n",
      "NOTE:     18   400   0.0008           0.9506     0.8314  1.411e+04       2861          0   0.001052     0.00\n",
      "NOTE:     19   400   0.0008           0.9192     0.8464  1.253e+04       2273          0   0.001052     0.00\n",
      "NOTE:     20   400   0.0008           0.8368     0.8408  1.225e+04       2319          0   0.001052     0.00\n",
      "NOTE:     21   400   0.0008           0.9331     0.8176  1.372e+04       3060          0   0.001052     0.00\n",
      "NOTE:     22   400   0.0008           0.8582     0.8588  1.327e+04       2181          0   0.001052     0.00\n",
      "NOTE:     23   400   0.0008           0.8155     0.8469  1.427e+04       2581          0   0.001052     0.00\n",
      "NOTE:     24   400   0.0008           0.8535     0.8363  1.307e+04       2559          0   0.001052     0.00\n",
      "NOTE:     25   400   0.0008           0.8195     0.8492  1.218e+04       2163          0   0.001053     0.00\n",
      "NOTE:     26   400   0.0008            1.096      0.846  1.623e+04       2954          0   0.001053     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  16       0.0008          0.8948     0.8383  3.561e+05   6.87e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008            0.996     0.8473  1.469e+04       2648          0   0.001053     0.00\n",
      "NOTE:      1   400   0.0008           0.7947     0.8546  1.204e+04       2047          0   0.001053     0.00\n",
      "NOTE:      2   400   0.0008           0.8798     0.8574  1.438e+04       2391          0   0.001053     0.00\n",
      "NOTE:      3   400   0.0008           0.9091     0.8466  1.254e+04       2272          0   0.001053     0.00\n",
      "NOTE:      4   400   0.0008           0.8471     0.8089  1.105e+04       2610          0   0.001053     0.00\n",
      "NOTE:      5   400   0.0008           0.9547     0.8119  1.336e+04       3096          0   0.001053     0.00\n",
      "NOTE:      6   400   0.0008           0.8625     0.8285  1.278e+04       2647          0   0.001053     0.00\n",
      "NOTE:      7   400   0.0008           0.7653     0.8863  1.317e+04       1690          0   0.001054     0.00\n",
      "NOTE:      8   400   0.0008           0.6799     0.8209       8535       1862          0   0.001054     0.00\n",
      "NOTE:      9   400   0.0008           0.8341     0.8437   1.22e+04       2260          0   0.001054     0.00\n",
      "NOTE:     10   400   0.0008           0.8432     0.8777  1.381e+04       1925          0   0.001054     0.00\n",
      "NOTE:     11   400   0.0008            1.004     0.8353  1.487e+04       2932          0   0.001054     0.00\n",
      "NOTE:     12   400   0.0008           0.9121     0.8646  1.456e+04       2280          0   0.001054     0.00\n",
      "NOTE:     13   400   0.0008             1.09     0.8704  1.669e+04       2485          0   0.001054     0.00\n",
      "NOTE:     14   400   0.0008           0.9845     0.8472  1.517e+04       2735          0   0.001055     0.00\n",
      "NOTE:     15   400   0.0008           0.9323     0.8253  1.377e+04       2916          0   0.001055     0.00\n",
      "NOTE:     16   400   0.0008            1.125     0.8406    1.7e+04       3224          0   0.001055     0.00\n",
      "NOTE:     17   400   0.0008            0.885     0.8808  1.478e+04       2000          0   0.001055     0.00\n",
      "NOTE:     18   400   0.0008           0.9726     0.8329  1.367e+04       2743          0   0.001056     0.00\n",
      "NOTE:     19   400   0.0008           0.8533     0.8358  1.275e+04       2505          0   0.001056     0.00\n",
      "NOTE:     20   400   0.0008           0.9635     0.8275  1.312e+04       2735          0   0.001056     0.00\n",
      "NOTE:     21   400   0.0008           0.9184     0.8199  1.349e+04       2963          0   0.001056     0.00\n",
      "NOTE:     22   400   0.0008           0.8759     0.8768  1.376e+04       1932          0   0.001056     0.00\n",
      "NOTE:     23   400   0.0008            1.067      0.859  1.652e+04       2713          0   0.001056     0.00\n",
      "NOTE:     24   400   0.0008           0.9637     0.8157  1.347e+04       3044          0   0.001057     0.00\n",
      "NOTE:     25   400   0.0008           0.8957     0.8826  1.503e+04       1999          0   0.001057     0.00\n",
      "NOTE:     26   400   0.0008           0.8409     0.8216  1.231e+04       2672          0   0.001057     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  17       0.0008           0.913     0.8459  3.695e+05  6.733e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.9545     0.8118  1.385e+04       3211          0   0.001057     0.00\n",
      "NOTE:      1   400   0.0008           0.8456     0.8602  1.298e+04       2111          0   0.001057     0.00\n",
      "NOTE:      2   400   0.0008           0.9108      0.787  1.191e+04       3223          0   0.001058     0.00\n",
      "NOTE:      3   400   0.0008            1.018     0.8281  1.485e+04       3081          0   0.001058     0.00\n",
      "NOTE:      4   400   0.0008           0.7979     0.8622   1.29e+04       2061          0   0.001058     0.00\n",
      "NOTE:      5   400   0.0008            1.036     0.8619  1.577e+04       2528          0   0.001058     0.00\n",
      "NOTE:      6   400   0.0008             1.09     0.8529  1.627e+04       2806          0   0.001058     0.00\n",
      "NOTE:      7   400   0.0008           0.9357     0.8217  1.372e+04       2976          0   0.001058     0.00\n",
      "NOTE:      8   400   0.0008           0.9934     0.8768  1.529e+04       2148          0   0.001058     0.00\n",
      "NOTE:      9   400   0.0008            0.931     0.8629  1.393e+04       2213          0   0.001059     0.00\n",
      "NOTE:     10   400   0.0008           0.8503     0.8294  1.281e+04       2634          0   0.001059     0.00\n",
      "NOTE:     11   400   0.0008           0.8155     0.8691  1.373e+04       2069          0   0.001059     0.00\n",
      "NOTE:     12   400   0.0008           0.8511     0.7962  1.094e+04       2800          0   0.001059     0.00\n",
      "NOTE:     13   400   0.0008           0.9199     0.8801  1.475e+04       2009          0   0.001059     0.00\n",
      "NOTE:     14   400   0.0008           0.9568     0.8344  1.286e+04       2552          0   0.001059     0.00\n",
      "NOTE:     15   400   0.0008           0.7244      0.841  1.088e+04       2058          0    0.00106     0.00\n",
      "NOTE:     16   400   0.0008           0.7539     0.8328  1.021e+04       2050          0    0.00106     0.00\n",
      "NOTE:     17   400   0.0008           0.8572     0.8574  1.287e+04       2141          0    0.00106     0.00\n",
      "NOTE:     18   400   0.0008            0.961     0.8841  1.557e+04       2040          0    0.00106     0.00\n",
      "NOTE:     19   400   0.0008           0.8613     0.8462  1.317e+04       2394          0    0.00106     0.00\n",
      "NOTE:     20   400   0.0008           0.9474     0.8499  1.402e+04       2476          0    0.00106     0.00\n",
      "NOTE:     21   400   0.0008           0.8575     0.8448  1.258e+04       2310          0   0.001061     0.00\n",
      "NOTE:     22   400   0.0008           0.8401     0.8979  1.403e+04       1596          0   0.001061     0.00\n",
      "NOTE:     23   400   0.0008           0.9824     0.8381  1.576e+04       3043          0   0.001061     0.00\n",
      "NOTE:     24   400   0.0008           0.9329     0.8412  1.474e+04       2783          0   0.001061     0.00\n",
      "NOTE:     25   400   0.0008           0.8112     0.8995  1.418e+04       1584          0   0.001061     0.00\n",
      "NOTE:     26   400   0.0008           0.8651     0.8488  1.336e+04       2379          0   0.001062     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  18       0.0008             0.9     0.8493  3.679e+05  6.528e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.8692     0.8656  1.282e+04       1991          0   0.001062     0.00\n",
      "NOTE:      1   400   0.0008           0.8022     0.8751  1.348e+04       1923          0   0.001062     0.00\n",
      "NOTE:      2   400   0.0008           0.8081     0.8606  1.215e+04       1968          0   0.001062     0.00\n",
      "NOTE:      3   400   0.0008           0.8878     0.8205  1.231e+04       2693          0   0.001063     0.00\n",
      "NOTE:      4   400   0.0008            1.092     0.8514  1.737e+04       3032          0   0.001063     0.00\n",
      "NOTE:      5   400   0.0008           0.8506     0.8382  1.286e+04       2481          0   0.001063     0.00\n",
      "NOTE:      6   400   0.0008           0.9151       0.87  1.444e+04       2159          0   0.001063     0.00\n",
      "NOTE:      7   400   0.0008           0.9224     0.7923  1.257e+04       3295          0   0.001064     0.00\n",
      "NOTE:      8   400   0.0008           0.8892     0.8672  1.509e+04       2310          0   0.001064     0.00\n",
      "NOTE:      9   400   0.0008           0.9448     0.8512   1.21e+04       2115          0   0.001064     0.00\n",
      "NOTE:     10   400   0.0008           0.8767     0.8638  1.327e+04       2091          0   0.001065     0.00\n",
      "NOTE:     11   400   0.0008            1.038     0.8494  1.659e+04       2941          0   0.001065     0.00\n",
      "NOTE:     12   400   0.0008           0.8721     0.8359  1.317e+04       2587          0   0.001065     0.00\n",
      "NOTE:     13   400   0.0008           0.9278     0.8963  1.485e+04       1717          0   0.001065     0.00\n",
      "NOTE:     14   400   0.0008           0.9787     0.8768  1.567e+04       2202          0   0.001066     0.00\n",
      "NOTE:     15   400   0.0008           0.9283      0.898  1.585e+04       1800          0   0.001066     0.00\n",
      "NOTE:     16   400   0.0008           0.9121     0.8737  1.298e+04       1876          0   0.001066     0.00\n",
      "NOTE:     17   400   0.0008           0.8985      0.853  1.264e+04       2178          0   0.001067     0.00\n",
      "NOTE:     18   400   0.0008           0.7578     0.8578  1.152e+04       1910          0   0.001067     0.00\n",
      "NOTE:     19   400   0.0008           0.8951     0.8276  1.388e+04       2893          0   0.001067     0.00\n",
      "NOTE:     20   400   0.0008           0.8329     0.9038  1.367e+04       1456          0   0.001068     0.00\n",
      "NOTE:     21   400   0.0008           0.8643     0.8829  1.436e+04       1904          0   0.001068     0.00\n",
      "NOTE:     22   400   0.0008            0.852     0.8359  1.336e+04       2623          0   0.001068     0.00\n",
      "NOTE:     23   400   0.0008           0.7467     0.8729  1.284e+04       1869          0   0.001069     0.00\n",
      "NOTE:     24   400   0.0008           0.7513     0.8845  1.261e+04       1647          0   0.001069     0.00\n",
      "NOTE:     25   400   0.0008           0.8286     0.8388  1.252e+04       2405          0    0.00107     0.00\n",
      "NOTE:     26   400   0.0008           0.9017      0.864  1.368e+04       2152          0    0.00107     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  19       0.0008          0.8831     0.8596  3.686e+05  6.022e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.8726      0.853   1.25e+04       2154          0    0.00107     0.00\n",
      "NOTE:      1   400   0.0008           0.7998     0.8652  1.208e+04       1882          0   0.001071     0.00\n",
      "NOTE:      2   400   0.0008           0.9026     0.8392  1.293e+04       2478          0   0.001071     0.00\n",
      "NOTE:      3   400   0.0008           0.9902     0.8601  1.447e+04       2353          0   0.001071     0.00\n",
      "NOTE:      4   400   0.0008            0.863     0.8521  1.264e+04       2194          0   0.001072     0.00\n",
      "NOTE:      5   400   0.0008           0.8863     0.8402  1.333e+04       2534          0   0.001072     0.00\n",
      "NOTE:      6   400   0.0008           0.8905     0.8275  1.276e+04       2660          0   0.001072     0.00\n",
      "NOTE:      7   400   0.0008           0.8678     0.8426  1.344e+04       2512          0   0.001072     0.00\n",
      "NOTE:      8   400   0.0008           0.9414     0.8233   1.36e+04       2919          0   0.001073     0.00\n",
      "NOTE:      9   400   0.0008           0.8338     0.8702  1.421e+04       2120          0   0.001073     0.00\n",
      "NOTE:     10   400   0.0008           0.8999     0.8851  1.684e+04       2186          0   0.001073     0.00\n",
      "NOTE:     11   400   0.0008           0.6827     0.8687  1.067e+04       1612          0   0.001073     0.00\n",
      "NOTE:     12   400   0.0008           0.9018      0.877    1.5e+04       2104          0   0.001074     0.00\n",
      "NOTE:     13   400   0.0008            0.925     0.8521  1.501e+04       2605          0   0.001074     0.00\n",
      "NOTE:     14   400   0.0008           0.8632     0.8701  1.386e+04       2069          0   0.001074     0.00\n",
      "NOTE:     15   400   0.0008           0.8406     0.8473  1.337e+04       2409          0   0.001074     0.00\n",
      "NOTE:     16   400   0.0008           0.7836     0.8827    1.2e+04       1594          0   0.001075     0.00\n",
      "NOTE:     17   400   0.0008           0.8091     0.8706  1.343e+04       1996          0   0.001075     0.00\n",
      "NOTE:     18   400   0.0008           0.8916     0.8869  1.563e+04       1994          0   0.001075     0.00\n",
      "NOTE:     19   400   0.0008           0.7796     0.8483  1.131e+04       2022          0   0.001076     0.00\n",
      "NOTE:     20   400   0.0008            1.017     0.8187  1.364e+04       3021          0   0.001076     0.00\n",
      "NOTE:     21   400   0.0008           0.9955     0.8373   1.48e+04       2876          0   0.001076     0.00\n",
      "NOTE:     22   400   0.0008           0.8023     0.8373  1.182e+04       2296          0   0.001077     0.00\n",
      "NOTE:     23   400   0.0008           0.8231     0.8466  1.201e+04       2176          0   0.001077     0.00\n",
      "NOTE:     24   400   0.0008           0.7626     0.8424  1.038e+04       1942          0   0.001077     0.00\n",
      "NOTE:     25   400   0.0008           0.9407     0.9173  1.698e+04       1530          0   0.001077     0.00\n",
      "NOTE:     26   400   0.0008            1.023     0.8316  1.494e+04       3025          0   0.001078     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  20       0.0008          0.8737     0.8558  3.636e+05  6.126e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.8299     0.8679  1.286e+04       1957          0   0.001078     0.00\n",
      "NOTE:      1   400   0.0008           0.8986       0.83  1.344e+04       2754          0   0.001078     0.00\n",
      "NOTE:      2   400   0.0008           0.8277     0.8028  1.143e+04       2807          0   0.001078     0.00\n",
      "NOTE:      3   400   0.0008            0.901      0.822  1.212e+04       2625          0   0.001078     0.00\n",
      "NOTE:      4   400   0.0008           0.7813     0.8661  1.148e+04       1776          0   0.001079     0.00\n",
      "NOTE:      5   400   0.0008           0.8421     0.8355  1.249e+04       2459          0   0.001079     0.00\n",
      "NOTE:      6   400   0.0008           0.7511     0.8073  1.149e+04       2743          0   0.001079     0.00\n",
      "NOTE:      7   400   0.0008           0.9644     0.8203  1.272e+04       2787          0   0.001079     0.00\n",
      "NOTE:      8   400   0.0008           0.7886     0.8314  1.232e+04       2499          0   0.001079     0.00\n",
      "NOTE:      9   400   0.0008           0.9588     0.8456  1.353e+04       2471          0   0.001079     0.00\n",
      "NOTE:     10   400   0.0008           0.7989     0.8763  1.302e+04       1838          0   0.001079     0.00\n",
      "NOTE:     11   400   0.0008           0.8313     0.8218  1.068e+04       2317          0   0.001079     0.00\n",
      "NOTE:     12   400   0.0008           0.9602     0.8665  1.416e+04       2181          0   0.001079     0.00\n",
      "NOTE:     13   400   0.0008            1.021     0.8692   1.53e+04       2302          0   0.001079     0.00\n",
      "NOTE:     14   400   0.0008           0.8444     0.8224  1.277e+04       2757          0    0.00108     0.00\n",
      "NOTE:     15   400   0.0008            1.023     0.8523  1.553e+04       2692          0    0.00108     0.00\n",
      "NOTE:     16   400   0.0008            0.812     0.9044  1.354e+04       1431          0    0.00108     0.00\n",
      "NOTE:     17   400   0.0008           0.9047     0.8208  1.285e+04       2805          0    0.00108     0.00\n",
      "NOTE:     18   400   0.0008           0.9156      0.836  1.441e+04       2828          0    0.00108     0.00\n",
      "NOTE:     19   400   0.0008           0.8322       0.83   1.35e+04       2765          0    0.00108     0.00\n",
      "NOTE:     20   400   0.0008           0.9564     0.7937  1.269e+04       3300          0   0.001081     0.00\n",
      "NOTE:     21   400   0.0008            1.125      0.867  1.772e+04       2717          0   0.001081     0.00\n",
      "NOTE:     22   400   0.0008           0.8239     0.8471   1.16e+04       2094          0   0.001081     0.00\n",
      "NOTE:     23   400   0.0008           0.9038     0.8261  1.277e+04       2689          0   0.001081     0.00\n",
      "NOTE:     24   400   0.0008           0.9172     0.8245  1.285e+04       2736          0   0.001081     0.00\n",
      "NOTE:     25   400   0.0008           0.9018      0.883  1.567e+04       2075          0   0.001082     0.00\n",
      "NOTE:     26   400   0.0008           0.8611     0.8469  1.335e+04       2413          0   0.001082     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  21       0.0008           0.888     0.8421  3.563e+05  6.682e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.8159     0.8816  1.387e+04       1863          0   0.001082     0.00\n",
      "NOTE:      1   400   0.0008           0.9493     0.8161  1.423e+04       3207          0   0.001082     0.00\n",
      "NOTE:      2   400   0.0008           0.9064     0.8418  1.424e+04       2676          0   0.001083     0.00\n",
      "NOTE:      3   400   0.0008           0.8911     0.8932  1.572e+04       1880          0   0.001083     0.00\n",
      "NOTE:      4   400   0.0008            0.865     0.8346  1.301e+04       2578          0   0.001083     0.00\n",
      "NOTE:      5   400   0.0008           0.8834     0.8419  1.335e+04       2506          0   0.001083     0.00\n",
      "NOTE:      6   400   0.0008           0.9384     0.8687  1.409e+04       2129          0   0.001084     0.00\n",
      "NOTE:      7   400   0.0008           0.9918     0.8301  1.486e+04       3042          0   0.001084     0.00\n",
      "NOTE:      8   400   0.0008           0.9227     0.8829  1.629e+04       2160          0   0.001084     0.00\n",
      "NOTE:      9   400   0.0008            1.001     0.8767  1.638e+04       2303          0   0.001084     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     10   400   0.0008           0.8796     0.8167  1.231e+04       2763          0   0.001085     0.00\n",
      "NOTE:     11   400   0.0008           0.7661      0.822  1.085e+04       2349          0   0.001085     0.00\n",
      "NOTE:     12   400   0.0008           0.8858     0.8387  1.467e+04       2821          0   0.001085     0.00\n",
      "NOTE:     13   400   0.0008           0.8179     0.8762  1.328e+04       1876          0   0.001085     0.00\n",
      "NOTE:     14   400   0.0008           0.8714     0.8561  1.284e+04       2158          0   0.001086     0.00\n",
      "NOTE:     15   400   0.0008           0.7989     0.8523  1.259e+04       2182          0   0.001086     0.00\n",
      "NOTE:     16   400   0.0008           0.8419     0.8436  1.199e+04       2223          0   0.001086     0.00\n",
      "NOTE:     17   400   0.0008           0.8602     0.8561  1.372e+04       2307          0   0.001086     0.00\n",
      "NOTE:     18   400   0.0008           0.9182     0.8369  1.398e+04       2726          0   0.001087     0.00\n",
      "NOTE:     19   400   0.0008           0.7651     0.8294  1.172e+04       2410          0   0.001087     0.00\n",
      "NOTE:     20   400   0.0008            0.988     0.8775  1.568e+04       2189          0   0.001087     0.00\n",
      "NOTE:     21   400   0.0008           0.9075     0.8392  1.405e+04       2692          0   0.001088     0.00\n",
      "NOTE:     22   400   0.0008           0.9367      0.814  1.292e+04       2953          0   0.001088     0.00\n",
      "NOTE:     23   400   0.0008           0.9419     0.8345  1.377e+04       2730          0   0.001088     0.00\n",
      "NOTE:     24   400   0.0008           0.8891     0.8787  1.292e+04       1783          0   0.001088     0.00\n",
      "NOTE:     25   400   0.0008           0.7519     0.8516  1.231e+04       2145          0   0.001088     0.00\n",
      "NOTE:     26   400   0.0008           0.8587     0.8826  1.476e+04       1963          0   0.001088     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  22       0.0008          0.8831     0.8515  3.704e+05  6.461e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.7577      0.859  1.178e+04       1934          0   0.001089     0.00\n",
      "NOTE:      1   400   0.0008           0.7198     0.8295  1.102e+04       2264          0   0.001089     0.00\n",
      "NOTE:      2   400   0.0008            0.944     0.8599  1.461e+04       2380          0   0.001089     0.00\n",
      "NOTE:      3   400   0.0008           0.8817     0.8645  1.446e+04       2267          0   0.001089     0.00\n",
      "NOTE:      4   400   0.0008           0.8486     0.8075  1.295e+04       3088          0    0.00109     0.00\n",
      "NOTE:      5   400   0.0008           0.6528     0.8653       9633       1499          0    0.00109     0.00\n",
      "NOTE:      6   400   0.0008           0.7799     0.8598  1.284e+04       2095          0    0.00109     0.00\n",
      "NOTE:      7   400   0.0008           0.9373     0.8897   1.58e+04       1958          0    0.00109     0.00\n",
      "NOTE:      8   400   0.0008           0.8327     0.8764  1.335e+04       1884          0   0.001091     0.00\n",
      "NOTE:      9   400   0.0008            1.026     0.8785  1.575e+04       2178          0   0.001091     0.00\n",
      "NOTE:     10   400   0.0008           0.9108     0.8675  1.502e+04       2295          0   0.001091     0.00\n",
      "NOTE:     11   400   0.0008            0.933      0.856   1.45e+04       2440          0   0.001091     0.00\n",
      "NOTE:     12   400   0.0008             0.73     0.8957  1.226e+04       1427          0   0.001092     0.00\n",
      "NOTE:     13   400   0.0008           0.9161     0.8639   1.46e+04       2300          0   0.001092     0.00\n",
      "NOTE:     14   400   0.0008           0.9899     0.8527  1.566e+04       2705          0   0.001093     0.00\n",
      "NOTE:     15   400   0.0008            0.786     0.8399  1.141e+04       2174          0   0.001093     0.00\n",
      "NOTE:     16   400   0.0008           0.9143     0.8226  1.286e+04       2773          0   0.001093     0.00\n",
      "NOTE:     17   400   0.0008            0.778      0.855  1.164e+04       1973          0   0.001094     0.00\n",
      "NOTE:     18   400   0.0008            1.021     0.8594  1.417e+04       2319          0   0.001094     0.00\n",
      "NOTE:     19   400   0.0008           0.7132     0.8801  1.215e+04       1655          0   0.001094     0.00\n",
      "NOTE:     20   400   0.0008           0.7847     0.8272  1.172e+04       2448          0   0.001095     0.00\n",
      "NOTE:     21   400   0.0008           0.9177     0.8694  1.323e+04       1986          0   0.001095     0.00\n",
      "NOTE:     22   400   0.0008           0.8799     0.8799  1.395e+04       1903          0   0.001095     0.00\n",
      "NOTE:     23   400   0.0008            0.836     0.8753  1.288e+04       1834          0   0.001096     0.00\n",
      "NOTE:     24   400   0.0008           0.9405      0.867  1.496e+04       2296          0   0.001096     0.00\n",
      "NOTE:     25   400   0.0008           0.9534     0.8447  1.378e+04       2534          0   0.001096     0.00\n",
      "NOTE:     26   400   0.0008           0.9197     0.9082  1.681e+04       1698          0   0.001096     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  23       0.0008          0.8631     0.8619  3.638e+05  5.831e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.8441     0.9201  1.525e+04       1324          0   0.001097     0.00\n",
      "NOTE:      1   400   0.0008            0.877     0.8227  1.233e+04       2657          0   0.001097     0.00\n",
      "NOTE:      2   400   0.0008           0.8426     0.8326  1.193e+04       2397          0   0.001097     0.00\n",
      "NOTE:      3   400   0.0008           0.8091     0.7942  1.132e+04       2933          0   0.001098     0.00\n",
      "NOTE:      4   400   0.0008           0.8189     0.8637  1.344e+04       2121          0   0.001098     0.00\n",
      "NOTE:      5   400   0.0008           0.9467     0.8485  1.537e+04       2743          0   0.001098     0.00\n",
      "NOTE:      6   400   0.0008           0.8814     0.8683  1.391e+04       2110          0   0.001099     0.00\n",
      "NOTE:      7   400   0.0008           0.8757     0.8058  1.267e+04       3054          0   0.001099     0.00\n",
      "NOTE:      8   400   0.0008            1.037     0.8483   1.58e+04       2826          0   0.001099     0.00\n",
      "NOTE:      9   400   0.0008           0.7906     0.8258  1.044e+04       2203          0   0.001099     0.00\n",
      "NOTE:     10   400   0.0008           0.9135     0.8862  1.604e+04       2060          0   0.001099     0.00\n",
      "NOTE:     11   400   0.0008            0.842     0.8352  1.254e+04       2475          0     0.0011     0.00\n",
      "NOTE:     12   400   0.0008            0.926     0.8225  1.333e+04       2878          0     0.0011     0.00\n",
      "NOTE:     13   400   0.0008            0.992     0.8378  1.478e+04       2863          0     0.0011     0.00\n",
      "NOTE:     14   400   0.0008           0.8901     0.8423  1.348e+04       2524          0     0.0011     0.00\n",
      "NOTE:     15   400   0.0008           0.9156     0.8677  1.438e+04       2193          0     0.0011     0.00\n",
      "NOTE:     16   400   0.0008            1.012     0.8034  1.382e+04       3382          0     0.0011     0.00\n",
      "NOTE:     17   400   0.0008           0.8085     0.8252  1.136e+04       2406          0     0.0011     0.00\n",
      "NOTE:     18   400   0.0008           0.8482     0.8748   1.44e+04       2061          0   0.001101     0.00\n",
      "NOTE:     19   400   0.0008           0.8946     0.8959   1.43e+04       1662          0   0.001101     0.00\n",
      "NOTE:     20   400   0.0008           0.8014     0.8536  1.197e+04       2053          0   0.001101     0.00\n",
      "NOTE:     21   400   0.0008           0.8939     0.8115   1.16e+04       2695          0   0.001101     0.00\n",
      "NOTE:     22   400   0.0008           0.7518     0.8297  1.091e+04       2240          0   0.001101     0.00\n",
      "NOTE:     23   400   0.0008           0.7919       0.85  1.265e+04       2231          0   0.001101     0.00\n",
      "NOTE:     24   400   0.0008           0.7615     0.8299  1.193e+04       2446          0   0.001101     0.00\n",
      "NOTE:     25   400   0.0008           0.9424     0.8445  1.438e+04       2647          0   0.001101     0.00\n",
      "NOTE:     26   400   0.0008           0.9523     0.8498  1.517e+04       2681          0   0.001102     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  24       0.0008          0.8763     0.8452  3.595e+05  6.587e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008             1.06     0.8697   1.65e+04       2471          0   0.001102     0.00\n",
      "NOTE:      1   400   0.0008             1.02     0.8616  1.586e+04       2548          0   0.001102     0.00\n",
      "NOTE:      2   400   0.0008           0.9221     0.8368  1.361e+04       2655          0   0.001102     0.00\n",
      "NOTE:      3   400   0.0008           0.9184     0.8101  1.238e+04       2902          0   0.001102     0.00\n",
      "NOTE:      4   400   0.0008           0.9214     0.8361  1.412e+04       2768          0   0.001102     0.00\n",
      "NOTE:      5   400   0.0008           0.9335     0.8181   1.32e+04       2935          0   0.001102     0.00\n",
      "NOTE:      6   400   0.0008           0.9392     0.8942  1.593e+04       1884          0   0.001102     0.00\n",
      "NOTE:      7   400   0.0008           0.9442     0.8358  1.409e+04       2769          0   0.001102     0.00\n",
      "NOTE:      8   400   0.0008           0.9153     0.8578  1.397e+04       2315          0   0.001102     0.00\n",
      "NOTE:      9   400   0.0008            0.834     0.8939  1.452e+04       1724          0   0.001103     0.00\n",
      "NOTE:     10   400   0.0008           0.8507     0.8928  1.324e+04       1590          0   0.001103     0.00\n",
      "NOTE:     11   400   0.0008           0.8845      0.864  1.403e+04       2209          0   0.001103     0.00\n",
      "NOTE:     12   400   0.0008           0.8058     0.8715  1.304e+04       1923          0   0.001103     0.00\n",
      "NOTE:     13   400   0.0008           0.9274     0.8253  1.406e+04       2975          0   0.001104     0.00\n",
      "NOTE:     14   400   0.0008           0.8579     0.9241  1.492e+04       1225          0   0.001104     0.00\n",
      "NOTE:     15   400   0.0008           0.8002     0.8493  1.232e+04       2187          0   0.001104     0.00\n",
      "NOTE:     16   400   0.0008            1.116     0.8537  1.653e+04       2832          0   0.001105     0.00\n",
      "NOTE:     17   400   0.0008           0.7869     0.8771  1.307e+04       1830          0   0.001105     0.00\n",
      "NOTE:     18   400   0.0008            0.918     0.8658  1.474e+04       2285          0   0.001105     0.00\n",
      "NOTE:     19   400   0.0008           0.8421     0.8567  1.274e+04       2131          0   0.001106     0.00\n",
      "NOTE:     20   400   0.0008           0.8065      0.827  1.166e+04       2439          0   0.001106     0.00\n",
      "NOTE:     21   400   0.0008           0.8931     0.8738  1.412e+04       2039          0   0.001106     0.00\n",
      "NOTE:     22   400   0.0008           0.8368     0.8405  1.287e+04       2442          0   0.001107     0.00\n",
      "NOTE:     23   400   0.0008           0.8785      0.855  1.271e+04       2156          0   0.001107     0.00\n",
      "NOTE:     24   400   0.0008           0.8149     0.8813  1.285e+04       1731          0   0.001107     0.00\n",
      "NOTE:     25   400   0.0008           0.9358     0.8712  1.464e+04       2163          0   0.001108     0.00\n",
      "NOTE:     26   400   0.0008             0.87      0.836  1.328e+04       2604          0   0.001108     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  25       0.0008          0.8975     0.8586   3.75e+05  6.173e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.8119     0.8247  1.152e+04       2448          0   0.001108     0.00\n",
      "NOTE:      1   400   0.0008            0.928     0.8726  1.404e+04       2049          0   0.001109     0.00\n",
      "NOTE:      2   400   0.0008           0.9235     0.8545   1.38e+04       2350          0   0.001109     0.00\n",
      "NOTE:      3   400   0.0008           0.7973     0.8686   1.23e+04       1861          0   0.001109     0.00\n",
      "NOTE:      4   400   0.0008           0.8281     0.8315  1.215e+04       2462          0   0.001109     0.00\n",
      "NOTE:      5   400   0.0008           0.8225     0.8353  1.287e+04       2537          0    0.00111     0.00\n",
      "NOTE:      6   400   0.0008           0.8096     0.8904  1.391e+04       1712          0    0.00111     0.00\n",
      "NOTE:      7   400   0.0008           0.9902     0.8366  1.505e+04       2940          0    0.00111     0.00\n",
      "NOTE:      8   400   0.0008            1.021     0.9003  1.652e+04       1830          0    0.00111     0.00\n",
      "NOTE:      9   400   0.0008           0.8495     0.8596  1.417e+04       2313          0   0.001111     0.00\n",
      "NOTE:     10   400   0.0008           0.8249     0.8201  1.228e+04       2695          0   0.001111     0.00\n",
      "NOTE:     11   400   0.0008           0.9075     0.8996  1.542e+04       1720          0   0.001111     0.00\n",
      "NOTE:     12   400   0.0008            0.974     0.8352  1.452e+04       2865          0   0.001111     0.00\n",
      "NOTE:     13   400   0.0008           0.7831     0.8308  1.176e+04       2395          0   0.001112     0.00\n",
      "NOTE:     14   400   0.0008           0.8923     0.8398  1.273e+04       2427          0   0.001112     0.00\n",
      "NOTE:     15   400   0.0008            1.023     0.8244  1.442e+04       3073          0   0.001112     0.00\n",
      "NOTE:     16   400   0.0008           0.9423     0.8904  1.379e+04       1697          0   0.001113     0.00\n",
      "NOTE:     17   400   0.0008           0.9346     0.8293  1.373e+04       2826          0   0.001113     0.00\n",
      "NOTE:     18   400   0.0008             1.05     0.8507   1.61e+04       2825          0   0.001113     0.00\n",
      "NOTE:     19   400   0.0008           0.8885     0.7914  1.315e+04       3465          0   0.001113     0.00\n",
      "NOTE:     20   400   0.0008           0.9688     0.8723  1.472e+04       2155          0   0.001113     0.00\n",
      "NOTE:     21   400   0.0008           0.8546     0.8405  1.197e+04       2271          0   0.001113     0.00\n",
      "NOTE:     22   400   0.0008           0.7838     0.8787  1.259e+04       1738          0   0.001114     0.00\n",
      "NOTE:     23   400   0.0008             1.03     0.8562  1.485e+04       2494          0   0.001114     0.00\n",
      "NOTE:     24   400   0.0008            0.908       0.87  1.396e+04       2086          0   0.001114     0.00\n",
      "NOTE:     25   400   0.0008           0.9522     0.8675  1.333e+04       2037          0   0.001114     0.00\n",
      "NOTE:     26   400   0.0008           0.8978     0.8951  1.681e+04       1970          0   0.001114     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  26       0.0008          0.9036     0.8549  3.725e+05  6.324e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.8703     0.8514  1.394e+04       2433          0   0.001114     0.00\n",
      "NOTE:      1   400   0.0008            1.051     0.8399  1.558e+04       2970          0   0.001115     0.00\n",
      "NOTE:      2   400   0.0008           0.8711     0.8533  1.375e+04       2363          0   0.001115     0.00\n",
      "NOTE:      3   400   0.0008            1.001     0.8417  1.483e+04       2789          0   0.001115     0.00\n",
      "NOTE:      4   400   0.0008           0.8349     0.8373  1.207e+04       2344          0   0.001115     0.00\n",
      "NOTE:      5   400   0.0008           0.8571     0.8369  1.293e+04       2520          0   0.001116     0.00\n",
      "NOTE:      6   400   0.0008           0.7204     0.8572   1.06e+04       1767          0   0.001116     0.00\n",
      "NOTE:      7   400   0.0008           0.8652     0.9156  1.466e+04       1352          0   0.001116     0.00\n",
      "NOTE:      8   400   0.0008           0.8758     0.8333  1.311e+04       2622          0   0.001116     0.00\n",
      "NOTE:      9   400   0.0008            1.002     0.8747  1.732e+04       2481          0   0.001116     0.00\n",
      "NOTE:     10   400   0.0008           0.7942     0.8485  1.146e+04       2046          0   0.001117     0.00\n",
      "NOTE:     11   400   0.0008            0.815     0.9111  1.455e+04       1419          0   0.001117     0.00\n",
      "NOTE:     12   400   0.0008           0.8002     0.8308  1.126e+04       2294          0   0.001117     0.00\n",
      "NOTE:     13   400   0.0008            1.013     0.8303  1.467e+04       2998          0   0.001118     0.00\n",
      "NOTE:     14   400   0.0008            1.072     0.8709  1.677e+04       2485          0   0.001118     0.00\n",
      "NOTE:     15   400   0.0008            1.111     0.8288  1.523e+04       3145          0   0.001118     0.00\n",
      "NOTE:     16   400   0.0008           0.9244     0.8543   1.45e+04       2472          0   0.001118     0.00\n",
      "NOTE:     17   400   0.0008           0.7848      0.863  1.178e+04       1870          0   0.001118     0.00\n",
      "NOTE:     18   400   0.0008           0.8406     0.8744  1.334e+04       1917          0   0.001118     0.00\n",
      "NOTE:     19   400   0.0008            0.863     0.8857  1.361e+04       1757          0   0.001119     0.00\n",
      "NOTE:     20   400   0.0008           0.8403     0.8599  1.406e+04       2292          0   0.001119     0.00\n",
      "NOTE:     21   400   0.0008            0.941     0.8661  1.394e+04       2156          0   0.001119     0.00\n",
      "NOTE:     22   400   0.0008           0.9007     0.8374  1.385e+04       2690          0   0.001119     0.00\n",
      "NOTE:     23   400   0.0008           0.9049     0.8534   1.33e+04       2285          0    0.00112     0.00\n",
      "NOTE:     24   400   0.0008           0.8891     0.8661  1.376e+04       2128          0    0.00112     0.00\n",
      "NOTE:     25   400   0.0008           0.9364     0.8926  1.625e+04       1955          0    0.00112     0.00\n",
      "NOTE:     26   400   0.0008           0.8802     0.8474  1.281e+04       2306          0    0.00112     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  27       0.0008          0.8985      0.858  3.739e+05  6.186e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.8893     0.8578  1.368e+04       2268          0    0.00112     0.00\n",
      "NOTE:      1   400   0.0008           0.8905     0.8581  1.253e+04       2072          0    0.00112     0.00\n",
      "NOTE:      2   400   0.0008           0.8896     0.8563  1.469e+04       2465          0   0.001121     0.00\n",
      "NOTE:      3   400   0.0008           0.8729     0.8437  1.284e+04       2380          0   0.001121     0.00\n",
      "NOTE:      4   400   0.0008           0.9872     0.8648  1.559e+04       2437          0   0.001121     0.00\n",
      "NOTE:      5   400   0.0008           0.9191     0.8677  1.442e+04       2199          0   0.001121     0.00\n",
      "NOTE:      6   400   0.0008           0.9423     0.8485   1.43e+04       2554          0   0.001122     0.00\n",
      "NOTE:      7   400   0.0008            0.915     0.8277  1.372e+04       2857          0   0.001122     0.00\n",
      "NOTE:      8   400   0.0008           0.9751     0.8785  1.593e+04       2202          0   0.001122     0.00\n",
      "NOTE:      9   400   0.0008           0.9511     0.8371  1.451e+04       2823          0   0.001122     0.00\n",
      "NOTE:     10   400   0.0008           0.9484     0.8705  1.516e+04       2255          0   0.001122     0.00\n",
      "NOTE:     11   400   0.0008           0.7618     0.8467   1.16e+04       2100          0   0.001123     0.00\n",
      "NOTE:     12   400   0.0008            1.011     0.8382  1.598e+04       3085          0   0.001123     0.00\n",
      "NOTE:     13   400   0.0008            0.862     0.8227  1.292e+04       2785          0   0.001123     0.00\n",
      "NOTE:     14   400   0.0008           0.9018     0.8289   1.28e+04       2641          0   0.001123     0.00\n",
      "NOTE:     15   400   0.0008           0.8708     0.8711  1.519e+04       2249          0   0.001123     0.00\n",
      "NOTE:     16   400   0.0008             1.01     0.8638   1.52e+04       2396          0   0.001123     0.00\n",
      "NOTE:     17   400   0.0008           0.8774     0.8311  1.292e+04       2625          0   0.001124     0.00\n",
      "NOTE:     18   400   0.0008           0.8471     0.8721  1.357e+04       1990          0   0.001124     0.00\n",
      "NOTE:     19   400   0.0008           0.9005      0.831  1.164e+04       2367          0   0.001124     0.00\n",
      "NOTE:     20   400   0.0008           0.8486     0.9072  1.532e+04       1567          0   0.001124     0.00\n",
      "NOTE:     21   400   0.0008           0.8479      0.842  1.288e+04       2416          0   0.001124     0.00\n",
      "NOTE:     22   400   0.0008           0.8488     0.8777  1.321e+04       1841          0   0.001124     0.00\n",
      "NOTE:     23   400   0.0008           0.8381     0.8507   1.23e+04       2159          0   0.001125     0.00\n",
      "NOTE:     24   400   0.0008           0.8535     0.8708  1.374e+04       2040          0   0.001125     0.00\n",
      "NOTE:     25   400   0.0008            1.057     0.8257  1.553e+04       3278          0   0.001125     0.00\n",
      "NOTE:     26   400   0.0008           0.8436     0.8583  1.345e+04       2221          0   0.001125     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  28       0.0008          0.9022     0.8539  3.756e+05  6.427e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400   0.0008           0.9519     0.8656  1.494e+04       2321          0   0.001126     0.00\n",
      "NOTE:      1   400   0.0008            1.091     0.8523  1.648e+04       2855          0   0.001126     0.00\n",
      "NOTE:      2   400   0.0008            0.937     0.8167  1.344e+04       3015          0   0.001126     0.00\n",
      "NOTE:      3   400   0.0008           0.8707     0.8642  1.416e+04       2224          0   0.001126     0.00\n",
      "NOTE:      4   400   0.0008           0.9575     0.8156  1.271e+04       2873          0   0.001127     0.00\n",
      "NOTE:      5   400   0.0008           0.9829     0.8604  1.436e+04       2330          0   0.001127     0.00\n",
      "NOTE:      6   400   0.0008           0.8846     0.8508  1.384e+04       2426          0   0.001127     0.00\n",
      "NOTE:      7   400   0.0008            0.897     0.8517  1.501e+04       2612          0   0.001127     0.00\n",
      "NOTE:      8   400   0.0008            1.066     0.8255  1.573e+04       3325          0   0.001127     0.00\n",
      "NOTE:      9   400   0.0008            0.791     0.8593  1.338e+04       2190          0   0.001128     0.00\n",
      "NOTE:     10   400   0.0008           0.8085     0.8694  1.371e+04       2059          0   0.001128     0.00\n",
      "NOTE:     11   400   0.0008           0.8151     0.8576  1.228e+04       2038          0   0.001128     0.00\n",
      "NOTE:     12   400   0.0008           0.7637     0.9009  1.394e+04       1533          0   0.001128     0.00\n",
      "NOTE:     13   400   0.0008           0.8829     0.8576  1.305e+04       2168          0   0.001128     0.00\n",
      "NOTE:     14   400   0.0008           0.8359     0.9067   1.56e+04       1606          0   0.001129     0.00\n",
      "NOTE:     15   400   0.0008           0.9274     0.8778  1.599e+04       2226          0   0.001129     0.00\n",
      "NOTE:     16   400   0.0008            1.022     0.8483  1.597e+04       2856          0   0.001129     0.00\n",
      "NOTE:     17   400   0.0008           0.8298     0.8205  1.175e+04       2571          0    0.00113     0.00\n",
      "NOTE:     18   400   0.0008           0.8618     0.8285  1.264e+04       2616          0    0.00113     0.00\n",
      "NOTE:     19   400   0.0008            1.035     0.8802  1.826e+04       2484          0    0.00113     0.00\n",
      "NOTE:     20   400   0.0008           0.9471      0.851   1.47e+04       2575          0    0.00113     0.00\n",
      "NOTE:     21   400   0.0008           0.9366     0.8498   1.35e+04       2386          0   0.001131     0.00\n",
      "NOTE:     22   400   0.0008           0.9433     0.8106  1.268e+04       2964          0   0.001131     0.00\n",
      "NOTE:     23   400   0.0008           0.8201     0.8524  1.372e+04       2374          0   0.001131     0.00\n",
      "NOTE:     24   400   0.0008           0.8792     0.8726  1.354e+04       1978          0   0.001131     0.00\n",
      "NOTE:     25   400   0.0008           0.9592     0.7932  1.206e+04       3144          0   0.001132     0.00\n",
      "NOTE:     26   400   0.0008           0.8536     0.8922  1.507e+04       1820          0   0.001132     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  29       0.0008          0.9093     0.8537  3.825e+05  6.557e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00064           0.8961      0.857  1.345e+04       2243          0   0.001132     0.00\n",
      "NOTE:      1   400  0.00064           0.8821     0.8293   1.33e+04       2738          0   0.001132     0.00\n",
      "NOTE:      2   400  0.00064           0.9244     0.8633  1.428e+04       2262          0   0.001133     0.00\n",
      "NOTE:      3   400  0.00064           0.8667     0.8485  1.239e+04       2211          0   0.001133     0.00\n",
      "NOTE:      4   400  0.00064           0.9079     0.8315  1.418e+04       2872          0   0.001133     0.00\n",
      "NOTE:      5   400  0.00064           0.8544     0.8482  1.343e+04       2403          0   0.001133     0.00\n",
      "NOTE:      6   400  0.00064           0.7984     0.8853  1.333e+04       1728          0   0.001133     0.00\n",
      "NOTE:      7   400  0.00064           0.8671     0.8747  1.376e+04       1970          0   0.001134     0.00\n",
      "NOTE:      8   400  0.00064            0.872     0.8144  1.202e+04       2739          0   0.001134     0.00\n",
      "NOTE:      9   400  0.00064           0.8809     0.8292  1.251e+04       2577          0   0.001134     0.00\n",
      "NOTE:     10   400  0.00064           0.9509     0.8695  1.448e+04       2173          0   0.001134     0.00\n",
      "NOTE:     11   400  0.00064           0.7148     0.8622  1.198e+04       1915          0   0.001134     0.00\n",
      "NOTE:     12   400  0.00064           0.7992      0.863  1.282e+04       2035          0   0.001134     0.00\n",
      "NOTE:     13   400  0.00064           0.8878     0.8628    1.5e+04       2384          0   0.001135     0.00\n",
      "NOTE:     14   400  0.00064           0.7811     0.8613  1.143e+04       1840          0   0.001135     0.00\n",
      "NOTE:     15   400  0.00064           0.9162     0.8775  1.511e+04       2109          0   0.001135     0.00\n",
      "NOTE:     16   400  0.00064           0.8924     0.8494  1.381e+04       2447          0   0.001135     0.00\n",
      "NOTE:     17   400  0.00064           0.9044     0.8654  1.545e+04       2403          0   0.001135     0.00\n",
      "NOTE:     18   400  0.00064           0.9003     0.8677  1.545e+04       2355          0   0.001136     0.00\n",
      "NOTE:     19   400  0.00064           0.7811     0.8962  1.418e+04       1643          0   0.001136     0.00\n",
      "NOTE:     20   400  0.00064           0.8006     0.8743  1.183e+04       1701          0   0.001136     0.00\n",
      "NOTE:     21   400  0.00064           0.9265     0.8522  1.351e+04       2344          0   0.001136     0.00\n",
      "NOTE:     22   400  0.00064           0.8362     0.8669  1.425e+04       2187          0   0.001136     0.00\n",
      "NOTE:     23   400  0.00064           0.8547     0.8903  1.286e+04       1584          0   0.001137     0.00\n",
      "NOTE:     24   400  0.00064            1.025     0.8618  1.551e+04       2487          0   0.001137     0.00\n",
      "NOTE:     25   400  0.00064           0.8372     0.8305  1.228e+04       2506          0   0.001137     0.00\n",
      "NOTE:     26   400  0.00064           0.8247     0.8777  1.281e+04       1786          0   0.001137     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  30       0.0006           0.866     0.8597  3.654e+05  5.964e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00064           0.9069     0.8835  1.531e+04       2018          0   0.001138     0.00\n",
      "NOTE:      1   400  0.00064           0.9499     0.8722  1.551e+04       2272          0   0.001138     0.00\n",
      "NOTE:      2   400  0.00064           0.9067     0.8863  1.412e+04       1811          0   0.001138     0.00\n",
      "NOTE:      3   400  0.00064           0.8688     0.8332  1.185e+04       2371          0   0.001138     0.00\n",
      "NOTE:      4   400  0.00064            1.043     0.8887  1.651e+04       2067          0   0.001138     0.00\n",
      "NOTE:      5   400  0.00064           0.9906     0.8755  1.612e+04       2291          0   0.001139     0.00\n",
      "NOTE:      6   400  0.00064           0.8088     0.8709  1.209e+04       1791          0   0.001139     0.00\n",
      "NOTE:      7   400  0.00064            1.094     0.8377  1.645e+04       3188          0   0.001139     0.00\n",
      "NOTE:      8   400  0.00064            1.051      0.807  1.345e+04       3217          0   0.001139     0.00\n",
      "NOTE:      9   400  0.00064            0.935      0.867  1.436e+04       2202          0   0.001139     0.00\n",
      "NOTE:     10   400  0.00064           0.8972     0.8709  1.367e+04       2026          0   0.001139     0.00\n",
      "NOTE:     11   400  0.00064           0.8453     0.8711  1.324e+04       1959          0    0.00114     0.00\n",
      "NOTE:     12   400  0.00064           0.9158     0.7965    1.3e+04       3320          0    0.00114     0.00\n",
      "NOTE:     13   400  0.00064           0.8822      0.863   1.43e+04       2269          0    0.00114     0.00\n",
      "NOTE:     14   400  0.00064           0.8629     0.8351  1.202e+04       2373          0    0.00114     0.00\n",
      "NOTE:     15   400  0.00064           0.8987     0.8773  1.575e+04       2203          0    0.00114     0.00\n",
      "NOTE:     16   400  0.00064           0.8246     0.8647  1.224e+04       1914          0    0.00114     0.00\n",
      "NOTE:     17   400  0.00064           0.8833     0.8606  1.491e+04       2415          0    0.00114     0.00\n",
      "NOTE:     18   400  0.00064           0.9812     0.8511  1.524e+04       2665          0    0.00114     0.00\n",
      "NOTE:     19   400  0.00064           0.8826     0.8346  1.304e+04       2584          0    0.00114     0.00\n",
      "NOTE:     20   400  0.00064           0.9009     0.8617  1.347e+04       2162          0   0.001141     0.00\n",
      "NOTE:     21   400  0.00064             1.02     0.8459  1.593e+04       2903          0   0.001141     0.00\n",
      "NOTE:     22   400  0.00064           0.5828     0.8837       9344       1230          0   0.001141     0.00\n",
      "NOTE:     23   400  0.00064           0.9721     0.8357   1.52e+04       2987          0   0.001141     0.00\n",
      "NOTE:     24   400  0.00064           0.9683     0.8368  1.461e+04       2849          0   0.001141     0.00\n",
      "NOTE:     25   400  0.00064           0.8578      0.839  1.327e+04       2547          0   0.001141     0.00\n",
      "NOTE:     26   400  0.00064           0.8987     0.8605  1.288e+04       2088          0   0.001141     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  31       0.0006          0.9122     0.8557  3.779e+05  6.372e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00064           0.9164     0.8402  1.343e+04       2554          0   0.001141     0.00\n",
      "NOTE:      1   400  0.00064           0.9482     0.8328  1.382e+04       2774          0   0.001141     0.00\n",
      "NOTE:      2   400  0.00064           0.7801     0.7961  1.124e+04       2880          0   0.001141     0.00\n",
      "NOTE:      3   400  0.00064            0.862     0.8384  1.249e+04       2408          0   0.001141     0.00\n",
      "NOTE:      4   400  0.00064           0.7899     0.8568  1.256e+04       2100          0   0.001142     0.00\n",
      "NOTE:      5   400  0.00064           0.9798     0.8479   1.31e+04       2351          0   0.001142     0.00\n",
      "NOTE:      6   400  0.00064             1.01     0.8917  1.606e+04       1951          0   0.001142     0.00\n",
      "NOTE:      7   400  0.00064           0.9046     0.8767  1.529e+04       2150          0   0.001142     0.00\n",
      "NOTE:      8   400  0.00064           0.8318     0.8401   1.35e+04       2570          0   0.001142     0.00\n",
      "NOTE:      9   400  0.00064           0.8872     0.8449  1.341e+04       2463          0   0.001142     0.00\n",
      "NOTE:     10   400  0.00064           0.7408     0.8078       9431       2244          0   0.001142     0.00\n",
      "NOTE:     11   400  0.00064           0.8396     0.8664  1.442e+04       2223          0   0.001142     0.00\n",
      "NOTE:     12   400  0.00064             1.01     0.8677  1.547e+04       2359          0   0.001142     0.00\n",
      "NOTE:     13   400  0.00064           0.7941     0.8552  1.164e+04       1970          0   0.001142     0.00\n",
      "NOTE:     14   400  0.00064            0.948     0.8267  1.436e+04       3011          0   0.001142     0.00\n",
      "NOTE:     15   400  0.00064            1.128     0.8251  1.538e+04       3259          0   0.001142     0.00\n",
      "NOTE:     16   400  0.00064           0.8534      0.871  1.273e+04       1884          0   0.001142     0.00\n",
      "NOTE:     17   400  0.00064           0.8886     0.9035  1.535e+04       1640          0   0.001143     0.00\n",
      "NOTE:     18   400  0.00064           0.7355     0.8476  1.117e+04       2008          0   0.001143     0.00\n",
      "NOTE:     19   400  0.00064           0.8385     0.7828  1.102e+04       3059          0   0.001143     0.00\n",
      "NOTE:     20   400  0.00064           0.8584     0.8307  1.253e+04       2554          0   0.001143     0.00\n",
      "NOTE:     21   400  0.00064           0.9917     0.8662   1.53e+04       2363          0   0.001143     0.00\n",
      "NOTE:     22   400  0.00064           0.9339     0.8445   1.45e+04       2670          0   0.001143     0.00\n",
      "NOTE:     23   400  0.00064           0.7868     0.8872  1.299e+04       1652          0   0.001143     0.00\n",
      "NOTE:     24   400  0.00064            0.842     0.8397   1.22e+04       2329          0   0.001143     0.00\n",
      "NOTE:     25   400  0.00064           0.8203     0.8329  1.216e+04       2440          0   0.001143     0.00\n",
      "NOTE:     26   400  0.00064             1.06     0.8236  1.506e+04       3225          0   0.001143     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  32       0.0006          0.8882     0.8471  3.606e+05  6.509e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00064           0.9604     0.8643  1.459e+04       2291          0   0.001143     0.00\n",
      "NOTE:      1   400  0.00064           0.9006      0.803  1.232e+04       3022          0   0.001143     0.00\n",
      "NOTE:      2   400  0.00064           0.9258     0.8634  1.553e+04       2456          0   0.001143     0.00\n",
      "NOTE:      3   400  0.00064            0.822     0.8604  1.245e+04       2019          0   0.001143     0.00\n",
      "NOTE:      4   400  0.00064            1.025     0.8341  1.455e+04       2894          0   0.001143     0.00\n",
      "NOTE:      5   400  0.00064           0.9116     0.8794  1.408e+04       1931          0   0.001144     0.00\n",
      "NOTE:      6   400  0.00064           0.8819     0.8944  1.582e+04       1867          0   0.001144     0.00\n",
      "NOTE:      7   400  0.00064            0.786     0.8467  1.201e+04       2174          0   0.001144     0.00\n",
      "NOTE:      8   400  0.00064            1.051     0.8704  1.613e+04       2403          0   0.001144     0.00\n",
      "NOTE:      9   400  0.00064            1.065     0.7981  1.535e+04       3883          0   0.001144     0.00\n",
      "NOTE:     10   400  0.00064            0.816     0.8344  1.299e+04       2579          0   0.001144     0.00\n",
      "NOTE:     11   400  0.00064           0.9652      0.877  1.473e+04       2066          0   0.001145     0.00\n",
      "NOTE:     12   400  0.00064           0.8318      0.848  1.243e+04       2227          0   0.001145     0.00\n",
      "NOTE:     13   400  0.00064           0.8571     0.8743  1.397e+04       2008          0   0.001145     0.00\n",
      "NOTE:     14   400  0.00064            0.683     0.8715  1.127e+04       1661          0   0.001145     0.00\n",
      "NOTE:     15   400  0.00064           0.8301     0.8468    1.2e+04       2170          0   0.001145     0.00\n",
      "NOTE:     16   400  0.00064            1.052     0.8402  1.452e+04       2762          0   0.001146     0.00\n",
      "NOTE:     17   400  0.00064           0.9386     0.8542  1.403e+04       2395          0   0.001146     0.00\n",
      "NOTE:     18   400  0.00064           0.8872     0.8922  1.506e+04       1820          0   0.001146     0.00\n",
      "NOTE:     19   400  0.00064           0.6956     0.8899  1.126e+04       1393          0   0.001146     0.00\n",
      "NOTE:     20   400  0.00064           0.9724     0.8653  1.565e+04       2436          0   0.001146     0.00\n",
      "NOTE:     21   400  0.00064           0.9634     0.8287  1.339e+04       2767          0   0.001146     0.00\n",
      "NOTE:     22   400  0.00064           0.7919     0.8785   1.42e+04       1964          0   0.001146     0.00\n",
      "NOTE:     23   400  0.00064           0.9526     0.8763  1.379e+04       1947          0   0.001147     0.00\n",
      "NOTE:     24   400  0.00064           0.9268     0.8556  1.329e+04       2243          0   0.001147     0.00\n",
      "NOTE:     25   400  0.00064           0.9319     0.8745   1.56e+04       2239          0   0.001147     0.00\n",
      "NOTE:     26   400  0.00064           0.9897     0.8541  1.526e+04       2606          0   0.001147     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  33       0.0006          0.9042     0.8581  3.762e+05  6.222e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00064           0.9892     0.8561  1.434e+04       2411          0   0.001147     0.00\n",
      "NOTE:      1   400  0.00064           0.8275     0.8281  1.254e+04       2602          0   0.001147     0.00\n",
      "NOTE:      2   400  0.00064            0.893      0.857  1.411e+04       2354          0   0.001147     0.00\n",
      "NOTE:      3   400  0.00064           0.7983     0.8297  1.028e+04       2110          0   0.001148     0.00\n",
      "NOTE:      4   400  0.00064            0.946     0.8809  1.494e+04       2020          0   0.001148     0.00\n",
      "NOTE:      5   400  0.00064           0.7881     0.8832  1.344e+04       1777          0   0.001148     0.00\n",
      "NOTE:      6   400  0.00064           0.7958     0.8294  1.272e+04       2617          0   0.001148     0.00\n",
      "NOTE:      7   400  0.00064           0.8479     0.8244  1.216e+04       2590          0   0.001148     0.00\n",
      "NOTE:      8   400  0.00064           0.8958     0.8272  1.261e+04       2635          0   0.001148     0.00\n",
      "NOTE:      9   400  0.00064           0.8298      0.859  1.377e+04       2261          0   0.001148     0.00\n",
      "NOTE:     10   400  0.00064            0.759      0.852  1.181e+04       2051          0   0.001149     0.00\n",
      "NOTE:     11   400  0.00064            1.042     0.8613  1.586e+04       2553          0   0.001149     0.00\n",
      "NOTE:     12   400  0.00064           0.8083     0.8678  1.262e+04       1921          0   0.001149     0.00\n",
      "NOTE:     13   400  0.00064            1.032      0.847  1.502e+04       2714          0   0.001149     0.00\n",
      "NOTE:     14   400  0.00064           0.9362     0.8401  1.333e+04       2537          0   0.001149     0.00\n",
      "NOTE:     15   400  0.00064           0.9866     0.8691  1.615e+04       2432          0   0.001149     0.00\n",
      "NOTE:     16   400  0.00064           0.9683     0.8375  1.416e+04       2747          0   0.001149     0.00\n",
      "NOTE:     17   400  0.00064           0.7239     0.8548  1.097e+04       1863          0    0.00115     0.00\n",
      "NOTE:     18   400  0.00064           0.8579     0.8634  1.285e+04       2033          0    0.00115     0.00\n",
      "NOTE:     19   400  0.00064           0.9155     0.8359  1.284e+04       2520          0    0.00115     0.00\n",
      "NOTE:     20   400  0.00064           0.8296     0.8316  1.189e+04       2407          0    0.00115     0.00\n",
      "NOTE:     21   400  0.00064           0.8779     0.8424  1.258e+04       2355          0    0.00115     0.00\n",
      "NOTE:     22   400  0.00064           0.7282     0.8651  1.107e+04       1726          0    0.00115     0.00\n",
      "NOTE:     23   400  0.00064            0.969     0.7946  1.328e+04       3432          0    0.00115     0.00\n",
      "NOTE:     24   400  0.00064           0.8028     0.8472  1.228e+04       2215          0   0.001151     0.00\n",
      "NOTE:     25   400  0.00064           0.8952     0.8321  1.304e+04       2631          0   0.001151     0.00\n",
      "NOTE:     26   400  0.00064           0.7951      0.849  1.202e+04       2137          0   0.001151     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  34       0.0006          0.8718     0.8471  3.527e+05  6.365e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00064           0.8236     0.8671  1.205e+04       1846          0   0.001151     0.00\n",
      "NOTE:      1   400  0.00064           0.7869     0.8849  1.301e+04       1693          0   0.001151     0.00\n",
      "NOTE:      2   400  0.00064           0.8686     0.8942  1.472e+04       1742          0   0.001151     0.00\n",
      "NOTE:      3   400  0.00064           0.9123     0.8478  1.298e+04       2330          0   0.001151     0.00\n",
      "NOTE:      4   400  0.00064           0.8324     0.8294  1.274e+04       2619          0   0.001151     0.00\n",
      "NOTE:      5   400  0.00064           0.8457     0.8923  1.357e+04       1638          0   0.001151     0.00\n",
      "NOTE:      6   400  0.00064            1.073      0.861  1.684e+04       2719          0   0.001152     0.00\n",
      "NOTE:      7   400  0.00064           0.9629     0.8581  1.541e+04       2547          0   0.001152     0.00\n",
      "NOTE:      8   400  0.00064           0.9087     0.8583  1.316e+04       2172          0   0.001152     0.00\n",
      "NOTE:      9   400  0.00064           0.7946     0.8555  1.299e+04       2194          0   0.001152     0.00\n",
      "NOTE:     10   400  0.00064           0.9898     0.8367  1.433e+04       2798          0   0.001152     0.00\n",
      "NOTE:     11   400  0.00064           0.8559     0.8521  1.301e+04       2258          0   0.001152     0.00\n",
      "NOTE:     12   400  0.00064            1.012     0.8788  1.616e+04       2229          0   0.001152     0.00\n",
      "NOTE:     13   400  0.00064           0.9847     0.8729   1.58e+04       2301          0   0.001153     0.00\n",
      "NOTE:     14   400  0.00064           0.8294     0.8792  1.554e+04       2135          0   0.001153     0.00\n",
      "NOTE:     15   400  0.00064           0.8781     0.8556  1.343e+04       2268          0   0.001153     0.00\n",
      "NOTE:     16   400  0.00064            1.006     0.8409  1.468e+04       2777          0   0.001153     0.00\n",
      "NOTE:     17   400  0.00064            1.092      0.843  1.607e+04       2994          0   0.001153     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     18   400  0.00064           0.9358     0.8296  1.364e+04       2801          0   0.001154     0.00\n",
      "NOTE:     19   400  0.00064           0.8067     0.8483    1.3e+04       2325          0   0.001154     0.00\n",
      "NOTE:     20   400  0.00064           0.9265     0.8727  1.537e+04       2243          0   0.001154     0.00\n",
      "NOTE:     21   400  0.00064           0.8115     0.8664  1.357e+04       2093          0   0.001154     0.00\n",
      "NOTE:     22   400  0.00064            0.795     0.8685  1.272e+04       1925          0   0.001154     0.00\n",
      "NOTE:     23   400  0.00064           0.9989     0.8361  1.494e+04       2929          0   0.001155     0.00\n",
      "NOTE:     24   400  0.00064           0.7856     0.8786  1.235e+04       1706          0   0.001155     0.00\n",
      "NOTE:     25   400  0.00064            0.746     0.8682  1.236e+04       1876          0   0.001155     0.00\n",
      "NOTE:     26   400  0.00064           0.9225     0.8493  1.366e+04       2424          0   0.001155     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  35       0.0006          0.8957     0.8599  3.781e+05  6.158e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00064            0.949     0.8796  1.516e+04       2074          0   0.001155     0.00\n",
      "NOTE:      1   400  0.00064           0.9687     0.8476  1.421e+04       2554          0   0.001156     0.00\n",
      "NOTE:      2   400  0.00064           0.8895     0.8497  1.321e+04       2336          0   0.001156     0.00\n",
      "NOTE:      3   400  0.00064            1.013     0.8155  1.454e+04       3290          0   0.001156     0.00\n",
      "NOTE:      4   400  0.00064            0.887     0.8947  1.467e+04       1726          0   0.001156     0.00\n",
      "NOTE:      5   400  0.00064            0.841     0.8773  1.317e+04       1841          0   0.001156     0.00\n",
      "NOTE:      6   400  0.00064           0.8114     0.8091  1.134e+04       2675          0   0.001156     0.00\n",
      "NOTE:      7   400  0.00064           0.9372     0.8665  1.483e+04       2285          0   0.001157     0.00\n",
      "NOTE:      8   400  0.00064            0.956      0.838  1.417e+04       2740          0   0.001157     0.00\n",
      "NOTE:      9   400  0.00064           0.7733     0.8586  1.241e+04       2043          0   0.001157     0.00\n",
      "NOTE:     10   400  0.00064            0.776     0.8503  1.133e+04       1996          0   0.001157     0.00\n",
      "NOTE:     11   400  0.00064           0.8209     0.7937  1.099e+04       2856          0   0.001157     0.00\n",
      "NOTE:     12   400  0.00064           0.9262     0.8546  1.379e+04       2346          0   0.001157     0.00\n",
      "NOTE:     13   400  0.00064           0.8346     0.8711  1.381e+04       2043          0   0.001157     0.00\n",
      "NOTE:     14   400  0.00064           0.9089      0.862  1.444e+04       2313          0   0.001157     0.00\n",
      "NOTE:     15   400  0.00064           0.9412     0.8694  1.481e+04       2226          0   0.001158     0.00\n",
      "NOTE:     16   400  0.00064           0.9094     0.8653   1.49e+04       2319          0   0.001158     0.00\n",
      "NOTE:     17   400  0.00064           0.9228     0.8696  1.527e+04       2289          0   0.001158     0.00\n",
      "NOTE:     18   400  0.00064           0.8407      0.875  1.422e+04       2032          0   0.001158     0.00\n",
      "NOTE:     19   400  0.00064            1.015     0.8465   1.57e+04       2848          0   0.001158     0.00\n",
      "NOTE:     20   400  0.00064           0.8037      0.859  1.244e+04       2041          0   0.001158     0.00\n",
      "NOTE:     21   400  0.00064           0.7941     0.8046  1.062e+04       2579          0   0.001158     0.00\n",
      "NOTE:     22   400  0.00064           0.9951     0.8742  1.681e+04       2418          0   0.001159     0.00\n",
      "NOTE:     23   400  0.00064            0.893     0.8611  1.381e+04       2227          0   0.001159     0.00\n",
      "NOTE:     24   400  0.00064           0.8965     0.8648  1.391e+04       2176          0   0.001159     0.00\n",
      "NOTE:     25   400  0.00064            1.043     0.8674    1.6e+04       2447          0   0.001159     0.00\n",
      "NOTE:     26   400  0.00064            1.144     0.8681  1.845e+04       2804          0   0.001159     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  36       0.0006          0.9071     0.8565   3.79e+05  6.352e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00064           0.8617     0.8825  1.407e+04       1873          0   0.001159     0.00\n",
      "NOTE:      1   400  0.00064           0.8089     0.8425  1.218e+04       2277          0    0.00116     0.00\n",
      "NOTE:      2   400  0.00064            0.835     0.8738  1.361e+04       1965          0    0.00116     0.00\n",
      "NOTE:      3   400  0.00064           0.9717     0.8299  1.357e+04       2783          0    0.00116     0.00\n",
      "NOTE:      4   400  0.00064           0.9638      0.855  1.511e+04       2562          0    0.00116     0.00\n",
      "NOTE:      5   400  0.00064           0.9076      0.865  1.447e+04       2259          0    0.00116     0.00\n",
      "NOTE:      6   400  0.00064           0.8127     0.8576  1.332e+04       2211          0   0.001161     0.00\n",
      "NOTE:      7   400  0.00064           0.9899     0.8997  1.691e+04       1884          0   0.001161     0.00\n",
      "NOTE:      8   400  0.00064           0.8134     0.8389  1.211e+04       2326          0   0.001161     0.00\n",
      "NOTE:      9   400  0.00064           0.8262     0.8353  1.231e+04       2427          0   0.001161     0.00\n",
      "NOTE:     10   400  0.00064            1.033     0.8509  1.592e+04       2791          0   0.001162     0.00\n",
      "NOTE:     11   400  0.00064           0.8996     0.8682   1.44e+04       2186          0   0.001162     0.00\n",
      "NOTE:     12   400  0.00064           0.7981     0.8559  1.192e+04       2007          0   0.001162     0.00\n",
      "NOTE:     13   400  0.00064           0.8509     0.8514  1.303e+04       2275          0   0.001162     0.00\n",
      "NOTE:     14   400  0.00064           0.9773     0.8819  1.657e+04       2220          0   0.001162     0.00\n",
      "NOTE:     15   400  0.00064           0.7682      0.874  1.232e+04       1776          0   0.001162     0.00\n",
      "NOTE:     16   400  0.00064           0.7407     0.8334  1.158e+04       2315          0   0.001163     0.00\n",
      "NOTE:     17   400  0.00064           0.7422     0.8407  1.174e+04       2223          0   0.001163     0.00\n",
      "NOTE:     18   400  0.00064           0.9654     0.8458  1.449e+04       2641          0   0.001163     0.00\n",
      "NOTE:     19   400  0.00064           0.9591     0.8768  1.556e+04       2185          0   0.001163     0.00\n",
      "NOTE:     20   400  0.00064            0.808      0.886  1.438e+04       1850          0   0.001163     0.00\n",
      "NOTE:     21   400  0.00064           0.9411     0.8291   1.28e+04       2639          0   0.001164     0.00\n",
      "NOTE:     22   400  0.00064           0.9719     0.8579  1.558e+04       2581          0   0.001164     0.00\n",
      "NOTE:     23   400  0.00064           0.8787      0.863  1.304e+04       2070          0   0.001164     0.00\n",
      "NOTE:     24   400  0.00064           0.9613     0.8639  1.469e+04       2314          0   0.001164     0.00\n",
      "NOTE:     25   400  0.00064            1.111     0.7861  1.487e+04       4046          0   0.001164     0.00\n",
      "NOTE:     26   400  0.00064           0.9832     0.8223  1.359e+04       2935          0   0.001165     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  37       0.0006          0.8956     0.8547  3.741e+05  6.362e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00064           0.9965     0.8688  1.488e+04       2246          0   0.001165     0.00\n",
      "NOTE:      1   400  0.00064           0.8261     0.8455  1.277e+04       2334          0   0.001165     0.00\n",
      "NOTE:      2   400  0.00064            1.101     0.8794  1.832e+04       2512          0   0.001165     0.00\n",
      "NOTE:      3   400  0.00064           0.9684     0.8953  1.656e+04       1936          0   0.001165     0.00\n",
      "NOTE:      4   400  0.00064           0.9017     0.8617  1.362e+04       2186          0   0.001165     0.00\n",
      "NOTE:      5   400  0.00064           0.8234     0.8587  1.281e+04       2107          0   0.001165     0.00\n",
      "NOTE:      6   400  0.00064           0.8317     0.8816  1.379e+04       1853          0   0.001166     0.00\n",
      "NOTE:      7   400  0.00064           0.7789     0.8905  1.246e+04       1532          0   0.001166     0.00\n",
      "NOTE:      8   400  0.00064           0.8612     0.9007  1.407e+04       1552          0   0.001166     0.00\n",
      "NOTE:      9   400  0.00064            1.103     0.8368  1.627e+04       3173          0   0.001166     0.00\n",
      "NOTE:     10   400  0.00064           0.9208     0.7893  1.215e+04       3242          0   0.001167     0.00\n",
      "NOTE:     11   400  0.00064            0.914     0.8927  1.528e+04       1836          0   0.001167     0.00\n",
      "NOTE:     12   400  0.00064           0.9274     0.8722  1.474e+04       2159          0   0.001167     0.00\n",
      "NOTE:     13   400  0.00064           0.9562     0.8707  1.397e+04       2075          0   0.001168     0.00\n",
      "NOTE:     14   400  0.00064           0.8748     0.8821  1.451e+04       1940          0   0.001168     0.00\n",
      "NOTE:     15   400  0.00064           0.9241     0.8573  1.413e+04       2352          0   0.001168     0.00\n",
      "NOTE:     16   400  0.00064            0.875     0.9175  1.556e+04       1399          0   0.001168     0.00\n",
      "NOTE:     17   400  0.00064           0.8626     0.8508  1.265e+04       2218          0   0.001169     0.00\n",
      "NOTE:     18   400  0.00064           0.7979     0.8338  1.137e+04       2266          0   0.001169     0.00\n",
      "NOTE:     19   400  0.00064           0.8061     0.8573  1.372e+04       2285          0   0.001169     0.00\n",
      "NOTE:     20   400  0.00064           0.8356     0.8412  1.321e+04       2494          0   0.001169     0.00\n",
      "NOTE:     21   400  0.00064           0.9096     0.8467  1.502e+04       2719          0    0.00117     0.00\n",
      "NOTE:     22   400  0.00064           0.9974      0.864  1.465e+04       2305          0    0.00117     0.00\n",
      "NOTE:     23   400  0.00064           0.7831     0.8257  1.105e+04       2333          0    0.00117     0.00\n",
      "NOTE:     24   400  0.00064           0.9301     0.8236  1.281e+04       2743          0    0.00117     0.00\n",
      "NOTE:     25   400  0.00064           0.7887     0.8309   1.16e+04       2362          0    0.00117     0.00\n",
      "NOTE:     26   400  0.00064           0.8568     0.8262  1.231e+04       2589          0    0.00117     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  38       0.0006          0.8945     0.8604  3.743e+05  6.075e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.8544     0.8443   1.31e+04       2417          0    0.00117     0.00\n",
      "NOTE:      1   400 0.000512           0.8168     0.8783  1.263e+04       1750          0   0.001171     0.00\n",
      "NOTE:      2   400 0.000512           0.9831     0.8726  1.598e+04       2333          0   0.001171     0.00\n",
      "NOTE:      3   400 0.000512           0.8269     0.8485  1.235e+04       2206          0   0.001171     0.00\n",
      "NOTE:      4   400 0.000512           0.8879     0.8258  1.322e+04       2789          0   0.001171     0.00\n",
      "NOTE:      5   400 0.000512           0.9172     0.8391  1.293e+04       2479          0   0.001171     0.00\n",
      "NOTE:      6   400 0.000512            1.088     0.8797  1.743e+04       2384          0   0.001171     0.00\n",
      "NOTE:      7   400 0.000512           0.9141     0.8095  1.177e+04       2770          0   0.001171     0.00\n",
      "NOTE:      8   400 0.000512            1.105     0.8293  1.536e+04       3162          0   0.001171     0.00\n",
      "NOTE:      9   400 0.000512           0.8361     0.8469  1.211e+04       2189          0   0.001172     0.00\n",
      "NOTE:     10   400 0.000512           0.8116     0.8631   1.33e+04       2110          0   0.001172     0.00\n",
      "NOTE:     11   400 0.000512            1.061     0.8611  1.717e+04       2770          0   0.001172     0.00\n",
      "NOTE:     12   400 0.000512           0.9413     0.8551  1.408e+04       2385          0   0.001172     0.00\n",
      "NOTE:     13   400 0.000512            0.956     0.8504  1.473e+04       2590          0   0.001172     0.00\n",
      "NOTE:     14   400 0.000512           0.9344     0.8636  1.498e+04       2365          0   0.001172     0.00\n",
      "NOTE:     15   400 0.000512           0.8951     0.8597  1.523e+04       2487          0   0.001172     0.00\n",
      "NOTE:     16   400 0.000512           0.8073     0.8785  1.375e+04       1901          0   0.001172     0.00\n",
      "NOTE:     17   400 0.000512            0.932     0.8696  1.486e+04       2229          0   0.001172     0.00\n",
      "NOTE:     18   400 0.000512           0.8474     0.8921  1.447e+04       1750          0   0.001172     0.00\n",
      "NOTE:     19   400 0.000512           0.9807     0.8887  1.522e+04       1905          0   0.001173     0.00\n",
      "NOTE:     20   400 0.000512           0.9432      0.848   1.43e+04       2562          0   0.001173     0.00\n",
      "NOTE:     21   400 0.000512           0.9542     0.8749  1.491e+04       2132          0   0.001173     0.00\n",
      "NOTE:     22   400 0.000512           0.8628     0.8088  1.261e+04       2981          0   0.001173     0.00\n",
      "NOTE:     23   400 0.000512           0.8944     0.8756   1.43e+04       2032          0   0.001173     0.00\n",
      "NOTE:     24   400 0.000512           0.9783     0.8337  1.425e+04       2842          0   0.001173     0.00\n",
      "NOTE:     25   400 0.000512           0.9188     0.8846  1.448e+04       1890          0   0.001173     0.00\n",
      "NOTE:     26   400 0.000512            1.046     0.8071  1.455e+04       3478          0   0.001174     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  39       0.0005          0.9257     0.8555  3.841e+05  6.489e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.8677     0.8478  1.317e+04       2363          0   0.001174     0.00\n",
      "NOTE:      1   400 0.000512           0.8875     0.8596  1.395e+04       2279          0   0.001174     0.00\n",
      "NOTE:      2   400 0.000512           0.8636      0.852  1.312e+04       2279          0   0.001174     0.00\n",
      "NOTE:      3   400 0.000512           0.9331     0.8629  1.495e+04       2374          0   0.001174     0.00\n",
      "NOTE:      4   400 0.000512           0.8773     0.8693  1.428e+04       2146          0   0.001174     0.00\n",
      "NOTE:      5   400 0.000512           0.9669      0.849   1.59e+04       2827          0   0.001174     0.00\n",
      "NOTE:      6   400 0.000512           0.8914     0.8507  1.362e+04       2390          0   0.001174     0.00\n",
      "NOTE:      7   400 0.000512           0.8897     0.8419  1.205e+04       2263          0   0.001175     0.00\n",
      "NOTE:      8   400 0.000512           0.8923     0.8072  1.346e+04       3215          0   0.001175     0.00\n",
      "NOTE:      9   400 0.000512           0.8733     0.8534   1.32e+04       2268          0   0.001175     0.00\n",
      "NOTE:     10   400 0.000512           0.8798     0.8637  1.397e+04       2205          0   0.001175     0.00\n",
      "NOTE:     11   400 0.000512            0.845     0.8277  1.149e+04       2391          0   0.001175     0.00\n",
      "NOTE:     12   400 0.000512           0.8897     0.8764  1.519e+04       2143          0   0.001175     0.00\n",
      "NOTE:     13   400 0.000512           0.8174     0.8891  1.374e+04       1713          0   0.001175     0.00\n",
      "NOTE:     14   400 0.000512           0.9718     0.8603  1.376e+04       2235          0   0.001175     0.00\n",
      "NOTE:     15   400 0.000512           0.7999     0.8836   1.25e+04       1647          0   0.001175     0.00\n",
      "NOTE:     16   400 0.000512           0.9948     0.8509  1.635e+04       2866          0   0.001176     0.00\n",
      "NOTE:     17   400 0.000512           0.9445     0.8817  1.529e+04       2052          0   0.001176     0.00\n",
      "NOTE:     18   400 0.000512           0.9335     0.8831  1.477e+04       1954          0   0.001176     0.00\n",
      "NOTE:     19   400 0.000512           0.8645     0.8677  1.361e+04       2075          0   0.001176     0.00\n",
      "NOTE:     20   400 0.000512            0.952     0.8889  1.613e+04       2016          0   0.001176     0.00\n",
      "NOTE:     21   400 0.000512           0.7858      0.874  1.365e+04       1968          0   0.001176     0.00\n",
      "NOTE:     22   400 0.000512           0.8712     0.8229    1.3e+04       2798          0   0.001177     0.00\n",
      "NOTE:     23   400 0.000512           0.9615     0.8421  1.363e+04       2555          0   0.001177     0.00\n",
      "NOTE:     24   400 0.000512           0.9865     0.8632  1.553e+04       2461          0   0.001177     0.00\n",
      "NOTE:     25   400 0.000512           0.9265     0.8215  1.415e+04       3073          0   0.001177     0.00\n",
      "NOTE:     26   400 0.000512           0.8503     0.8655  1.314e+04       2041          0   0.001177     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  40       0.0005          0.8969     0.8578  3.776e+05   6.26e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512            0.901     0.8924  1.358e+04       1637          0   0.001178     0.00\n",
      "NOTE:      1   400 0.000512           0.8802     0.8231  1.284e+04       2759          0   0.001178     0.00\n",
      "NOTE:      2   400 0.000512           0.8536     0.8392  1.283e+04       2457          0   0.001178     0.00\n",
      "NOTE:      3   400 0.000512           0.7359     0.8832  1.162e+04       1537          0   0.001178     0.00\n",
      "NOTE:      4   400 0.000512           0.7645     0.8804  1.244e+04       1690          0   0.001178     0.00\n",
      "NOTE:      5   400 0.000512           0.8398     0.8501  1.263e+04       2228          0   0.001178     0.00\n",
      "NOTE:      6   400 0.000512           0.9885     0.8877  1.708e+04       2161          0   0.001178     0.00\n",
      "NOTE:      7   400 0.000512           0.9534     0.8593  1.585e+04       2595          0   0.001179     0.00\n",
      "NOTE:      8   400 0.000512           0.8595     0.8455  1.282e+04       2342          0   0.001179     0.00\n",
      "NOTE:      9   400 0.000512           0.7961     0.8484  1.231e+04       2200          0   0.001179     0.00\n",
      "NOTE:     10   400 0.000512           0.9442     0.8724  1.497e+04       2190          0   0.001179     0.00\n",
      "NOTE:     11   400 0.000512            0.888     0.8734  1.338e+04       1940          0   0.001179     0.00\n",
      "NOTE:     12   400 0.000512           0.8606     0.8671  1.408e+04       2159          0    0.00118     0.00\n",
      "NOTE:     13   400 0.000512           0.8654     0.8302  1.289e+04       2638          0    0.00118     0.00\n",
      "NOTE:     14   400 0.000512           0.9028     0.8478  1.383e+04       2483          0    0.00118     0.00\n",
      "NOTE:     15   400 0.000512             0.85     0.8763  1.355e+04       1912          0    0.00118     0.00\n",
      "NOTE:     16   400 0.000512           0.8494     0.8466  1.219e+04       2208          0    0.00118     0.00\n",
      "NOTE:     17   400 0.000512           0.8371      0.903  1.445e+04       1552          0    0.00118     0.00\n",
      "NOTE:     18   400 0.000512           0.9298     0.8002  1.229e+04       3067          0   0.001181     0.00\n",
      "NOTE:     19   400 0.000512           0.8818     0.8904   1.49e+04       1835          0   0.001181     0.00\n",
      "NOTE:     20   400 0.000512           0.9094      0.882   1.43e+04       1914          0   0.001181     0.00\n",
      "NOTE:     21   400 0.000512           0.7471     0.8431  1.128e+04       2100          0   0.001181     0.00\n",
      "NOTE:     22   400 0.000512            0.909     0.8681  1.442e+04       2191          0   0.001181     0.00\n",
      "NOTE:     23   400 0.000512           0.8212     0.8682  1.314e+04       1994          0   0.001182     0.00\n",
      "NOTE:     24   400 0.000512           0.8894     0.8572  1.362e+04       2270          0   0.001182     0.00\n",
      "NOTE:     25   400 0.000512           0.8652     0.8925  1.411e+04       1699          0   0.001182     0.00\n",
      "NOTE:     26   400 0.000512           0.7078     0.8123       9895       2287          0   0.001182     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  41       0.0005          0.8604     0.8616  3.613e+05  5.805e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.9391     0.8756  1.563e+04       2222          0   0.001182     0.00\n",
      "NOTE:      1   400 0.000512           0.9326     0.8495  1.368e+04       2424          0   0.001182     0.00\n",
      "NOTE:      2   400 0.000512           0.8194     0.8703   1.28e+04       1907          0   0.001183     0.00\n",
      "NOTE:      3   400 0.000512            0.821     0.8419  1.219e+04       2289          0   0.001183     0.00\n",
      "NOTE:      4   400 0.000512           0.8997     0.8621  1.363e+04       2180          0   0.001183     0.00\n",
      "NOTE:      5   400 0.000512           0.8161     0.8883  1.327e+04       1669          0   0.001183     0.00\n",
      "NOTE:      6   400 0.000512           0.8047        0.9  1.385e+04       1538          0   0.001183     0.00\n",
      "NOTE:      7   400 0.000512            0.932     0.8513  1.425e+04       2488          0   0.001183     0.00\n",
      "NOTE:      8   400 0.000512           0.8726     0.8492  1.339e+04       2377          0   0.001183     0.00\n",
      "NOTE:      9   400 0.000512           0.8449     0.8271  1.099e+04       2296          0   0.001184     0.00\n",
      "NOTE:     10   400 0.000512           0.9975     0.8831  1.727e+04       2287          0   0.001184     0.00\n",
      "NOTE:     11   400 0.000512           0.9583     0.8797  1.533e+04       2097          0   0.001184     0.00\n",
      "NOTE:     12   400 0.000512           0.8757     0.8406  1.252e+04       2375          0   0.001184     0.00\n",
      "NOTE:     13   400 0.000512           0.6722     0.8618  1.076e+04       1724          0   0.001184     0.00\n",
      "NOTE:     14   400 0.000512           0.8521     0.8651  1.413e+04       2204          0   0.001184     0.00\n",
      "NOTE:     15   400 0.000512           0.8819     0.8904  1.421e+04       1748          0   0.001184     0.00\n",
      "NOTE:     16   400 0.000512           0.7426     0.8749  1.106e+04       1582          0   0.001185     0.00\n",
      "NOTE:     17   400 0.000512            1.045     0.8359  1.525e+04       2993          0   0.001185     0.00\n",
      "NOTE:     18   400 0.000512           0.8613     0.8443  1.345e+04       2479          0   0.001185     0.00\n",
      "NOTE:     19   400 0.000512           0.7906     0.9145  1.481e+04       1384          0   0.001185     0.00\n",
      "NOTE:     20   400 0.000512           0.9168     0.8337  1.349e+04       2690          0   0.001185     0.00\n",
      "NOTE:     21   400 0.000512           0.8453     0.8464  1.313e+04       2382          0   0.001185     0.00\n",
      "NOTE:     22   400 0.000512           0.9087     0.8562   1.36e+04       2284          0   0.001185     0.00\n",
      "NOTE:     23   400 0.000512           0.7847     0.8632  1.342e+04       2128          0   0.001185     0.00\n",
      "NOTE:     24   400 0.000512           0.7806     0.8862  1.274e+04       1636          0   0.001186     0.00\n",
      "NOTE:     25   400 0.000512           0.9046     0.8565  1.279e+04       2142          0   0.001186     0.00\n",
      "NOTE:     26   400 0.000512           0.8582     0.8721  1.325e+04       1944          0   0.001186     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  42       0.0005          0.8651     0.8639  3.649e+05  5.747e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.9134     0.8953  1.465e+04       1714          0   0.001186     0.00\n",
      "NOTE:      1   400 0.000512            0.793     0.8666  1.296e+04       1996          0   0.001186     0.00\n",
      "NOTE:      2   400 0.000512           0.8809     0.8096   1.22e+04       2870          0   0.001186     0.00\n",
      "NOTE:      3   400 0.000512           0.7855     0.8567  1.224e+04       2046          0   0.001186     0.00\n",
      "NOTE:      4   400 0.000512           0.9631      0.872  1.469e+04       2156          0   0.001187     0.00\n",
      "NOTE:      5   400 0.000512           0.9055     0.8783   1.44e+04       1994          0   0.001187     0.00\n",
      "NOTE:      6   400 0.000512            0.874     0.8956  1.498e+04       1746          0   0.001187     0.00\n",
      "NOTE:      7   400 0.000512           0.8883     0.8669  1.425e+04       2187          0   0.001187     0.00\n",
      "NOTE:      8   400 0.000512            1.085     0.8591  1.645e+04       2698          0   0.001187     0.00\n",
      "NOTE:      9   400 0.000512           0.9527     0.8751  1.498e+04       2137          0   0.001187     0.00\n",
      "NOTE:     10   400 0.000512           0.7035     0.8652  1.197e+04       1865          0   0.001187     0.00\n",
      "NOTE:     11   400 0.000512           0.8131     0.7921  1.096e+04       2875          0   0.001188     0.00\n",
      "NOTE:     12   400 0.000512           0.8932     0.8594  1.409e+04       2305          0   0.001188     0.00\n",
      "NOTE:     13   400 0.000512           0.9281     0.8775  1.469e+04       2050          0   0.001188     0.00\n",
      "NOTE:     14   400 0.000512           0.8277     0.8946  1.294e+04       1525          0   0.001188     0.00\n",
      "NOTE:     15   400 0.000512           0.9762     0.8888  1.577e+04       1974          0   0.001188     0.00\n",
      "NOTE:     16   400 0.000512             0.77     0.8796  1.234e+04       1689          0   0.001188     0.00\n",
      "NOTE:     17   400 0.000512           0.8461     0.8589  1.309e+04       2151          0   0.001189     0.00\n",
      "NOTE:     18   400 0.000512           0.8892     0.7853  1.206e+04       3297          0   0.001189     0.00\n",
      "NOTE:     19   400 0.000512           0.7823     0.8902  1.375e+04       1696          0   0.001189     0.00\n",
      "NOTE:     20   400 0.000512           0.9548     0.8635  1.476e+04       2332          0   0.001189     0.00\n",
      "NOTE:     21   400 0.000512           0.9979     0.8663  1.505e+04       2322          0   0.001189     0.00\n",
      "NOTE:     22   400 0.000512           0.9597     0.8841  1.525e+04       1999          0   0.001189     0.00\n",
      "NOTE:     23   400 0.000512           0.9871     0.8173  1.395e+04       3119          0    0.00119     0.00\n",
      "NOTE:     24   400 0.000512           0.9317     0.8455  1.399e+04       2556          0    0.00119     0.00\n",
      "NOTE:     25   400 0.000512           0.8852     0.8707   1.31e+04       1945          0    0.00119     0.00\n",
      "NOTE:     26   400 0.000512           0.8756     0.8498  1.374e+04       2428          0    0.00119     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  43       0.0005          0.8912     0.8622  3.733e+05  5.967e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.8729     0.8407  1.262e+04       2391          0    0.00119     0.00\n",
      "NOTE:      1   400 0.000512            1.037     0.8341  1.476e+04       2935          0    0.00119     0.00\n",
      "NOTE:      2   400 0.000512           0.8835     0.8459  1.197e+04       2180          0    0.00119     0.00\n",
      "NOTE:      3   400 0.000512           0.8033     0.8692  1.339e+04       2014          0    0.00119     0.00\n",
      "NOTE:      4   400 0.000512           0.8601      0.775  1.116e+04       3240          0   0.001191     0.00\n",
      "NOTE:      5   400 0.000512            1.013     0.8744  1.595e+04       2291          0   0.001191     0.00\n",
      "NOTE:      6   400 0.000512           0.9381     0.8646  1.463e+04       2292          0   0.001191     0.00\n",
      "NOTE:      7   400 0.000512           0.9138     0.8637  1.372e+04       2166          0   0.001191     0.00\n",
      "NOTE:      8   400 0.000512             1.02     0.8379  1.457e+04       2819          0   0.001191     0.00\n",
      "NOTE:      9   400 0.000512           0.8175     0.8663  1.294e+04       1996          0   0.001191     0.00\n",
      "NOTE:     10   400 0.000512           0.8763     0.8647  1.404e+04       2196          0   0.001191     0.00\n",
      "NOTE:     11   400 0.000512           0.9492     0.8646  1.462e+04       2289          0   0.001191     0.00\n",
      "NOTE:     12   400 0.000512           0.8056     0.8854  1.409e+04       1824          0   0.001191     0.00\n",
      "NOTE:     13   400 0.000512           0.9644     0.8368   1.35e+04       2633          0   0.001191     0.00\n",
      "NOTE:     14   400 0.000512            0.798     0.8993  1.354e+04       1515          0   0.001191     0.00\n",
      "NOTE:     15   400 0.000512             0.86     0.8662  1.457e+04       2251          0   0.001191     0.00\n",
      "NOTE:     16   400 0.000512           0.8579     0.8263  1.215e+04       2554          0   0.001191     0.00\n",
      "NOTE:     17   400 0.000512           0.8053     0.8326  1.117e+04       2246          0   0.001191     0.00\n",
      "NOTE:     18   400 0.000512           0.9627     0.8587  1.483e+04       2440          0   0.001191     0.00\n",
      "NOTE:     19   400 0.000512           0.8299     0.8628  1.271e+04       2022          0   0.001191     0.00\n",
      "NOTE:     20   400 0.000512           0.9478     0.8426   1.37e+04       2559          0   0.001191     0.00\n",
      "NOTE:     21   400 0.000512           0.8344     0.8444  1.317e+04       2428          0   0.001191     0.00\n",
      "NOTE:     22   400 0.000512           0.9805     0.8678  1.468e+04       2237          0   0.001191     0.00\n",
      "NOTE:     23   400 0.000512           0.8334     0.8815  1.366e+04       1836          0   0.001191     0.00\n",
      "NOTE:     24   400 0.000512           0.9285     0.8382  1.349e+04       2605          0   0.001191     0.00\n",
      "NOTE:     25   400 0.000512           0.8013     0.8455  1.196e+04       2185          0   0.001191     0.00\n",
      "NOTE:     26   400 0.000512           0.8284     0.8532  1.227e+04       2111          0   0.001191     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  44       0.0005          0.8897     0.8539  3.638e+05  6.226e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.8272     0.8475  1.206e+04       2169          0   0.001191     0.00\n",
      "NOTE:      1   400 0.000512           0.8464     0.7987  1.289e+04       3249          0   0.001191     0.00\n",
      "NOTE:      2   400 0.000512           0.9473     0.8737  1.578e+04       2281          0   0.001192     0.00\n",
      "NOTE:      3   400 0.000512            1.059      0.833   1.62e+04       3249          0   0.001192     0.00\n",
      "NOTE:      4   400 0.000512            0.873     0.8194  1.241e+04       2736          0   0.001192     0.00\n",
      "NOTE:      5   400 0.000512           0.7689     0.8906  1.283e+04       1576          0   0.001192     0.00\n",
      "NOTE:      6   400 0.000512           0.7468     0.8634   1.14e+04       1804          0   0.001192     0.00\n",
      "NOTE:      7   400 0.000512            0.898     0.8745  1.376e+04       1975          0   0.001192     0.00\n",
      "NOTE:      8   400 0.000512           0.9191     0.8647  1.429e+04       2237          0   0.001192     0.00\n",
      "NOTE:      9   400 0.000512           0.8122     0.8758  1.402e+04       1988          0   0.001192     0.00\n",
      "NOTE:     10   400 0.000512           0.9233     0.8684  1.508e+04       2285          0   0.001192     0.00\n",
      "NOTE:     11   400 0.000512           0.9611     0.8584  1.376e+04       2270          0   0.001192     0.00\n",
      "NOTE:     12   400 0.000512           0.8947     0.8555  1.298e+04       2192          0   0.001192     0.00\n",
      "NOTE:     13   400 0.000512           0.9947     0.8377   1.44e+04       2791          0   0.001192     0.00\n",
      "NOTE:     14   400 0.000512             0.91      0.881  1.601e+04       2162          0   0.001192     0.00\n",
      "NOTE:     15   400 0.000512           0.8267     0.8755  1.364e+04       1940          0   0.001192     0.00\n",
      "NOTE:     16   400 0.000512            1.062     0.8739  1.654e+04       2387          0   0.001192     0.00\n",
      "NOTE:     17   400 0.000512           0.7568     0.9015  1.364e+04       1490          0   0.001192     0.00\n",
      "NOTE:     18   400 0.000512           0.8548     0.8552  1.247e+04       2112          0   0.001192     0.00\n",
      "NOTE:     19   400 0.000512            0.641     0.9001  1.055e+04       1171          0   0.001192     0.00\n",
      "NOTE:     20   400 0.000512           0.9894     0.8218  1.376e+04       2985          0   0.001193     0.00\n",
      "NOTE:     21   400 0.000512             1.05     0.8643  1.728e+04       2712          0   0.001193     0.00\n",
      "NOTE:     22   400 0.000512           0.7833     0.8526  1.197e+04       2069          0   0.001193     0.00\n",
      "NOTE:     23   400 0.000512             1.03     0.8523  1.526e+04       2643          0   0.001193     0.00\n",
      "NOTE:     24   400 0.000512           0.9429     0.8553  1.365e+04       2309          0   0.001193     0.00\n",
      "NOTE:     25   400 0.000512            0.807     0.8346  1.134e+04       2247          0   0.001193     0.00\n",
      "NOTE:     26   400 0.000512           0.7443     0.8674  1.195e+04       1827          0   0.001193     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  45       0.0005          0.8841     0.8587  3.699e+05  6.086e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.8778     0.8869   1.52e+04       1938          0   0.001193     0.00\n",
      "NOTE:      1   400 0.000512           0.8393      0.887  1.385e+04       1764          0   0.001193     0.00\n",
      "NOTE:      2   400 0.000512           0.8613     0.8031  1.277e+04       3132          0   0.001193     0.00\n",
      "NOTE:      3   400 0.000512           0.7976     0.8813  1.385e+04       1864          0   0.001193     0.00\n",
      "NOTE:      4   400 0.000512           0.7718     0.8859  1.221e+04       1572          0   0.001193     0.00\n",
      "NOTE:      5   400 0.000512            0.843     0.8872  1.482e+04       1885          0   0.001193     0.00\n",
      "NOTE:      6   400 0.000512           0.7729     0.8913  1.476e+04       1800          0   0.001193     0.00\n",
      "NOTE:      7   400 0.000512           0.8516      0.813   1.29e+04       2967          0   0.001194     0.00\n",
      "NOTE:      8   400 0.000512            1.025      0.825   1.48e+04       3139          0   0.001194     0.00\n",
      "NOTE:      9   400 0.000512           0.7518     0.8674    1.2e+04       1835          0   0.001194     0.00\n",
      "NOTE:     10   400 0.000512           0.9027     0.8719  1.342e+04       1972          0   0.001194     0.00\n",
      "NOTE:     11   400 0.000512            0.762     0.8435  1.093e+04       2028          0   0.001194     0.00\n",
      "NOTE:     12   400 0.000512           0.8792     0.8337   1.23e+04       2454          0   0.001194     0.00\n",
      "NOTE:     13   400 0.000512           0.7991     0.8415  1.201e+04       2261          0   0.001194     0.00\n",
      "NOTE:     14   400 0.000512           0.8664     0.8652  1.331e+04       2075          0   0.001194     0.00\n",
      "NOTE:     15   400 0.000512           0.8874     0.8986  1.541e+04       1739          0   0.001194     0.00\n",
      "NOTE:     16   400 0.000512           0.6201     0.8872       9846       1252          0   0.001194     0.00\n",
      "NOTE:     17   400 0.000512           0.8142     0.8836  1.287e+04       1695          0   0.001194     0.00\n",
      "NOTE:     18   400 0.000512           0.8327     0.8733  1.308e+04       1898          0   0.001194     0.00\n",
      "NOTE:     19   400 0.000512           0.9613     0.8691  1.413e+04       2127          0   0.001194     0.00\n",
      "NOTE:     20   400 0.000512           0.8404     0.8266  1.145e+04       2402          0   0.001194     0.00\n",
      "NOTE:     21   400 0.000512           0.9808     0.8542  1.469e+04       2508          0   0.001195     0.00\n",
      "NOTE:     22   400 0.000512           0.9076     0.8739  1.454e+04       2098          0   0.001195     0.00\n",
      "NOTE:     23   400 0.000512           0.9193     0.8967  1.503e+04       1731          0   0.001195     0.00\n",
      "NOTE:     24   400 0.000512           0.8073     0.8671  1.246e+04       1910          0   0.001195     0.00\n",
      "NOTE:     25   400 0.000512            1.018     0.8607  1.534e+04       2483          0   0.001195     0.00\n",
      "NOTE:     26   400 0.000512           0.9518     0.8176  1.266e+04       2824          0   0.001195     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  46       0.0005          0.8571     0.8628  3.606e+05  5.735e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.9194     0.8845  1.461e+04       1909          0   0.001196     0.00\n",
      "NOTE:      1   400 0.000512            1.112     0.8442  1.702e+04       3140          0   0.001196     0.00\n",
      "NOTE:      2   400 0.000512           0.9947      0.869   1.48e+04       2232          0   0.001196     0.00\n",
      "NOTE:      3   400 0.000512           0.8032     0.8214  1.204e+04       2618          0   0.001196     0.00\n",
      "NOTE:      4   400 0.000512           0.9506     0.8663  1.468e+04       2264          0   0.001196     0.00\n",
      "NOTE:      5   400 0.000512            1.041      0.859  1.633e+04       2680          0   0.001196     0.00\n",
      "NOTE:      6   400 0.000512             0.89     0.8377    1.4e+04       2712          0   0.001196     0.00\n",
      "NOTE:      7   400 0.000512           0.7691     0.8897  1.257e+04       1559          0   0.001197     0.00\n",
      "NOTE:      8   400 0.000512            0.902     0.8645  1.376e+04       2157          0   0.001197     0.00\n",
      "NOTE:      9   400 0.000512           0.8121     0.8709  1.345e+04       1993          0   0.001197     0.00\n",
      "NOTE:     10   400 0.000512           0.8215     0.8633  1.257e+04       1991          0   0.001197     0.00\n",
      "NOTE:     11   400 0.000512           0.8665      0.847  1.243e+04       2244          0   0.001197     0.00\n",
      "NOTE:     12   400 0.000512           0.8567     0.8862  1.367e+04       1756          0   0.001197     0.00\n",
      "NOTE:     13   400 0.000512           0.9738     0.8289  1.403e+04       2896          0   0.001197     0.00\n",
      "NOTE:     14   400 0.000512           0.7923     0.8514  1.217e+04       2124          0   0.001197     0.00\n",
      "NOTE:     15   400 0.000512           0.8945     0.8831  1.518e+04       2010          0   0.001198     0.00\n",
      "NOTE:     16   400 0.000512           0.8394     0.8741  1.319e+04       1901          0   0.001198     0.00\n",
      "NOTE:     17   400 0.000512           0.9689     0.8542  1.511e+04       2578          0   0.001198     0.00\n",
      "NOTE:     18   400 0.000512           0.8055      0.867  1.211e+04       1857          0   0.001198     0.00\n",
      "NOTE:     19   400 0.000512           0.7912     0.8725  1.309e+04       1913          0   0.001198     0.00\n",
      "NOTE:     20   400 0.000512           0.9041     0.8681  1.491e+04       2264          0   0.001198     0.00\n",
      "NOTE:     21   400 0.000512            1.002     0.8753  1.674e+04       2385          0   0.001198     0.00\n",
      "NOTE:     22   400 0.000512           0.8367     0.8367  1.193e+04       2328          0   0.001198     0.00\n",
      "NOTE:     23   400 0.000512           0.7918     0.8492  1.257e+04       2232          0   0.001199     0.00\n",
      "NOTE:     24   400 0.000512           0.9011      0.893  1.516e+04       1817          0   0.001199     0.00\n",
      "NOTE:     25   400 0.000512           0.8142     0.8778  1.331e+04       1852          0   0.001199     0.00\n",
      "NOTE:     26   400 0.000512           0.9833      0.859  1.519e+04       2495          0   0.001199     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  47       0.0005          0.8903     0.8628  3.766e+05  5.991e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.8422     0.8693  1.298e+04       1952          0   0.001199     0.00\n",
      "NOTE:      1   400 0.000512           0.7792     0.8844  1.302e+04       1702          0   0.001199     0.00\n",
      "NOTE:      2   400 0.000512           0.9532     0.8308  1.421e+04       2895          0   0.001199     0.00\n",
      "NOTE:      3   400 0.000512            1.024     0.8371  1.428e+04       2779          0     0.0012     0.00\n",
      "NOTE:      4   400 0.000512           0.9375     0.8634  1.529e+04       2420          0     0.0012     0.00\n",
      "NOTE:      5   400 0.000512           0.9266     0.8044  1.359e+04       3305          0     0.0012     0.00\n",
      "NOTE:      6   400 0.000512           0.8834       0.88  1.414e+04       1928          0     0.0012     0.00\n",
      "NOTE:      7   400 0.000512           0.8745     0.8728  1.394e+04       2031          0     0.0012     0.00\n",
      "NOTE:      8   400 0.000512           0.8536     0.8883  1.488e+04       1872          0     0.0012     0.00\n",
      "NOTE:      9   400 0.000512            0.884     0.9022  1.487e+04       1611          0     0.0012     0.00\n",
      "NOTE:     10   400 0.000512            1.039     0.8461  1.587e+04       2887          0     0.0012     0.00\n",
      "NOTE:     11   400 0.000512           0.8761     0.8317   1.22e+04       2469          0     0.0012     0.00\n",
      "NOTE:     12   400 0.000512           0.7364     0.8549  1.116e+04       1894          0     0.0012     0.00\n",
      "NOTE:     13   400 0.000512           0.8669     0.8602  1.338e+04       2174          0   0.001201     0.00\n",
      "NOTE:     14   400 0.000512           0.8035     0.8434  1.253e+04       2327          0   0.001201     0.00\n",
      "NOTE:     15   400 0.000512           0.8827     0.8457  1.449e+04       2643          0   0.001201     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     16   400 0.000512            0.827     0.8762  1.241e+04       1754          0   0.001201     0.00\n",
      "NOTE:     17   400 0.000512           0.9356     0.8863  1.462e+04       1876          0   0.001201     0.00\n",
      "NOTE:     18   400 0.000512            0.832     0.8163  1.186e+04       2669          0   0.001201     0.00\n",
      "NOTE:     19   400 0.000512            0.891     0.8732  1.611e+04       2340          0   0.001201     0.00\n",
      "NOTE:     20   400 0.000512            0.896     0.8304  1.165e+04       2380          0   0.001201     0.00\n",
      "NOTE:     21   400 0.000512           0.7572     0.8541  1.195e+04       2040          0   0.001201     0.00\n",
      "NOTE:     22   400 0.000512           0.8209     0.8799  1.313e+04       1793          0   0.001201     0.00\n",
      "NOTE:     23   400 0.000512           0.7831      0.856  1.237e+04       2082          0   0.001201     0.00\n",
      "NOTE:     24   400 0.000512           0.8988     0.8429   1.48e+04       2759          0   0.001201     0.00\n",
      "NOTE:     25   400 0.000512           0.7813     0.9016  1.384e+04       1511          0   0.001201     0.00\n",
      "NOTE:     26   400 0.000512           0.8387     0.8621  1.453e+04       2324          0   0.001201     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  48       0.0005          0.8676      0.859  3.681e+05  6.042e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.7523     0.8718  1.152e+04       1694          0   0.001202     0.00\n",
      "NOTE:      1   400 0.000512           0.7845      0.839  1.206e+04       2314          0   0.001202     0.00\n",
      "NOTE:      2   400 0.000512           0.8292       0.86  1.451e+04       2362          0   0.001202     0.00\n",
      "NOTE:      3   400 0.000512           0.8345      0.806  1.065e+04       2562          0   0.001202     0.00\n",
      "NOTE:      4   400 0.000512            0.846     0.8768  1.352e+04       1901          0   0.001202     0.00\n",
      "NOTE:      5   400 0.000512           0.9918     0.8516  1.515e+04       2639          0   0.001202     0.00\n",
      "NOTE:      6   400 0.000512           0.8002     0.8534  1.254e+04       2153          0   0.001202     0.00\n",
      "NOTE:      7   400 0.000512           0.7874     0.8861  1.416e+04       1820          0   0.001202     0.00\n",
      "NOTE:      8   400 0.000512           0.8467     0.8781  1.276e+04       1772          0   0.001202     0.00\n",
      "NOTE:      9   400 0.000512           0.7933     0.8752  1.363e+04       1944          0   0.001203     0.00\n",
      "NOTE:     10   400 0.000512           0.9147     0.8516  1.443e+04       2515          0   0.001203     0.00\n",
      "NOTE:     11   400 0.000512           0.9546     0.8495  1.383e+04       2451          0   0.001203     0.00\n",
      "NOTE:     12   400 0.000512           0.9813      0.872  1.647e+04       2417          0   0.001203     0.00\n",
      "NOTE:     13   400 0.000512            1.091     0.8431  1.649e+04       3069          0   0.001203     0.00\n",
      "NOTE:     14   400 0.000512           0.9396     0.8423  1.429e+04       2675          0   0.001203     0.00\n",
      "NOTE:     15   400 0.000512           0.8038     0.8492  1.251e+04       2223          0   0.001203     0.00\n",
      "NOTE:     16   400 0.000512            0.894     0.8527   1.36e+04       2349          0   0.001203     0.00\n",
      "NOTE:     17   400 0.000512           0.8544     0.8689  1.385e+04       2090          0   0.001204     0.00\n",
      "NOTE:     18   400 0.000512           0.9401     0.8444   1.44e+04       2653          0   0.001204     0.00\n",
      "NOTE:     19   400 0.000512           0.9587     0.8555  1.504e+04       2539          0   0.001204     0.00\n",
      "NOTE:     20   400 0.000512           0.9914     0.8398  1.522e+04       2903          0   0.001204     0.00\n",
      "NOTE:     21   400 0.000512           0.8627     0.8737  1.352e+04       1955          0   0.001204     0.00\n",
      "NOTE:     22   400 0.000512            0.852      0.853  1.304e+04       2247          0   0.001204     0.00\n",
      "NOTE:     23   400 0.000512           0.8541     0.8741  1.503e+04       2166          0   0.001204     0.00\n",
      "NOTE:     24   400 0.000512           0.7943     0.8538  1.128e+04       1931          0   0.001204     0.00\n",
      "NOTE:     25   400 0.000512           0.8207     0.8662  1.268e+04       1959          0   0.001204     0.00\n",
      "NOTE:     26   400 0.000512           0.7121     0.8541  1.045e+04       1785          0   0.001205     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  49       0.0005          0.8699     0.8572  3.666e+05  6.109e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.7521     0.8398   1.09e+04       2079          0   0.001205     0.00\n",
      "NOTE:      1   400 0.000512            1.099     0.8828  1.753e+04       2328          0   0.001205     0.00\n",
      "NOTE:      2   400 0.000512           0.8524     0.8532  1.261e+04       2170          0   0.001205     0.00\n",
      "NOTE:      3   400 0.000512            0.927     0.8135  1.384e+04       3172          0   0.001205     0.00\n",
      "NOTE:      4   400 0.000512           0.8427     0.8689  1.367e+04       2063          0   0.001205     0.00\n",
      "NOTE:      5   400 0.000512           0.8538     0.8426  1.282e+04       2394          0   0.001205     0.00\n",
      "NOTE:      6   400 0.000512           0.8563     0.8662  1.323e+04       2043          0   0.001205     0.00\n",
      "NOTE:      7   400 0.000512           0.8791     0.8885  1.358e+04       1704          0   0.001206     0.00\n",
      "NOTE:      8   400 0.000512           0.7959      0.873  1.284e+04       1869          0   0.001206     0.00\n",
      "NOTE:      9   400 0.000512           0.9128     0.8343   1.31e+04       2602          0   0.001206     0.00\n",
      "NOTE:     10   400 0.000512           0.7659     0.8931  1.273e+04       1523          0   0.001206     0.00\n",
      "NOTE:     11   400 0.000512           0.8539     0.8313  1.322e+04       2684          0   0.001206     0.00\n",
      "NOTE:     12   400 0.000512           0.8134       0.83    1.2e+04       2459          0   0.001206     0.00\n",
      "NOTE:     13   400 0.000512           0.8815     0.8634  1.396e+04       2207          0   0.001206     0.00\n",
      "NOTE:     14   400 0.000512           0.8201     0.8392  1.224e+04       2345          0   0.001206     0.00\n",
      "NOTE:     15   400 0.000512           0.9936     0.8336  1.337e+04       2669          0   0.001206     0.00\n",
      "NOTE:     16   400 0.000512           0.6562     0.9089  1.158e+04       1161          0   0.001206     0.00\n",
      "NOTE:     17   400 0.000512           0.8062     0.8242  1.186e+04       2529          0   0.001206     0.00\n",
      "NOTE:     18   400 0.000512             0.91      0.843  1.393e+04       2593          0   0.001206     0.00\n",
      "NOTE:     19   400 0.000512           0.8466     0.8776  1.362e+04       1899          0   0.001206     0.00\n",
      "NOTE:     20   400 0.000512            0.774     0.8659  1.266e+04       1961          0   0.001206     0.00\n",
      "NOTE:     21   400 0.000512           0.9427     0.8531  1.467e+04       2526          0   0.001207     0.00\n",
      "NOTE:     22   400 0.000512           0.8651      0.858  1.328e+04       2198          0   0.001207     0.00\n",
      "NOTE:     23   400 0.000512           0.9369     0.8825  1.608e+04       2142          0   0.001207     0.00\n",
      "NOTE:     24   400 0.000512           0.8392     0.8697  1.304e+04       1954          0   0.001207     0.00\n",
      "NOTE:     25   400 0.000512           0.9033     0.9063  1.464e+04       1514          0   0.001207     0.00\n",
      "NOTE:     26   400 0.000512           0.7539     0.8502  1.146e+04       2018          0   0.001207     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  50       0.0005          0.8568     0.8591  3.584e+05  5.881e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.8705     0.8668    1.3e+04       1998          0   0.001207     0.00\n",
      "NOTE:      1   400 0.000512           0.8665     0.8276  1.253e+04       2609          0   0.001207     0.00\n",
      "NOTE:      2   400 0.000512           0.9543     0.8399  1.302e+04       2483          0   0.001207     0.00\n",
      "NOTE:      3   400 0.000512            1.049     0.8984  1.734e+04       1960          0   0.001207     0.00\n",
      "NOTE:      4   400 0.000512           0.9544     0.8991  1.571e+04       1762          0   0.001208     0.00\n",
      "NOTE:      5   400 0.000512           0.8779      0.849  1.419e+04       2523          0   0.001208     0.00\n",
      "NOTE:      6   400 0.000512           0.7425     0.8695  1.145e+04       1718          0   0.001208     0.00\n",
      "NOTE:      7   400 0.000512             0.82     0.8595  1.409e+04       2303          0   0.001208     0.00\n",
      "NOTE:      8   400 0.000512           0.8314     0.8186   1.31e+04       2904          0   0.001208     0.00\n",
      "NOTE:      9   400 0.000512           0.9015     0.8719  1.322e+04       1943          0   0.001208     0.00\n",
      "NOTE:     10   400 0.000512           0.8103     0.8526   1.21e+04       2093          0   0.001209     0.00\n",
      "NOTE:     11   400 0.000512           0.6424     0.8984  1.171e+04       1324          0   0.001209     0.00\n",
      "NOTE:     12   400 0.000512           0.9337     0.8885  1.612e+04       2023          0   0.001209     0.00\n",
      "NOTE:     13   400 0.000512           0.9927     0.8454  1.383e+04       2529          0   0.001209     0.00\n",
      "NOTE:     14   400 0.000512           0.8576     0.8572  1.357e+04       2260          0   0.001209     0.00\n",
      "NOTE:     15   400 0.000512           0.8757     0.8922  1.451e+04       1753          0   0.001209     0.00\n",
      "NOTE:     16   400 0.000512           0.7682     0.8344  1.143e+04       2269          0    0.00121     0.00\n",
      "NOTE:     17   400 0.000512           0.7676     0.8432   1.19e+04       2212          0    0.00121     0.00\n",
      "NOTE:     18   400 0.000512           0.9141     0.8809   1.51e+04       2041          0    0.00121     0.00\n",
      "NOTE:     19   400 0.000512            1.047      0.861  1.557e+04       2514          0    0.00121     0.00\n",
      "NOTE:     20   400 0.000512           0.6882     0.8052       9403       2275          0    0.00121     0.00\n",
      "NOTE:     21   400 0.000512            1.011      0.828  1.482e+04       3079          0    0.00121     0.00\n",
      "NOTE:     22   400 0.000512           0.8859     0.8199  1.331e+04       2923          0   0.001211     0.00\n",
      "NOTE:     23   400 0.000512            1.024      0.874  1.581e+04       2280          0   0.001211     0.00\n",
      "NOTE:     24   400 0.000512           0.9532     0.8684  1.518e+04       2301          0   0.001211     0.00\n",
      "NOTE:     25   400 0.000512            0.821     0.8969  1.488e+04       1711          0   0.001211     0.00\n",
      "NOTE:     26   400 0.000512           0.8164     0.8417  1.217e+04       2288          0   0.001211     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  51       0.0005          0.8769       0.86   3.69e+05  6.008e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.9407     0.8415  1.325e+04       2497          0   0.001211     0.00\n",
      "NOTE:      1   400 0.000512           0.9466     0.8382  1.376e+04       2657          0   0.001211     0.00\n",
      "NOTE:      2   400 0.000512           0.9972     0.8756  1.626e+04       2311          0   0.001212     0.00\n",
      "NOTE:      3   400 0.000512           0.7416     0.8846  1.211e+04       1579          0   0.001212     0.00\n",
      "NOTE:      4   400 0.000512           0.8481     0.8921   1.45e+04       1753          0   0.001212     0.00\n",
      "NOTE:      5   400 0.000512            1.058     0.8759  1.598e+04       2264          0   0.001212     0.00\n",
      "NOTE:      6   400 0.000512           0.8826     0.8393  1.294e+04       2477          0   0.001212     0.00\n",
      "NOTE:      7   400 0.000512           0.7874     0.8801  1.343e+04       1830          0   0.001212     0.00\n",
      "NOTE:      8   400 0.000512           0.9109     0.8484   1.36e+04       2430          0   0.001213     0.00\n",
      "NOTE:      9   400 0.000512           0.8028     0.8738  1.421e+04       2053          0   0.001213     0.00\n",
      "NOTE:     10   400 0.000512           0.8614     0.8847  1.352e+04       1762          0   0.001213     0.00\n",
      "NOTE:     11   400 0.000512           0.8556     0.8421  1.161e+04       2177          0   0.001213     0.00\n",
      "NOTE:     12   400 0.000512           0.9097     0.8623  1.356e+04       2164          0   0.001213     0.00\n",
      "NOTE:     13   400 0.000512           0.8813     0.8595  1.262e+04       2064          0   0.001214     0.00\n",
      "NOTE:     14   400 0.000512            0.955     0.8799  1.521e+04       2076          0   0.001214     0.00\n",
      "NOTE:     15   400 0.000512            1.063     0.8521  1.627e+04       2824          0   0.001214     0.00\n",
      "NOTE:     16   400 0.000512            1.125     0.8156  1.633e+04       3692          0   0.001214     0.00\n",
      "NOTE:     17   400 0.000512           0.8956     0.8571  1.411e+04       2353          0   0.001214     0.00\n",
      "NOTE:     18   400 0.000512           0.9412      0.867  1.455e+04       2231          0   0.001215     0.00\n",
      "NOTE:     19   400 0.000512           0.8181     0.8709  1.326e+04       1965          0   0.001215     0.00\n",
      "NOTE:     20   400 0.000512           0.9438      0.832  1.362e+04       2749          0   0.001215     0.00\n",
      "NOTE:     21   400 0.000512           0.9551     0.8123  1.261e+04       2913          0   0.001215     0.00\n",
      "NOTE:     22   400 0.000512           0.9172     0.8334  1.363e+04       2725          0   0.001215     0.00\n",
      "NOTE:     23   400 0.000512           0.8867     0.8937  1.371e+04       1631          0   0.001215     0.00\n",
      "NOTE:     24   400 0.000512            1.078     0.8425  1.632e+04       3051          0   0.001215     0.00\n",
      "NOTE:     25   400 0.000512           0.8846     0.8797  1.433e+04       1960          0   0.001216     0.00\n",
      "NOTE:     26   400 0.000512           0.8537      0.873  1.318e+04       1917          0   0.001216     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  52       0.0005          0.9163      0.859  3.785e+05  6.211e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.8291     0.8488   1.23e+04       2190          0   0.001216     0.00\n",
      "NOTE:      1   400 0.000512           0.8488     0.8526  1.373e+04       2373          0   0.001216     0.00\n",
      "NOTE:      2   400 0.000512           0.8216     0.8622  1.242e+04       1984          0   0.001216     0.00\n",
      "NOTE:      3   400 0.000512           0.8398     0.8985  1.347e+04       1522          0   0.001216     0.00\n",
      "NOTE:      4   400 0.000512             0.84     0.8511  1.308e+04       2289          0   0.001216     0.00\n",
      "NOTE:      5   400 0.000512           0.9743     0.8419  1.365e+04       2564          0   0.001216     0.00\n",
      "NOTE:      6   400 0.000512           0.8201     0.8918  1.544e+04       1872          0   0.001216     0.00\n",
      "NOTE:      7   400 0.000512           0.9039     0.8342  1.394e+04       2771          0   0.001217     0.00\n",
      "NOTE:      8   400 0.000512           0.9584     0.8952  1.558e+04       1825          0   0.001217     0.00\n",
      "NOTE:      9   400 0.000512           0.8412      0.826  1.204e+04       2536          0   0.001217     0.00\n",
      "NOTE:     10   400 0.000512           0.8022     0.8662  1.238e+04       1913          0   0.001217     0.00\n",
      "NOTE:     11   400 0.000512           0.9511     0.8448  1.404e+04       2579          0   0.001217     0.00\n",
      "NOTE:     12   400 0.000512           0.9405     0.8341  1.298e+04       2581          0   0.001217     0.00\n",
      "NOTE:     13   400 0.000512           0.8589     0.8809  1.323e+04       1789          0   0.001217     0.00\n",
      "NOTE:     14   400 0.000512           0.8962     0.8665  1.531e+04       2357          0   0.001217     0.00\n",
      "NOTE:     15   400 0.000512           0.9282     0.8748  1.562e+04       2235          0   0.001217     0.00\n",
      "NOTE:     16   400 0.000512           0.9668     0.8412  1.416e+04       2674          0   0.001218     0.00\n",
      "NOTE:     17   400 0.000512           0.8735     0.8343  1.246e+04       2474          0   0.001218     0.00\n",
      "NOTE:     18   400 0.000512           0.8266     0.8673  1.318e+04       2017          0   0.001218     0.00\n",
      "NOTE:     19   400 0.000512           0.9159     0.8396  1.334e+04       2547          0   0.001218     0.00\n",
      "NOTE:     20   400 0.000512           0.7795       0.85  1.294e+04       2284          0   0.001218     0.00\n",
      "NOTE:     21   400 0.000512           0.9957     0.8537  1.528e+04       2618          0   0.001218     0.00\n",
      "NOTE:     22   400 0.000512           0.8613     0.8724  1.396e+04       2042          0   0.001218     0.00\n",
      "NOTE:     23   400 0.000512           0.7987     0.8781  1.391e+04       1932          0   0.001218     0.00\n",
      "NOTE:     24   400 0.000512            0.972     0.8696  1.592e+04       2388          0   0.001218     0.00\n",
      "NOTE:     25   400 0.000512           0.9421     0.8272  1.358e+04       2836          0   0.001218     0.00\n",
      "NOTE:     26   400 0.000512           0.9275     0.8382  1.444e+04       2787          0   0.001218     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  53       0.0005          0.8857     0.8573  3.724e+05  6.198e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.9157     0.8592  1.353e+04       2218          0   0.001218     0.00\n",
      "NOTE:      1   400 0.000512           0.8929     0.8638  1.371e+04       2162          0   0.001218     0.00\n",
      "NOTE:      2   400 0.000512           0.8204     0.8517  1.254e+04       2182          0   0.001218     0.00\n",
      "NOTE:      3   400 0.000512           0.8063     0.8806  1.292e+04       1752          0   0.001218     0.00\n",
      "NOTE:      4   400 0.000512           0.7723     0.8827  1.246e+04       1656          0   0.001219     0.00\n",
      "NOTE:      5   400 0.000512           0.8702     0.8054  1.055e+04       2550          0   0.001219     0.00\n",
      "NOTE:      6   400 0.000512            0.819     0.8277  1.211e+04       2520          0   0.001219     0.00\n",
      "NOTE:      7   400 0.000512            1.044     0.7779  1.327e+04       3788          0   0.001219     0.00\n",
      "NOTE:      8   400 0.000512           0.8602     0.8287  1.268e+04       2620          0   0.001219     0.00\n",
      "NOTE:      9   400 0.000512           0.9305     0.7977  1.377e+04       3491          0   0.001219     0.00\n",
      "NOTE:     10   400 0.000512            0.824     0.8592  1.314e+04       2154          0   0.001219     0.00\n",
      "NOTE:     11   400 0.000512           0.9615     0.8686  1.443e+04       2182          0   0.001219     0.00\n",
      "NOTE:     12   400 0.000512            0.888     0.8602   1.47e+04       2388          0   0.001219     0.00\n",
      "NOTE:     13   400 0.000512           0.9054     0.8771  1.475e+04       2066          0   0.001219     0.00\n",
      "NOTE:     14   400 0.000512           0.9727     0.8572  1.553e+04       2587          0   0.001219     0.00\n",
      "NOTE:     15   400 0.000512           0.8402     0.8484  1.225e+04       2189          0   0.001219     0.00\n",
      "NOTE:     16   400 0.000512           0.8094     0.8716  1.398e+04       2059          0   0.001219     0.00\n",
      "NOTE:     17   400 0.000512           0.8556     0.8712  1.458e+04       2156          0   0.001219     0.00\n",
      "NOTE:     18   400 0.000512           0.7703     0.8644  1.292e+04       2027          0   0.001219     0.00\n",
      "NOTE:     19   400 0.000512           0.7769     0.8461  1.228e+04       2233          0    0.00122     0.00\n",
      "NOTE:     20   400 0.000512           0.9084     0.8577  1.361e+04       2258          0    0.00122     0.00\n",
      "NOTE:     21   400 0.000512           0.8186     0.8592  1.241e+04       2033          0    0.00122     0.00\n",
      "NOTE:     22   400 0.000512            0.912     0.8652  1.523e+04       2372          0    0.00122     0.00\n",
      "NOTE:     23   400 0.000512           0.8334      0.841   1.23e+04       2326          0    0.00122     0.00\n",
      "NOTE:     24   400 0.000512           0.8691     0.8822  1.343e+04       1793          0    0.00122     0.00\n",
      "NOTE:     25   400 0.000512           0.8829     0.8344   1.45e+04       2878          0    0.00122     0.00\n",
      "NOTE:     26   400 0.000512           0.9323     0.8281  1.255e+04       2604          0    0.00122     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  54       0.0005          0.8701     0.8506  3.601e+05  6.324e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.7582     0.8599  1.184e+04       1929          0    0.00122     0.00\n",
      "NOTE:      1   400 0.000512           0.8225     0.8308    1.2e+04       2444          0    0.00122     0.00\n",
      "NOTE:      2   400 0.000512            0.794     0.8772  1.359e+04       1902          0    0.00122     0.00\n",
      "NOTE:      3   400 0.000512           0.8367      0.852  1.327e+04       2305          0    0.00122     0.00\n",
      "NOTE:      4   400 0.000512            1.088     0.8658   1.61e+04       2497          0    0.00122     0.00\n",
      "NOTE:      5   400 0.000512           0.9263     0.8686  1.464e+04       2214          0    0.00122     0.00\n",
      "NOTE:      6   400 0.000512           0.8507     0.8087  1.121e+04       2652          0   0.001221     0.00\n",
      "NOTE:      7   400 0.000512           0.9245     0.8413  1.452e+04       2739          0   0.001221     0.00\n",
      "NOTE:      8   400 0.000512           0.8695     0.8988  1.401e+04       1578          0   0.001221     0.00\n",
      "NOTE:      9   400 0.000512           0.9911     0.8948  1.726e+04       2029          0   0.001221     0.00\n",
      "NOTE:     10   400 0.000512            0.829     0.8806  1.342e+04       1820          0   0.001221     0.00\n",
      "NOTE:     11   400 0.000512            0.904     0.8607  1.472e+04       2382          0   0.001221     0.00\n",
      "NOTE:     12   400 0.000512           0.8528     0.8473  1.309e+04       2359          0   0.001221     0.00\n",
      "NOTE:     13   400 0.000512           0.8998      0.893  1.354e+04       1622          0   0.001221     0.00\n",
      "NOTE:     14   400 0.000512           0.9944     0.8443  1.557e+04       2871          0   0.001221     0.00\n",
      "NOTE:     15   400 0.000512           0.9687     0.8793  1.574e+04       2161          0   0.001221     0.00\n",
      "NOTE:     16   400 0.000512           0.8444     0.8618  1.319e+04       2115          0   0.001221     0.00\n",
      "NOTE:     17   400 0.000512           0.6791     0.8992  1.164e+04       1306          0   0.001221     0.00\n",
      "NOTE:     18   400 0.000512            1.038     0.8235  1.418e+04       3039          0   0.001222     0.00\n",
      "NOTE:     19   400 0.000512           0.8384     0.8599  1.407e+04       2293          0   0.001222     0.00\n",
      "NOTE:     20   400 0.000512           0.9309     0.8903  1.587e+04       1955          0   0.001222     0.00\n",
      "NOTE:     21   400 0.000512            1.094     0.8857  1.877e+04       2422          0   0.001222     0.00\n",
      "NOTE:     22   400 0.000512           0.9038     0.8364  1.351e+04       2642          0   0.001222     0.00\n",
      "NOTE:     23   400 0.000512           0.7878     0.8401  1.321e+04       2514          0   0.001222     0.00\n",
      "NOTE:     24   400 0.000512           0.7556     0.8981  1.282e+04       1455          0   0.001222     0.00\n",
      "NOTE:     25   400 0.000512           0.8425     0.8345  1.219e+04       2419          0   0.001222     0.00\n",
      "NOTE:     26   400 0.000512           0.7608        0.9  1.225e+04       1361          0   0.001222     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  55       0.0005          0.8809     0.8644  3.762e+05  5.903e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000512           0.9677     0.8815  1.688e+04       2268          0   0.001222     0.00\n",
      "NOTE:      1   400 0.000512            0.874     0.8391  1.354e+04       2595          0   0.001223     0.00\n",
      "NOTE:      2   400 0.000512           0.9296     0.8489  1.241e+04       2209          0   0.001223     0.00\n",
      "NOTE:      3   400 0.000512           0.8763     0.8648  1.426e+04       2230          0   0.001223     0.00\n",
      "NOTE:      4   400 0.000512            1.015      0.882  1.698e+04       2271          0   0.001223     0.00\n",
      "NOTE:      5   400 0.000512           0.7109     0.8404  1.122e+04       2130          0   0.001223     0.00\n",
      "NOTE:      6   400 0.000512           0.9054     0.8616  1.387e+04       2229          0   0.001223     0.00\n",
      "NOTE:      7   400 0.000512           0.9359     0.8404  1.331e+04       2526          0   0.001224     0.00\n",
      "NOTE:      8   400 0.000512           0.7842     0.8604  1.272e+04       2064          0   0.001224     0.00\n",
      "NOTE:      9   400 0.000512            1.007     0.8677  1.618e+04       2467          0   0.001224     0.00\n",
      "NOTE:     10   400 0.000512            0.779     0.8342  1.095e+04       2175          0   0.001224     0.00\n",
      "NOTE:     11   400 0.000512           0.9115     0.8362  1.413e+04       2768          0   0.001224     0.00\n",
      "NOTE:     12   400 0.000512           0.9001     0.8659  1.369e+04       2120          0   0.001224     0.00\n",
      "NOTE:     13   400 0.000512           0.9052     0.8561  1.395e+04       2345          0   0.001224     0.00\n",
      "NOTE:     14   400 0.000512           0.9955     0.8669  1.498e+04       2299          0   0.001225     0.00\n",
      "NOTE:     15   400 0.000512           0.9064     0.8484  1.426e+04       2547          0   0.001225     0.00\n",
      "NOTE:     16   400 0.000512           0.9012     0.8028  1.239e+04       3044          0   0.001225     0.00\n",
      "NOTE:     17   400 0.000512           0.9359     0.8728  1.605e+04       2338          0   0.001225     0.00\n",
      "NOTE:     18   400 0.000512           0.9078     0.8664  1.518e+04       2341          0   0.001225     0.00\n",
      "NOTE:     19   400 0.000512           0.7781     0.8923  1.355e+04       1635          0   0.001225     0.00\n",
      "NOTE:     20   400 0.000512           0.8134     0.8647  1.259e+04       1969          0   0.001225     0.00\n",
      "NOTE:     21   400 0.000512           0.8411     0.8256    1.2e+04       2533          0   0.001225     0.00\n",
      "NOTE:     22   400 0.000512           0.8259     0.8484  1.352e+04       2416          0   0.001225     0.00\n",
      "NOTE:     23   400 0.000512           0.8698     0.8829  1.366e+04       1812          0   0.001225     0.00\n",
      "NOTE:     24   400 0.000512           0.8851     0.8011  1.231e+04       3055          0   0.001225     0.00\n",
      "NOTE:     25   400 0.000512            0.935     0.8584  1.539e+04       2539          0   0.001225     0.00\n",
      "NOTE:     26   400 0.000512           0.7406      0.873  1.163e+04       1691          0   0.001225     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  56       0.0005          0.8829     0.8558  3.716e+05  6.262e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00041           0.9122     0.8521  1.326e+04       2300          0   0.001225     0.00\n",
      "NOTE:      1   400  0.00041           0.7783     0.8553  1.252e+04       2118          0   0.001226     0.00\n",
      "NOTE:      2   400  0.00041           0.9366     0.8383  1.398e+04       2696          0   0.001226     0.00\n",
      "NOTE:      3   400  0.00041            0.881     0.8614  1.441e+04       2318          0   0.001226     0.00\n",
      "NOTE:      4   400  0.00041           0.8363     0.8694  1.369e+04       2057          0   0.001226     0.00\n",
      "NOTE:      5   400  0.00041           0.8645     0.8039  1.227e+04       2992          0   0.001226     0.00\n",
      "NOTE:      6   400  0.00041           0.9697     0.8441   1.55e+04       2863          0   0.001226     0.00\n",
      "NOTE:      7   400  0.00041           0.9226     0.8938  1.571e+04       1866          0   0.001226     0.00\n",
      "NOTE:      8   400  0.00041           0.8926     0.8578  1.371e+04       2272          0   0.001226     0.00\n",
      "NOTE:      9   400  0.00041           0.8586     0.8805  1.434e+04       1947          0   0.001226     0.00\n",
      "NOTE:     10   400  0.00041           0.9754     0.8626  1.522e+04       2424          0   0.001226     0.00\n",
      "NOTE:     11   400  0.00041            1.032     0.8417   1.53e+04       2877          0   0.001226     0.00\n",
      "NOTE:     12   400  0.00041             1.01     0.8548  1.557e+04       2645          0   0.001226     0.00\n",
      "NOTE:     13   400  0.00041            1.034     0.8579  1.627e+04       2694          0   0.001226     0.00\n",
      "NOTE:     14   400  0.00041           0.8235      0.877  1.265e+04       1774          0   0.001226     0.00\n",
      "NOTE:     15   400  0.00041            0.772     0.8929  1.234e+04       1481          0   0.001226     0.00\n",
      "NOTE:     16   400  0.00041           0.9447      0.881  1.429e+04       1930          0   0.001226     0.00\n",
      "NOTE:     17   400  0.00041           0.8825     0.8381  1.347e+04       2602          0   0.001227     0.00\n",
      "NOTE:     18   400  0.00041           0.9748     0.8828  1.642e+04       2180          0   0.001227     0.00\n",
      "NOTE:     19   400  0.00041           0.8074      0.857  1.257e+04       2098          0   0.001227     0.00\n",
      "NOTE:     20   400  0.00041           0.9416     0.8164   1.28e+04       2878          0   0.001227     0.00\n",
      "NOTE:     21   400  0.00041           0.7083     0.8565  1.173e+04       1966          0   0.001227     0.00\n",
      "NOTE:     22   400  0.00041           0.8734     0.8807  1.529e+04       2071          0   0.001227     0.00\n",
      "NOTE:     23   400  0.00041           0.9096     0.8381  1.409e+04       2721          0   0.001227     0.00\n",
      "NOTE:     24   400  0.00041           0.7591     0.8878  1.324e+04       1673          0   0.001227     0.00\n",
      "NOTE:     25   400  0.00041           0.8196     0.8705  1.362e+04       2025          0   0.001227     0.00\n",
      "NOTE:     26   400  0.00041           0.8606      0.887  1.395e+04       1777          0   0.001227     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  57       0.0004          0.8882     0.8606  3.782e+05  6.125e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00041           0.9071     0.8332  1.322e+04       2647          0   0.001228     0.00\n",
      "NOTE:      1   400  0.00041           0.7428     0.8606  1.133e+04       1835          0   0.001228     0.00\n",
      "NOTE:      2   400  0.00041           0.8222      0.883  1.422e+04       1883          0   0.001228     0.00\n",
      "NOTE:      3   400  0.00041           0.8393     0.8847  1.476e+04       1923          0   0.001228     0.00\n",
      "NOTE:      4   400  0.00041           0.9257     0.8352  1.336e+04       2636          0   0.001228     0.00\n",
      "NOTE:      5   400  0.00041           0.9006     0.8113  1.192e+04       2772          0   0.001228     0.00\n",
      "NOTE:      6   400  0.00041           0.6616     0.8506  1.074e+04       1887          0   0.001228     0.00\n",
      "NOTE:      7   400  0.00041           0.9365     0.8839  1.424e+04       1870          0   0.001228     0.00\n",
      "NOTE:      8   400  0.00041           0.7334     0.8198  1.048e+04       2303          0   0.001228     0.00\n",
      "NOTE:      9   400  0.00041           0.8683     0.8338  1.318e+04       2628          0   0.001229     0.00\n",
      "NOTE:     10   400  0.00041           0.9553     0.8408  1.354e+04       2564          0   0.001229     0.00\n",
      "NOTE:     11   400  0.00041           0.9328     0.8845  1.586e+04       2071          0   0.001229     0.00\n",
      "NOTE:     12   400  0.00041           0.8745     0.8522  1.327e+04       2302          0   0.001229     0.00\n",
      "NOTE:     13   400  0.00041           0.9773     0.8436  1.551e+04       2875          0   0.001229     0.00\n",
      "NOTE:     14   400  0.00041           0.8417     0.8425  1.405e+04       2627          0   0.001229     0.00\n",
      "NOTE:     15   400  0.00041           0.7721     0.8421  1.274e+04       2390          0   0.001229     0.00\n",
      "NOTE:     16   400  0.00041           0.8908      0.871  1.422e+04       2106          0   0.001229     0.00\n",
      "NOTE:     17   400  0.00041           0.8849     0.7925  1.197e+04       3134          0   0.001229     0.00\n",
      "NOTE:     18   400  0.00041            1.008     0.8296   1.32e+04       2712          0   0.001229     0.00\n",
      "NOTE:     19   400  0.00041            1.023     0.8371  1.465e+04       2851          0   0.001229     0.00\n",
      "NOTE:     20   400  0.00041            0.886     0.8622  1.383e+04       2211          0   0.001229     0.00\n",
      "NOTE:     21   400  0.00041           0.7647     0.9032  1.464e+04       1570          0   0.001229     0.00\n",
      "NOTE:     22   400  0.00041           0.9434     0.8688  1.531e+04       2311          0    0.00123     0.00\n",
      "NOTE:     23   400  0.00041           0.7998       0.88  1.354e+04       1846          0    0.00123     0.00\n",
      "NOTE:     24   400  0.00041           0.7794     0.8699   1.27e+04       1900          0    0.00123     0.00\n",
      "NOTE:     25   400  0.00041           0.7209     0.8624  1.127e+04       1799          0    0.00123     0.00\n",
      "NOTE:     26   400  0.00041           0.7386     0.8797   1.16e+04       1586          0    0.00123     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  58       0.0004          0.8567     0.8544  3.594e+05  6.124e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00041           0.8873     0.8658  1.456e+04       2256          0    0.00123     0.00\n",
      "NOTE:      1   400  0.00041            0.757     0.8237  1.014e+04       2171          0    0.00123     0.00\n",
      "NOTE:      2   400  0.00041            0.836     0.8458  1.174e+04       2140          0    0.00123     0.00\n",
      "NOTE:      3   400  0.00041           0.7953     0.8685   1.36e+04       2058          0    0.00123     0.00\n",
      "NOTE:      4   400  0.00041           0.8033     0.8537   1.21e+04       2072          0    0.00123     0.00\n",
      "NOTE:      5   400  0.00041           0.9965     0.8931  1.627e+04       1948          0    0.00123     0.00\n",
      "NOTE:      6   400  0.00041           0.9448     0.8454  1.361e+04       2488          0   0.001231     0.00\n",
      "NOTE:      7   400  0.00041           0.9463     0.8497  1.353e+04       2394          0   0.001231     0.00\n",
      "NOTE:      8   400  0.00041           0.8789     0.8654   1.36e+04       2116          0   0.001231     0.00\n",
      "NOTE:      9   400  0.00041           0.9424     0.8726  1.583e+04       2310          0   0.001231     0.00\n",
      "NOTE:     10   400  0.00041           0.8631     0.8591  1.215e+04       1993          0   0.001231     0.00\n",
      "NOTE:     11   400  0.00041           0.9571     0.8665  1.509e+04       2324          0   0.001231     0.00\n",
      "NOTE:     12   400  0.00041           0.8433     0.8766  1.361e+04       1917          0   0.001231     0.00\n",
      "NOTE:     13   400  0.00041           0.8924     0.8918  1.379e+04       1673          0   0.001231     0.00\n",
      "NOTE:     14   400  0.00041           0.9915      0.809  1.413e+04       3336          0   0.001231     0.00\n",
      "NOTE:     15   400  0.00041           0.8692      0.874  1.398e+04       2015          0   0.001231     0.00\n",
      "NOTE:     16   400  0.00041            1.039     0.8403  1.554e+04       2952          0   0.001231     0.00\n",
      "NOTE:     17   400  0.00041           0.8115     0.8869  1.394e+04       1777          0   0.001231     0.00\n",
      "NOTE:     18   400  0.00041           0.9315     0.8323  1.287e+04       2593          0   0.001231     0.00\n",
      "NOTE:     19   400  0.00041           0.9969     0.8232  1.462e+04       3140          0   0.001232     0.00\n",
      "NOTE:     20   400  0.00041           0.7833     0.8582  1.178e+04       1947          0   0.001232     0.00\n",
      "NOTE:     21   400  0.00041           0.9982     0.8147  1.399e+04       3182          0   0.001232     0.00\n",
      "NOTE:     22   400  0.00041           0.9127     0.8193  1.178e+04       2598          0   0.001232     0.00\n",
      "NOTE:     23   400  0.00041           0.8273     0.8491  1.224e+04       2176          0   0.001232     0.00\n",
      "NOTE:     24   400  0.00041             0.87     0.9021  1.502e+04       1631          0   0.001232     0.00\n",
      "NOTE:     25   400  0.00041            0.819     0.8748  1.291e+04       1847          0   0.001232     0.00\n",
      "NOTE:     26   400  0.00041           0.7913     0.8518  1.178e+04       2050          0   0.001232     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  59       0.0004          0.8883     0.8563  3.642e+05   6.11e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00041           0.9822     0.8666  1.567e+04       2413          0   0.001232     0.00\n",
      "NOTE:      1   400  0.00041           0.8158     0.8579  1.244e+04       2060          0   0.001232     0.00\n",
      "NOTE:      2   400  0.00041           0.9338     0.8274  1.379e+04       2876          0   0.001232     0.00\n",
      "NOTE:      3   400  0.00041           0.9849     0.8744  1.572e+04       2258          0   0.001232     0.00\n",
      "NOTE:      4   400  0.00041            1.009     0.8808  1.599e+04       2163          0   0.001232     0.00\n",
      "NOTE:      5   400  0.00041           0.9272     0.8545  1.352e+04       2302          0   0.001232     0.00\n",
      "NOTE:      6   400  0.00041           0.8681     0.7733  1.206e+04       3535          0   0.001232     0.00\n",
      "NOTE:      7   400  0.00041           0.8674     0.8239  1.182e+04       2525          0   0.001232     0.00\n",
      "NOTE:      8   400  0.00041           0.9439      0.837  1.362e+04       2652          0   0.001232     0.00\n",
      "NOTE:      9   400  0.00041           0.9084     0.8603  1.421e+04       2309          0   0.001232     0.00\n",
      "NOTE:     10   400  0.00041            1.002     0.8324  1.489e+04       2998          0   0.001232     0.00\n",
      "NOTE:     11   400  0.00041           0.8629     0.8138  1.264e+04       2892          0   0.001232     0.00\n",
      "NOTE:     12   400  0.00041           0.9647     0.8669  1.586e+04       2434          0   0.001232     0.00\n",
      "NOTE:     13   400  0.00041           0.7519     0.8266  1.059e+04       2222          0   0.001232     0.00\n",
      "NOTE:     14   400  0.00041           0.8602     0.8392  1.297e+04       2484          0   0.001232     0.00\n",
      "NOTE:     15   400  0.00041            1.029     0.8893  1.583e+04       1970          0   0.001232     0.00\n",
      "NOTE:     16   400  0.00041           0.7803     0.8464  1.196e+04       2169          0   0.001232     0.00\n",
      "NOTE:     17   400  0.00041           0.8029     0.8519  1.235e+04       2147          0   0.001232     0.00\n",
      "NOTE:     18   400  0.00041           0.9412     0.8433  1.542e+04       2865          0   0.001232     0.00\n",
      "NOTE:     19   400  0.00041           0.9011     0.8584  1.431e+04       2361          0   0.001232     0.00\n",
      "NOTE:     20   400  0.00041           0.9001     0.8641  1.385e+04       2178          0   0.001233     0.00\n",
      "NOTE:     21   400  0.00041           0.7763     0.8794  1.202e+04       1649          0   0.001233     0.00\n",
      "NOTE:     22   400  0.00041           0.7893     0.8635  1.228e+04       1942          0   0.001233     0.00\n",
      "NOTE:     23   400  0.00041           0.8871     0.8657   1.39e+04       2156          0   0.001233     0.00\n",
      "NOTE:     24   400  0.00041           0.8924     0.8287  1.177e+04       2432          0   0.001233     0.00\n",
      "NOTE:     25   400  0.00041           0.8147     0.8702  1.295e+04       1932          0   0.001233     0.00\n",
      "NOTE:     26   400  0.00041           0.9551     0.8976  1.745e+04       1991          0   0.001233     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  60       0.0004          0.8945     0.8526  3.698e+05  6.392e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00041           0.7624       0.82   1.04e+04       2283          0   0.001233     0.00\n",
      "NOTE:      1   400  0.00041            1.005      0.875  1.754e+04       2505          0   0.001233     0.00\n",
      "NOTE:      2   400  0.00041           0.9456     0.8684  1.462e+04       2216          0   0.001233     0.00\n",
      "NOTE:      3   400  0.00041           0.8759     0.8587  1.457e+04       2397          0   0.001233     0.00\n",
      "NOTE:      4   400  0.00041           0.8866      0.858  1.281e+04       2120          0   0.001233     0.00\n",
      "NOTE:      5   400  0.00041           0.8355     0.8475  1.179e+04       2121          0   0.001234     0.00\n",
      "NOTE:      6   400  0.00041           0.8954     0.8913  1.418e+04       1730          0   0.001234     0.00\n",
      "NOTE:      7   400  0.00041           0.9407     0.8654  1.579e+04       2456          0   0.001234     0.00\n",
      "NOTE:      8   400  0.00041           0.9034     0.9126  1.499e+04       1436          0   0.001234     0.00\n",
      "NOTE:      9   400  0.00041           0.7603     0.8536  1.068e+04       1831          0   0.001234     0.00\n",
      "NOTE:     10   400  0.00041           0.8827     0.8536  1.322e+04       2268          0   0.001234     0.00\n",
      "NOTE:     11   400  0.00041           0.9116     0.8598  1.481e+04       2414          0   0.001234     0.00\n",
      "NOTE:     12   400  0.00041           0.9601      0.853  1.422e+04       2450          0   0.001234     0.00\n",
      "NOTE:     13   400  0.00041           0.9347     0.8819  1.529e+04       2047          0   0.001234     0.00\n",
      "NOTE:     14   400  0.00041           0.8842     0.8718  1.392e+04       2047          0   0.001235     0.00\n",
      "NOTE:     15   400  0.00041           0.8199     0.8596  1.385e+04       2263          0   0.001235     0.00\n",
      "NOTE:     16   400  0.00041           0.8989     0.8349  1.438e+04       2845          0   0.001235     0.00\n",
      "NOTE:     17   400  0.00041           0.8068     0.8659  1.301e+04       2014          0   0.001235     0.00\n",
      "NOTE:     18   400  0.00041            0.864     0.8447   1.25e+04       2298          0   0.001235     0.00\n",
      "NOTE:     19   400  0.00041            1.041     0.8704  1.683e+04       2506          0   0.001235     0.00\n",
      "NOTE:     20   400  0.00041           0.8886     0.8605  1.434e+04       2325          0   0.001235     0.00\n",
      "NOTE:     21   400  0.00041           0.8946     0.8639  1.362e+04       2145          0   0.001235     0.00\n",
      "NOTE:     22   400  0.00041           0.9295     0.8496  1.412e+04       2500          0   0.001236     0.00\n",
      "NOTE:     23   400  0.00041           0.9517     0.8927  1.585e+04       1906          0   0.001236     0.00\n",
      "NOTE:     24   400  0.00041            1.007     0.8669   1.56e+04       2396          0   0.001236     0.00\n",
      "NOTE:     25   400  0.00041           0.8857     0.8711  1.449e+04       2145          0   0.001236     0.00\n",
      "NOTE:     26   400  0.00041           0.9661     0.8466   1.51e+04       2736          0   0.001236     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  61       0.0004          0.9014     0.8636  3.825e+05   6.04e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00041           0.8076     0.8508  1.242e+04       2179          0   0.001236     0.00\n",
      "NOTE:      1   400  0.00041            1.034     0.8151  1.478e+04       3354          0   0.001236     0.00\n",
      "NOTE:      2   400  0.00041           0.9191     0.8725  1.444e+04       2109          0   0.001236     0.00\n",
      "NOTE:      3   400  0.00041           0.9092     0.8545  1.424e+04       2424          0   0.001236     0.00\n",
      "NOTE:      4   400  0.00041           0.9013     0.8464  1.491e+04       2704          0   0.001237     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      5   400  0.00041            1.023     0.8663  1.616e+04       2494          0   0.001237     0.00\n",
      "NOTE:      6   400  0.00041            1.044     0.8505  1.562e+04       2745          0   0.001237     0.00\n",
      "NOTE:      7   400  0.00041           0.9454     0.8773   1.46e+04       2041          0   0.001237     0.00\n",
      "NOTE:      8   400  0.00041           0.8531     0.8304  1.379e+04       2815          0   0.001237     0.00\n",
      "NOTE:      9   400  0.00041           0.9442     0.8328  1.339e+04       2688          0   0.001237     0.00\n",
      "NOTE:     10   400  0.00041           0.9929     0.8729  1.696e+04       2469          0   0.001237     0.00\n",
      "NOTE:     11   400  0.00041           0.9032     0.8718  1.439e+04       2117          0   0.001237     0.00\n",
      "NOTE:     12   400  0.00041           0.8493     0.8373  1.199e+04       2330          0   0.001237     0.00\n",
      "NOTE:     13   400  0.00041           0.9101     0.8708  1.377e+04       2043          0   0.001237     0.00\n",
      "NOTE:     14   400  0.00041            1.023     0.8452  1.566e+04       2867          0   0.001237     0.00\n",
      "NOTE:     15   400  0.00041           0.8853     0.8212   1.36e+04       2961          0   0.001237     0.00\n",
      "NOTE:     16   400  0.00041           0.9341     0.9016  1.508e+04       1645          0   0.001237     0.00\n",
      "NOTE:     17   400  0.00041           0.8977     0.8868  1.488e+04       1900          0   0.001237     0.00\n",
      "NOTE:     18   400  0.00041            1.026     0.8569  1.703e+04       2843          0   0.001238     0.00\n",
      "NOTE:     19   400  0.00041           0.8511     0.8408  1.232e+04       2332          0   0.001238     0.00\n",
      "NOTE:     20   400  0.00041           0.7277      0.851  1.215e+04       2127          0   0.001238     0.00\n",
      "NOTE:     21   400  0.00041           0.8356     0.8864  1.407e+04       1803          0   0.001238     0.00\n",
      "NOTE:     22   400  0.00041           0.8873     0.8858  1.495e+04       1928          0   0.001238     0.00\n",
      "NOTE:     23   400  0.00041           0.8288     0.8696  1.258e+04       1887          0   0.001238     0.00\n",
      "NOTE:     24   400  0.00041           0.9407     0.8306  1.363e+04       2781          0   0.001238     0.00\n",
      "NOTE:     25   400  0.00041           0.8707     0.8464  1.401e+04       2542          0   0.001238     0.00\n",
      "NOTE:     26   400  0.00041           0.7998      0.826  1.179e+04       2482          0   0.001238     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  62       0.0004           0.909     0.8557  3.832e+05  6.461e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00041           0.8804     0.8433  1.261e+04       2343          0   0.001238     0.00\n",
      "NOTE:      1   400  0.00041           0.8566     0.8591  1.333e+04       2187          0   0.001239     0.00\n",
      "NOTE:      2   400  0.00041            0.659     0.8785   1.14e+04       1576          0   0.001239     0.00\n",
      "NOTE:      3   400  0.00041           0.8425     0.8901  1.473e+04       1819          0   0.001239     0.00\n",
      "NOTE:      4   400  0.00041           0.8208      0.911  1.434e+04       1400          0   0.001239     0.00\n",
      "NOTE:      5   400  0.00041           0.8939      0.888   1.59e+04       2006          0   0.001239     0.00\n",
      "NOTE:      6   400  0.00041           0.8514       0.84  1.303e+04       2482          0   0.001239     0.00\n",
      "NOTE:      7   400  0.00041           0.7828     0.9008  1.345e+04       1480          0   0.001239     0.00\n",
      "NOTE:      8   400  0.00041           0.9634     0.8688  1.453e+04       2194          0   0.001239     0.00\n",
      "NOTE:      9   400  0.00041           0.8524     0.8906  1.413e+04       1736          0    0.00124     0.00\n",
      "NOTE:     10   400  0.00041           0.8478     0.8337   1.39e+04       2772          0    0.00124     0.00\n",
      "NOTE:     11   400  0.00041           0.8807     0.8803  1.586e+04       2156          0    0.00124     0.00\n",
      "NOTE:     12   400  0.00041           0.9544     0.8346  1.481e+04       2935          0    0.00124     0.00\n",
      "NOTE:     13   400  0.00041           0.8298     0.8838   1.38e+04       1815          0    0.00124     0.00\n",
      "NOTE:     14   400  0.00041           0.7569      0.887  1.366e+04       1741          0    0.00124     0.00\n",
      "NOTE:     15   400  0.00041            1.041     0.8495  1.673e+04       2963          0   0.001241     0.00\n",
      "NOTE:     16   400  0.00041           0.8158     0.8512  1.185e+04       2071          0   0.001241     0.00\n",
      "NOTE:     17   400  0.00041            0.868     0.8391   1.24e+04       2379          0   0.001241     0.00\n",
      "NOTE:     18   400  0.00041            1.006      0.847  1.438e+04       2597          0   0.001241     0.00\n",
      "NOTE:     19   400  0.00041           0.8865     0.8599  1.448e+04       2359          0   0.001241     0.00\n",
      "NOTE:     20   400  0.00041            0.842     0.8626  1.247e+04       1986          0   0.001241     0.00\n",
      "NOTE:     21   400  0.00041           0.8771     0.8393  1.261e+04       2413          0   0.001241     0.00\n",
      "NOTE:     22   400  0.00041            1.001     0.8455  1.551e+04       2833          0   0.001241     0.00\n",
      "NOTE:     23   400  0.00041           0.8692     0.8544  1.335e+04       2275          0   0.001242     0.00\n",
      "NOTE:     24   400  0.00041           0.8654     0.8477  1.394e+04       2504          0   0.001242     0.00\n",
      "NOTE:     25   400  0.00041           0.8756     0.8261   1.26e+04       2651          0   0.001242     0.00\n",
      "NOTE:     26   400  0.00041             0.99     0.8632   1.63e+04       2584          0   0.001242     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  63       0.0004          0.8745     0.8619  3.761e+05  6.026e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00041            0.867     0.9128  1.533e+04       1465          0   0.001242     0.00\n",
      "NOTE:      1   400  0.00041           0.7359     0.8885   1.27e+04       1593          0   0.001242     0.00\n",
      "NOTE:      2   400  0.00041           0.8513     0.8866  1.287e+04       1646          0   0.001242     0.00\n",
      "NOTE:      3   400  0.00041           0.6836      0.884  1.208e+04       1585          0   0.001242     0.00\n",
      "NOTE:      4   400  0.00041           0.9218     0.8587  1.375e+04       2263          0   0.001242     0.00\n",
      "NOTE:      5   400  0.00041            0.935     0.9134  1.505e+04       1428          0   0.001243     0.00\n",
      "NOTE:      6   400  0.00041           0.8337     0.9131  1.411e+04       1343          0   0.001243     0.00\n",
      "NOTE:      7   400  0.00041           0.7375     0.8405  1.073e+04       2036          0   0.001243     0.00\n",
      "NOTE:      8   400  0.00041           0.8054     0.8379  1.111e+04       2149          0   0.001243     0.00\n",
      "NOTE:      9   400  0.00041            1.043     0.8451  1.534e+04       2812          0   0.001243     0.00\n",
      "NOTE:     10   400  0.00041           0.8997     0.8529  1.456e+04       2512          0   0.001243     0.00\n",
      "NOTE:     11   400  0.00041            1.105      0.903  1.838e+04       1974          0   0.001243     0.00\n",
      "NOTE:     12   400  0.00041           0.8572     0.8905  1.411e+04       1735          0   0.001243     0.00\n",
      "NOTE:     13   400  0.00041           0.7832     0.8957   1.34e+04       1560          0   0.001244     0.00\n",
      "NOTE:     14   400  0.00041           0.9109     0.8825  1.522e+04       2026          0   0.001244     0.00\n",
      "NOTE:     15   400  0.00041           0.6702     0.8889  1.131e+04       1414          0   0.001244     0.00\n",
      "NOTE:     16   400  0.00041           0.7843     0.8761  1.288e+04       1822          0   0.001244     0.00\n",
      "NOTE:     17   400  0.00041           0.9841     0.8548  1.515e+04       2574          0   0.001244     0.00\n",
      "NOTE:     18   400  0.00041           0.7232     0.9149  1.222e+04       1136          0   0.001244     0.00\n",
      "NOTE:     19   400  0.00041           0.9135       0.89  1.549e+04       1914          0   0.001244     0.00\n",
      "NOTE:     20   400  0.00041            1.005     0.8643  1.615e+04       2535          0   0.001245     0.00\n",
      "NOTE:     21   400  0.00041            1.011     0.8376  1.455e+04       2820          0   0.001245     0.00\n",
      "NOTE:     22   400  0.00041           0.9477     0.8746  1.576e+04       2259          0   0.001245     0.00\n",
      "NOTE:     23   400  0.00041            0.832     0.8103  1.206e+04       2823          0   0.001245     0.00\n",
      "NOTE:     24   400  0.00041           0.9123     0.8714  1.453e+04       2143          0   0.001245     0.00\n",
      "NOTE:     25   400  0.00041           0.8891     0.8844  1.401e+04       1832          0   0.001245     0.00\n",
      "NOTE:     26   400  0.00041           0.7402     0.8489  1.143e+04       2034          0   0.001245     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  64       0.0004           0.866     0.8751  3.743e+05  5.343e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00041           0.8602     0.9143  1.385e+04       1299          0   0.001245     0.00\n",
      "NOTE:      1   400  0.00041           0.8613     0.8688  1.314e+04       1984          0   0.001246     0.00\n",
      "NOTE:      2   400  0.00041           0.8063     0.8306  1.158e+04       2362          0   0.001246     0.00\n",
      "NOTE:      3   400  0.00041           0.9263     0.8703  1.456e+04       2170          0   0.001246     0.00\n",
      "NOTE:      4   400  0.00041           0.9309     0.8829  1.595e+04       2115          0   0.001246     0.00\n",
      "NOTE:      5   400  0.00041            1.003     0.8842  1.607e+04       2105          0   0.001246     0.00\n",
      "NOTE:      6   400  0.00041            1.127     0.8156  1.506e+04       3406          0   0.001246     0.00\n",
      "NOTE:      7   400  0.00041            1.001     0.8515  1.645e+04       2869          0   0.001246     0.00\n",
      "NOTE:      8   400  0.00041           0.8272     0.8828  1.389e+04       1843          0   0.001246     0.00\n",
      "NOTE:      9   400  0.00041            1.055     0.8544  1.679e+04       2861          0   0.001247     0.00\n",
      "NOTE:     10   400  0.00041           0.8909      0.826  1.269e+04       2673          0   0.001247     0.00\n",
      "NOTE:     11   400  0.00041            0.986     0.8585  1.508e+04       2485          0   0.001247     0.00\n",
      "NOTE:     12   400  0.00041           0.8518     0.8006  1.263e+04       3146          0   0.001247     0.00\n",
      "NOTE:     13   400  0.00041           0.8403     0.8687  1.321e+04       1997          0   0.001247     0.00\n",
      "NOTE:     14   400  0.00041           0.6835     0.7985       8563       2161          0   0.001247     0.00\n",
      "NOTE:     15   400  0.00041           0.8662     0.8573  1.374e+04       2287          0   0.001247     0.00\n",
      "NOTE:     16   400  0.00041           0.8371      0.877  1.303e+04       1827          0   0.001247     0.00\n",
      "NOTE:     17   400  0.00041           0.9058     0.8681  1.376e+04       2091          0   0.001247     0.00\n",
      "NOTE:     18   400  0.00041           0.7925      0.842  1.237e+04       2321          0   0.001247     0.00\n",
      "NOTE:     19   400  0.00041           0.8585     0.9394  1.718e+04       1109          0   0.001248     0.00\n",
      "NOTE:     20   400  0.00041           0.9422     0.8684  1.523e+04       2309          0   0.001248     0.00\n",
      "NOTE:     21   400  0.00041           0.9334     0.8985  1.566e+04       1770          0   0.001248     0.00\n",
      "NOTE:     22   400  0.00041           0.7538      0.855  1.224e+04       2076          0   0.001248     0.00\n",
      "NOTE:     23   400  0.00041           0.8067     0.8651  1.211e+04       1889          0   0.001248     0.00\n",
      "NOTE:     24   400  0.00041            1.003     0.8475  1.455e+04       2619          0   0.001248     0.00\n",
      "NOTE:     25   400  0.00041           0.9445     0.8375  1.385e+04       2688          0   0.001249     0.00\n",
      "NOTE:     26   400  0.00041           0.8337     0.8641  1.383e+04       2175          0   0.001249     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  65       0.0004          0.8936     0.8615  3.771e+05  6.064e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000328           0.8237     0.8844  1.316e+04       1720          0   0.001249     0.00\n",
      "NOTE:      1   400 0.000328           0.9307     0.8592   1.41e+04       2311          0   0.001249     0.00\n",
      "NOTE:      2   400 0.000328           0.8438     0.8153  1.138e+04       2579          0   0.001249     0.00\n",
      "NOTE:      3   400 0.000328           0.9706     0.8628  1.548e+04       2462          0   0.001249     0.00\n",
      "NOTE:      4   400 0.000328           0.9488     0.8498   1.61e+04       2844          0   0.001249     0.00\n",
      "NOTE:      5   400 0.000328           0.9796     0.8442  1.457e+04       2688          0   0.001249     0.00\n",
      "NOTE:      6   400 0.000328            1.025     0.8387  1.569e+04       3019          0    0.00125     0.00\n",
      "NOTE:      7   400 0.000328            0.972     0.8632  1.521e+04       2409          0    0.00125     0.00\n",
      "NOTE:      8   400 0.000328           0.8678     0.8655  1.407e+04       2186          0    0.00125     0.00\n",
      "NOTE:      9   400 0.000328           0.9411     0.8783  1.554e+04       2153          0    0.00125     0.00\n",
      "NOTE:     10   400 0.000328           0.8055     0.8725  1.484e+04       2169          0    0.00125     0.00\n",
      "NOTE:     11   400 0.000328           0.8048     0.8587  1.175e+04       1933          0    0.00125     0.00\n",
      "NOTE:     12   400 0.000328           0.8044     0.8521  1.295e+04       2248          0    0.00125     0.00\n",
      "NOTE:     13   400 0.000328           0.9405      0.797  1.238e+04       3152          0    0.00125     0.00\n",
      "NOTE:     14   400 0.000328           0.8972     0.8663  1.414e+04       2181          0    0.00125     0.00\n",
      "NOTE:     15   400 0.000328           0.7636     0.8886  1.242e+04       1557          0    0.00125     0.00\n",
      "NOTE:     16   400 0.000328           0.9266     0.8475  1.338e+04       2409          0   0.001251     0.00\n",
      "NOTE:     17   400 0.000328            1.062     0.8313  1.577e+04       3201          0   0.001251     0.00\n",
      "NOTE:     18   400 0.000328           0.9356     0.8133  1.307e+04       3000          0   0.001251     0.00\n",
      "NOTE:     19   400 0.000328           0.8243     0.8862  1.341e+04       1722          0   0.001251     0.00\n",
      "NOTE:     20   400 0.000328           0.9442     0.8597  1.518e+04       2479          0   0.001251     0.00\n",
      "NOTE:     21   400 0.000328           0.9362      0.873   1.44e+04       2094          0   0.001251     0.00\n",
      "NOTE:     22   400 0.000328            1.022     0.8355  1.426e+04       2808          0   0.001251     0.00\n",
      "NOTE:     23   400 0.000328            1.044      0.855  1.665e+04       2823          0   0.001251     0.00\n",
      "NOTE:     24   400 0.000328           0.8352     0.9005  1.488e+04       1644          0   0.001251     0.00\n",
      "NOTE:     25   400 0.000328           0.8794      0.885  1.381e+04       1795          0   0.001251     0.00\n",
      "NOTE:     26   400 0.000328           0.9635     0.8754  1.477e+04       2101          0   0.001251     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  66       0.0003          0.9145     0.8575  3.833e+05  6.369e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000328           0.8252     0.8463  1.225e+04       2225          0   0.001251     0.00\n",
      "NOTE:      1   400 0.000328           0.9336     0.8368  1.438e+04       2805          0   0.001251     0.00\n",
      "NOTE:      2   400 0.000328           0.8543     0.8929  1.537e+04       1843          0   0.001251     0.00\n",
      "NOTE:      3   400 0.000328           0.7938     0.8484  1.243e+04       2222          0   0.001251     0.00\n",
      "NOTE:      4   400 0.000328           0.9381      0.871  1.564e+04       2316          0   0.001251     0.00\n",
      "NOTE:      5   400 0.000328           0.7746     0.9043  1.429e+04       1513          0   0.001251     0.00\n",
      "NOTE:      6   400 0.000328            0.874     0.8147  1.303e+04       2964          0   0.001252     0.00\n",
      "NOTE:      7   400 0.000328           0.7966     0.8787  1.433e+04       1978          0   0.001252     0.00\n",
      "NOTE:      8   400 0.000328           0.8308     0.8674  1.314e+04       2009          0   0.001252     0.00\n",
      "NOTE:      9   400 0.000328           0.9367     0.8516  1.454e+04       2534          0   0.001252     0.00\n",
      "NOTE:     10   400 0.000328           0.8202     0.8212  1.257e+04       2738          0   0.001252     0.00\n",
      "NOTE:     11   400 0.000328            1.004     0.8819  1.683e+04       2253          0   0.001252     0.00\n",
      "NOTE:     12   400 0.000328           0.7958     0.9047  1.409e+04       1484          0   0.001252     0.00\n",
      "NOTE:     13   400 0.000328           0.9188     0.8413  1.394e+04       2629          0   0.001252     0.00\n",
      "NOTE:     14   400 0.000328           0.9372     0.8384   1.37e+04       2640          0   0.001252     0.00\n",
      "NOTE:     15   400 0.000328           0.8806     0.8462  1.381e+04       2511          0   0.001253     0.00\n",
      "NOTE:     16   400 0.000328            1.006     0.8112   1.42e+04       3303          0   0.001253     0.00\n",
      "NOTE:     17   400 0.000328            1.038     0.8834  1.702e+04       2247          0   0.001253     0.00\n",
      "NOTE:     18   400 0.000328           0.9879     0.8826  1.673e+04       2225          0   0.001253     0.00\n",
      "NOTE:     19   400 0.000328           0.8315     0.8729  1.294e+04       1883          0   0.001253     0.00\n",
      "NOTE:     20   400 0.000328           0.9606     0.8479   1.37e+04       2459          0   0.001253     0.00\n",
      "NOTE:     21   400 0.000328            1.012     0.8492  1.478e+04       2624          0   0.001253     0.00\n",
      "NOTE:     22   400 0.000328           0.7917     0.8335   1.16e+04       2318          0   0.001253     0.00\n",
      "NOTE:     23   400 0.000328           0.7908      0.849  1.199e+04       2131          0   0.001253     0.00\n",
      "NOTE:     24   400 0.000328           0.7503     0.8524  1.157e+04       2004          0   0.001253     0.00\n",
      "NOTE:     25   400 0.000328           0.7794     0.8424  1.169e+04       2187          0   0.001253     0.00\n",
      "NOTE:     26   400 0.000328           0.9602     0.8338  1.435e+04       2859          0   0.001253     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  67       0.0003          0.8823     0.8563  3.749e+05   6.29e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000328           0.7394     0.8851  1.147e+04       1488          0   0.001253     0.00\n",
      "NOTE:      1   400 0.000328           0.8527     0.8633  1.423e+04       2254          0   0.001253     0.00\n",
      "NOTE:      2   400 0.000328           0.9301     0.8764  1.537e+04       2168          0   0.001254     0.00\n",
      "NOTE:      3   400 0.000328             1.06     0.8449  1.503e+04       2759          0   0.001254     0.00\n",
      "NOTE:      4   400 0.000328           0.9006     0.8836  1.561e+04       2056          0   0.001254     0.00\n",
      "NOTE:      5   400 0.000328           0.8826     0.8637  1.389e+04       2193          0   0.001254     0.00\n",
      "NOTE:      6   400 0.000328           0.8053     0.8341  1.256e+04       2499          0   0.001254     0.00\n",
      "NOTE:      7   400 0.000328           0.7841      0.885  1.158e+04       1505          0   0.001254     0.00\n",
      "NOTE:      8   400 0.000328           0.9278     0.8562  1.444e+04       2425          0   0.001254     0.00\n",
      "NOTE:      9   400 0.000328           0.8849     0.8822  1.452e+04       1939          0   0.001254     0.00\n",
      "NOTE:     10   400 0.000328           0.7771      0.889  1.333e+04       1665          0   0.001254     0.00\n",
      "NOTE:     11   400 0.000328           0.8509      0.853  1.382e+04       2381          0   0.001254     0.00\n",
      "NOTE:     12   400 0.000328           0.8814     0.8347  1.242e+04       2458          0   0.001254     0.00\n",
      "NOTE:     13   400 0.000328           0.8908     0.8625  1.436e+04       2289          0   0.001254     0.00\n",
      "NOTE:     14   400 0.000328           0.8397     0.8722  1.325e+04       1941          0   0.001254     0.00\n",
      "NOTE:     15   400 0.000328           0.8696     0.8902  1.534e+04       1893          0   0.001254     0.00\n",
      "NOTE:     16   400 0.000328           0.9188     0.8266  1.386e+04       2908          0   0.001254     0.00\n",
      "NOTE:     17   400 0.000328           0.8246     0.8436  1.271e+04       2356          0   0.001254     0.00\n",
      "NOTE:     18   400 0.000328            1.012     0.8523  1.484e+04       2573          0   0.001255     0.00\n",
      "NOTE:     19   400 0.000328           0.7696     0.9051  1.442e+04       1511          0   0.001255     0.00\n",
      "NOTE:     20   400 0.000328            1.103     0.8721  1.778e+04       2608          0   0.001255     0.00\n",
      "NOTE:     21   400 0.000328            1.004      0.865  1.648e+04       2571          0   0.001255     0.00\n",
      "NOTE:     22   400 0.000328            0.916      0.866  1.438e+04       2224          0   0.001255     0.00\n",
      "NOTE:     23   400 0.000328           0.8232     0.8544  1.218e+04       2076          0   0.001255     0.00\n",
      "NOTE:     24   400 0.000328           0.9356     0.8213  1.314e+04       2859          0   0.001255     0.00\n",
      "NOTE:     25   400 0.000328           0.9955      0.822  1.419e+04       3073          0   0.001255     0.00\n",
      "NOTE:     26   400 0.000328           0.8825     0.8271  1.205e+04       2518          0   0.001255     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  68       0.0003          0.8912     0.8604  3.772e+05  6.119e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000328           0.9487      0.815  1.365e+04       3100          0   0.001255     0.00\n",
      "NOTE:      1   400 0.000328           0.8818     0.8811  1.523e+04       2055          0   0.001255     0.00\n",
      "NOTE:      2   400 0.000328            0.982     0.8862  1.634e+04       2099          0   0.001255     0.00\n",
      "NOTE:      3   400 0.000328           0.8395     0.8856  1.321e+04       1706          0   0.001255     0.00\n",
      "NOTE:      4   400 0.000328           0.9348     0.8859  1.539e+04       1982          0   0.001255     0.00\n",
      "NOTE:      5   400 0.000328           0.9205     0.8617  1.422e+04       2282          0   0.001255     0.00\n",
      "NOTE:      6   400 0.000328           0.8637     0.8871  1.364e+04       1736          0   0.001256     0.00\n",
      "NOTE:      7   400 0.000328           0.9356     0.8473  1.354e+04       2439          0   0.001256     0.00\n",
      "NOTE:      8   400 0.000328           0.8679     0.8329  1.203e+04       2414          0   0.001256     0.00\n",
      "NOTE:      9   400 0.000328            0.803     0.8875  1.349e+04       1710          0   0.001256     0.00\n",
      "NOTE:     10   400 0.000328           0.9617     0.8914  1.526e+04       1859          0   0.001256     0.00\n",
      "NOTE:     11   400 0.000328           0.7259      0.842  1.026e+04       1925          0   0.001256     0.00\n",
      "NOTE:     12   400 0.000328           0.8526     0.8535  1.391e+04       2388          0   0.001256     0.00\n",
      "NOTE:     13   400 0.000328           0.7698     0.8791  1.214e+04       1670          0   0.001256     0.00\n",
      "NOTE:     14   400 0.000328           0.9739       0.84  1.357e+04       2585          0   0.001256     0.00\n",
      "NOTE:     15   400 0.000328           0.8164     0.8266  1.254e+04       2632          0   0.001256     0.00\n",
      "NOTE:     16   400 0.000328            0.863     0.8658  1.452e+04       2250          0   0.001256     0.00\n",
      "NOTE:     17   400 0.000328            0.758     0.8676   1.13e+04       1723          0   0.001256     0.00\n",
      "NOTE:     18   400 0.000328           0.8027     0.8821  1.357e+04       1813          0   0.001257     0.00\n",
      "NOTE:     19   400 0.000328           0.8298     0.8899  1.447e+04       1790          0   0.001257     0.00\n",
      "NOTE:     20   400 0.000328           0.8327     0.8815  1.261e+04       1695          0   0.001257     0.00\n",
      "NOTE:     21   400 0.000328           0.8106     0.8825  1.299e+04       1730          0   0.001257     0.00\n",
      "NOTE:     22   400 0.000328           0.8294     0.8858  1.418e+04       1829          0   0.001257     0.00\n",
      "NOTE:     23   400 0.000328           0.9649     0.8526  1.466e+04       2535          0   0.001257     0.00\n",
      "NOTE:     24   400 0.000328            1.006     0.8281  1.453e+04       3016          0   0.001257     0.00\n",
      "NOTE:     25   400 0.000328           0.8864      0.869  1.428e+04       2153          0   0.001257     0.00\n",
      "NOTE:     26   400 0.000328           0.9449     0.8408  1.474e+04       2792          0   0.001257     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  69       0.0003          0.8743     0.8648  3.703e+05  5.791e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000328           0.8822     0.8502  1.351e+04       2381          0   0.001257     0.00\n",
      "NOTE:      1   400 0.000328           0.8651      0.856  1.299e+04       2184          0   0.001257     0.00\n",
      "NOTE:      2   400 0.000328           0.8713     0.8648  1.491e+04       2331          0   0.001257     0.00\n",
      "NOTE:      3   400 0.000328           0.9251     0.8874  1.447e+04       1837          0   0.001257     0.00\n",
      "NOTE:      4   400 0.000328           0.7911     0.8677  1.255e+04       1913          0   0.001258     0.00\n",
      "NOTE:      5   400 0.000328           0.9505     0.8523   1.39e+04       2408          0   0.001258     0.00\n",
      "NOTE:      6   400 0.000328           0.9563     0.8391  1.436e+04       2754          0   0.001258     0.00\n",
      "NOTE:      7   400 0.000328           0.8921     0.8815  1.494e+04       2007          0   0.001258     0.00\n",
      "NOTE:      8   400 0.000328           0.7141     0.8857  1.147e+04       1480          0   0.001258     0.00\n",
      "NOTE:      9   400 0.000328            1.047     0.8511  1.497e+04       2620          0   0.001258     0.00\n",
      "NOTE:     10   400 0.000328           0.9008     0.8713  1.326e+04       1958          0   0.001258     0.00\n",
      "NOTE:     11   400 0.000328           0.9147     0.8431  1.365e+04       2539          0   0.001258     0.00\n",
      "NOTE:     12   400 0.000328           0.8057      0.819  1.246e+04       2752          0   0.001258     0.00\n",
      "NOTE:     13   400 0.000328           0.9235     0.8591   1.38e+04       2262          0   0.001258     0.00\n",
      "NOTE:     14   400 0.000328           0.9343     0.8727  1.561e+04       2276          0   0.001258     0.00\n",
      "NOTE:     15   400 0.000328            0.931     0.7815  1.232e+04       3443          0   0.001258     0.00\n",
      "NOTE:     16   400 0.000328           0.8847     0.8368  1.369e+04       2669          0   0.001258     0.00\n",
      "NOTE:     17   400 0.000328           0.8779      0.862  1.361e+04       2178          0   0.001258     0.00\n",
      "NOTE:     18   400 0.000328           0.9984     0.8711  1.526e+04       2258          0   0.001258     0.00\n",
      "NOTE:     19   400 0.000328            1.066     0.8846  1.791e+04       2337          0   0.001258     0.00\n",
      "NOTE:     20   400 0.000328           0.8628     0.8748  1.349e+04       1931          0   0.001258     0.00\n",
      "NOTE:     21   400 0.000328           0.9405     0.8782  1.491e+04       2069          0   0.001259     0.00\n",
      "NOTE:     22   400 0.000328            1.016     0.8143  1.434e+04       3270          0   0.001259     0.00\n",
      "NOTE:     23   400 0.000328           0.8311     0.8892  1.236e+04       1539          0   0.001259     0.00\n",
      "NOTE:     24   400 0.000328            0.971     0.8653  1.458e+04       2270          0   0.001259     0.00\n",
      "NOTE:     25   400 0.000328           0.7742     0.8833  1.302e+04       1720          0   0.001259     0.00\n",
      "NOTE:     26   400 0.000328            1.075     0.8427   1.59e+04       2969          0   0.001259     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  70       0.0003          0.9112     0.8585  3.782e+05  6.236e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000328            1.117      0.828  1.705e+04       3543          0   0.001259     0.00\n",
      "NOTE:      1   400 0.000328           0.8732     0.8796  1.388e+04       1901          0   0.001259     0.00\n",
      "NOTE:      2   400 0.000328           0.9368     0.9067  1.577e+04       1622          0   0.001259     0.00\n",
      "NOTE:      3   400 0.000328           0.8201     0.8534  1.205e+04       2071          0   0.001259     0.00\n",
      "NOTE:      4   400 0.000328            1.026     0.9039   1.74e+04       1850          0   0.001259     0.00\n",
      "NOTE:      5   400 0.000328           0.9824     0.8655  1.643e+04       2554          0   0.001259     0.00\n",
      "NOTE:      6   400 0.000328           0.7611     0.8469  1.167e+04       2111          0    0.00126     0.00\n",
      "NOTE:      7   400 0.000328            0.881     0.8751  1.459e+04       2082          0    0.00126     0.00\n",
      "NOTE:      8   400 0.000328            0.875      0.827  1.287e+04       2693          0    0.00126     0.00\n",
      "NOTE:      9   400 0.000328           0.8688     0.8513  1.366e+04       2387          0    0.00126     0.00\n",
      "NOTE:     10   400 0.000328           0.8122     0.8776  1.308e+04       1823          0    0.00126     0.00\n",
      "NOTE:     11   400 0.000328           0.9281     0.8008  1.282e+04       3190          0    0.00126     0.00\n",
      "NOTE:     12   400 0.000328           0.8614     0.9129  1.643e+04       1567          0    0.00126     0.00\n",
      "NOTE:     13   400 0.000328           0.9811     0.8769   1.57e+04       2204          0    0.00126     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     14   400 0.000328           0.8072     0.8352  1.133e+04       2236          0    0.00126     0.00\n",
      "NOTE:     15   400 0.000328           0.8519     0.8498  1.175e+04       2077          0    0.00126     0.00\n",
      "NOTE:     16   400 0.000328            1.034     0.8678  1.556e+04       2369          0   0.001261     0.00\n",
      "NOTE:     17   400 0.000328           0.7398     0.8861  1.244e+04       1598          0   0.001261     0.00\n",
      "NOTE:     18   400 0.000328           0.8998     0.8708  1.439e+04       2134          0   0.001261     0.00\n",
      "NOTE:     19   400 0.000328           0.8818     0.8766  1.429e+04       2012          0   0.001261     0.00\n",
      "NOTE:     20   400 0.000328           0.8503      0.897  1.438e+04       1651          0   0.001261     0.00\n",
      "NOTE:     21   400 0.000328           0.8884     0.8992  1.512e+04       1695          0   0.001261     0.00\n",
      "NOTE:     22   400 0.000328           0.8423     0.8444  1.223e+04       2253          0   0.001261     0.00\n",
      "NOTE:     23   400 0.000328           0.8333     0.8977  1.354e+04       1543          0   0.001261     0.00\n",
      "NOTE:     24   400 0.000328           0.8839      0.858  1.355e+04       2242          0   0.001261     0.00\n",
      "NOTE:     25   400 0.000328           0.9579     0.8552  1.457e+04       2467          0   0.001261     0.00\n",
      "NOTE:     26   400 0.000328           0.9244     0.8697  1.469e+04       2200          0   0.001262     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  71       0.0003          0.8933     0.8678  3.812e+05  5.808e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000328            0.889     0.8583  1.386e+04       2289          0   0.001262     0.00\n",
      "NOTE:      1   400 0.000328           0.9313     0.8856  1.503e+04       1941          0   0.001262     0.00\n",
      "NOTE:      2   400 0.000328            1.067       0.83  1.452e+04       2973          0   0.001262     0.00\n",
      "NOTE:      3   400 0.000328            0.813     0.8592   1.29e+04       2113          0   0.001262     0.00\n",
      "NOTE:      4   400 0.000328            0.893     0.8803  1.378e+04       1874          0   0.001262     0.00\n",
      "NOTE:      5   400 0.000328           0.9092     0.8517  1.377e+04       2398          0   0.001262     0.00\n",
      "NOTE:      6   400 0.000328            0.901     0.8435  1.215e+04       2253          0   0.001262     0.00\n",
      "NOTE:      7   400 0.000328            1.038     0.8649  1.615e+04       2524          0   0.001262     0.00\n",
      "NOTE:      8   400 0.000328           0.9104     0.8774  1.421e+04       1986          0   0.001262     0.00\n",
      "NOTE:      9   400 0.000328           0.9212     0.8917  1.446e+04       1755          0   0.001262     0.00\n",
      "NOTE:     10   400 0.000328           0.8521     0.8826  1.356e+04       1803          0   0.001262     0.00\n",
      "NOTE:     11   400 0.000328           0.7659     0.8886  1.234e+04       1547          0   0.001262     0.00\n",
      "NOTE:     12   400 0.000328           0.9392      0.825  1.317e+04       2794          0   0.001262     0.00\n",
      "NOTE:     13   400 0.000328           0.8822     0.8457  1.492e+04       2722          0   0.001262     0.00\n",
      "NOTE:     14   400 0.000328            0.893     0.8628  1.394e+04       2216          0   0.001263     0.00\n",
      "NOTE:     15   400 0.000328           0.9796     0.8847   1.68e+04       2189          0   0.001263     0.00\n",
      "NOTE:     16   400 0.000328           0.8404     0.8658  1.317e+04       2042          0   0.001263     0.00\n",
      "NOTE:     17   400 0.000328           0.9698     0.8491  1.351e+04       2401          0   0.001263     0.00\n",
      "NOTE:     18   400 0.000328           0.8813      0.858  1.348e+04       2231          0   0.001263     0.00\n",
      "NOTE:     19   400 0.000328           0.9311     0.8572  1.458e+04       2430          0   0.001263     0.00\n",
      "NOTE:     20   400 0.000328            0.834     0.8651   1.45e+04       2261          0   0.001263     0.00\n",
      "NOTE:     21   400 0.000328            0.719     0.8758  1.086e+04       1541          0   0.001263     0.00\n",
      "NOTE:     22   400 0.000328             0.77      0.887  1.326e+04       1690          0   0.001263     0.00\n",
      "NOTE:     23   400 0.000328            0.816     0.8902  1.345e+04       1658          0   0.001263     0.00\n",
      "NOTE:     24   400 0.000328            1.024      0.817  1.427e+04       3196          0   0.001264     0.00\n",
      "NOTE:     25   400 0.000328           0.9559     0.8346  1.353e+04       2681          0   0.001264     0.00\n",
      "NOTE:     26   400 0.000328            1.034     0.8654   1.64e+04       2550          0   0.001264     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  72       0.0003          0.9022     0.8624  3.766e+05  6.006e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000328           0.9817     0.8512  1.548e+04       2706          0   0.001264     0.00\n",
      "NOTE:      1   400 0.000328           0.9986     0.8818   1.65e+04       2210          0   0.001264     0.00\n",
      "NOTE:      2   400 0.000328            1.019     0.8397   1.57e+04       2998          0   0.001264     0.00\n",
      "NOTE:      3   400 0.000328           0.8278      0.868  1.302e+04       1980          0   0.001264     0.00\n",
      "NOTE:      4   400 0.000328            1.015     0.8788  1.701e+04       2346          0   0.001264     0.00\n",
      "NOTE:      5   400 0.000328           0.8494     0.8739  1.418e+04       2046          0   0.001264     0.00\n",
      "NOTE:      6   400 0.000328            0.818     0.8808  1.358e+04       1837          0   0.001264     0.00\n",
      "NOTE:      7   400 0.000328           0.7967     0.8947  1.276e+04       1501          0   0.001264     0.00\n",
      "NOTE:      8   400 0.000328           0.7911     0.8677  1.294e+04       1974          0   0.001264     0.00\n",
      "NOTE:      9   400 0.000328           0.7788     0.8796   1.23e+04       1683          0   0.001264     0.00\n",
      "NOTE:     10   400 0.000328            0.963     0.8672  1.528e+04       2341          0   0.001265     0.00\n",
      "NOTE:     11   400 0.000328           0.9058     0.8639  1.448e+04       2282          0   0.001265     0.00\n",
      "NOTE:     12   400 0.000328           0.9385     0.8143  1.356e+04       3093          0   0.001265     0.00\n",
      "NOTE:     13   400 0.000328           0.9083     0.8629  1.332e+04       2116          0   0.001265     0.00\n",
      "NOTE:     14   400 0.000328           0.7847     0.8525  1.262e+04       2183          0   0.001265     0.00\n",
      "NOTE:     15   400 0.000328           0.9028      0.876   1.38e+04       1953          0   0.001265     0.00\n",
      "NOTE:     16   400 0.000328           0.8281      0.836  1.171e+04       2296          0   0.001265     0.00\n",
      "NOTE:     17   400 0.000328           0.9687     0.8579   1.49e+04       2467          0   0.001265     0.00\n",
      "NOTE:     18   400 0.000328           0.8388     0.8346  1.248e+04       2473          0   0.001265     0.00\n",
      "NOTE:     19   400 0.000328           0.8902     0.8704  1.415e+04       2106          0   0.001265     0.00\n",
      "NOTE:     20   400 0.000328           0.8024     0.8898  1.215e+04       1505          0   0.001265     0.00\n",
      "NOTE:     21   400 0.000328           0.8239     0.8645  1.391e+04       2179          0   0.001265     0.00\n",
      "NOTE:     22   400 0.000328           0.8426     0.8949  1.525e+04       1790          0   0.001265     0.00\n",
      "NOTE:     23   400 0.000328           0.8557     0.8739  1.296e+04       1870          0   0.001265     0.00\n",
      "NOTE:     24   400 0.000328           0.7838     0.8709  1.343e+04       1991          0   0.001266     0.00\n",
      "NOTE:     25   400 0.000328           0.8946       0.85   1.42e+04       2506          0   0.001266     0.00\n",
      "NOTE:     26   400 0.000328           0.8344      0.857  1.423e+04       2375          0   0.001266     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  73       0.0003          0.8757     0.8647  3.759e+05  5.881e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000328            1.024     0.8421  1.527e+04       2862          0   0.001266     0.00\n",
      "NOTE:      1   400 0.000328           0.8539     0.8296  1.242e+04       2551          0   0.001266     0.00\n",
      "NOTE:      2   400 0.000328           0.9129     0.8595  1.513e+04       2474          0   0.001266     0.00\n",
      "NOTE:      3   400 0.000328           0.9038     0.8271  1.325e+04       2769          0   0.001266     0.00\n",
      "NOTE:      4   400 0.000328           0.7471     0.8683  1.148e+04       1742          0   0.001266     0.00\n",
      "NOTE:      5   400 0.000328           0.9119     0.8711  1.398e+04       2069          0   0.001266     0.00\n",
      "NOTE:      6   400 0.000328           0.8908     0.8894  1.417e+04       1761          0   0.001266     0.00\n",
      "NOTE:      7   400 0.000328           0.8881     0.8824  1.457e+04       1942          0   0.001266     0.00\n",
      "NOTE:      8   400 0.000328           0.6837     0.8745   1.23e+04       1764          0   0.001266     0.00\n",
      "NOTE:      9   400 0.000328           0.8797      0.878    1.4e+04       1945          0   0.001267     0.00\n",
      "NOTE:     10   400 0.000328           0.8888     0.8314  1.239e+04       2513          0   0.001267     0.00\n",
      "NOTE:     11   400 0.000328            0.984     0.8673  1.415e+04       2165          0   0.001267     0.00\n",
      "NOTE:     12   400 0.000328           0.9407     0.8972  1.534e+04       1758          0   0.001267     0.00\n",
      "NOTE:     13   400 0.000328           0.8925     0.8527  1.306e+04       2257          0   0.001267     0.00\n",
      "NOTE:     14   400 0.000328            0.826     0.8455  1.311e+04       2395          0   0.001267     0.00\n",
      "NOTE:     15   400 0.000328           0.8386     0.8828  1.405e+04       1866          0   0.001267     0.00\n",
      "NOTE:     16   400 0.000328           0.8222     0.8682  1.313e+04       1994          0   0.001267     0.00\n",
      "NOTE:     17   400 0.000328            0.836     0.8491  1.267e+04       2250          0   0.001267     0.00\n",
      "NOTE:     18   400 0.000328           0.9502      0.823  1.309e+04       2816          0   0.001267     0.00\n",
      "NOTE:     19   400 0.000328           0.8884     0.8454  1.377e+04       2519          0   0.001267     0.00\n",
      "NOTE:     20   400 0.000328           0.9922     0.8756  1.568e+04       2227          0   0.001267     0.00\n",
      "NOTE:     21   400 0.000328           0.9529     0.8513  1.414e+04       2469          0   0.001267     0.00\n",
      "NOTE:     22   400 0.000328           0.7899      0.877  1.308e+04       1835          0   0.001268     0.00\n",
      "NOTE:     23   400 0.000328           0.9411     0.8219  1.287e+04       2789          0   0.001268     0.00\n",
      "NOTE:     24   400 0.000328            1.023     0.8184  1.393e+04       3090          0   0.001268     0.00\n",
      "NOTE:     25   400 0.000328           0.8988      0.856  1.492e+04       2509          0   0.001268     0.00\n",
      "NOTE:     26   400 0.000328           0.8692     0.8565  1.494e+04       2503          0   0.001268     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  74       0.0003            0.89     0.8571  3.709e+05  6.183e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262           0.8659     0.8628  1.325e+04       2107          0   0.001268     0.00\n",
      "NOTE:      1   400 0.000262           0.8168      0.855  1.238e+04       2099          0   0.001268     0.00\n",
      "NOTE:      2   400 0.000262           0.8376     0.8706  1.254e+04       1863          0   0.001268     0.00\n",
      "NOTE:      3   400 0.000262           0.9908     0.8837  1.665e+04       2192          0   0.001268     0.00\n",
      "NOTE:      4   400 0.000262           0.8675     0.8599  1.513e+04       2465          0   0.001268     0.00\n",
      "NOTE:      5   400 0.000262           0.9439      0.841  1.458e+04       2756          0   0.001268     0.00\n",
      "NOTE:      6   400 0.000262           0.8819     0.9244  1.617e+04       1322          0   0.001268     0.00\n",
      "NOTE:      7   400 0.000262           0.8772     0.8761  1.508e+04       2133          0   0.001268     0.00\n",
      "NOTE:      8   400 0.000262           0.8823     0.8559  1.291e+04       2174          0   0.001268     0.00\n",
      "NOTE:      9   400 0.000262           0.8141     0.9018  1.325e+04       1443          0   0.001268     0.00\n",
      "NOTE:     10   400 0.000262            1.038     0.8342  1.519e+04       3018          0   0.001268     0.00\n",
      "NOTE:     11   400 0.000262           0.8516     0.8355  1.341e+04       2639          0   0.001269     0.00\n",
      "NOTE:     12   400 0.000262           0.7811     0.8612  1.299e+04       2093          0   0.001269     0.00\n",
      "NOTE:     13   400 0.000262           0.8205     0.8079  1.079e+04       2565          0   0.001269     0.00\n",
      "NOTE:     14   400 0.000262           0.7522     0.8428   1.26e+04       2350          0   0.001269     0.00\n",
      "NOTE:     15   400 0.000262            0.832     0.8531  1.167e+04       2010          0   0.001269     0.00\n",
      "NOTE:     16   400 0.000262           0.9623     0.8589  1.532e+04       2518          0   0.001269     0.00\n",
      "NOTE:     17   400 0.000262            0.929     0.8747  1.518e+04       2173          0   0.001269     0.00\n",
      "NOTE:     18   400 0.000262            0.921     0.8091  1.283e+04       3028          0   0.001269     0.00\n",
      "NOTE:     19   400 0.000262           0.8214     0.8688   1.29e+04       1948          0   0.001269     0.00\n",
      "NOTE:     20   400 0.000262           0.8657     0.8498  1.235e+04       2183          0   0.001269     0.00\n",
      "NOTE:     21   400 0.000262            1.007     0.8494  1.493e+04       2648          0   0.001269     0.00\n",
      "NOTE:     22   400 0.000262           0.8976     0.8708  1.468e+04       2178          0   0.001269     0.00\n",
      "NOTE:     23   400 0.000262           0.8732     0.8555   1.27e+04       2146          0   0.001269     0.00\n",
      "NOTE:     24   400 0.000262           0.8328     0.8888  1.405e+04       1757          0   0.001269     0.00\n",
      "NOTE:     25   400 0.000262           0.7882     0.8597  1.218e+04       1988          0   0.001269     0.00\n",
      "NOTE:     26   400 0.000262           0.9354     0.8605  1.235e+04       2001          0   0.001269     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  75       0.0003          0.8773     0.8602   3.68e+05   5.98e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262           0.7943     0.8727  1.296e+04       1891          0   0.001269     0.00\n",
      "NOTE:      1   400 0.000262           0.7078     0.8196  1.005e+04       2211          0   0.001269     0.00\n",
      "NOTE:      2   400 0.000262           0.8085     0.8628  1.287e+04       2045          0   0.001269     0.00\n",
      "NOTE:      3   400 0.000262           0.9305     0.8804  1.627e+04       2211          0   0.001269     0.00\n",
      "NOTE:      4   400 0.000262           0.8198     0.8027  1.122e+04       2757          0   0.001269     0.00\n",
      "NOTE:      5   400 0.000262           0.8652     0.8515  1.333e+04       2325          0   0.001269     0.00\n",
      "NOTE:      6   400 0.000262            0.807     0.8237  1.163e+04       2490          0   0.001269     0.00\n",
      "NOTE:      7   400 0.000262           0.8687     0.8493  1.201e+04       2130          0    0.00127     0.00\n",
      "NOTE:      8   400 0.000262           0.8225     0.8592  1.284e+04       2104          0   0.001269     0.00\n",
      "NOTE:      9   400 0.000262           0.8925     0.8223  1.348e+04       2914          0   0.001269     0.00\n",
      "NOTE:     10   400 0.000262            1.113     0.8506  1.743e+04       3061          0   0.001269     0.00\n",
      "NOTE:     11   400 0.000262           0.8649     0.8094  1.134e+04       2670          0   0.001269     0.00\n",
      "NOTE:     12   400 0.000262           0.9445     0.8635  1.487e+04       2351          0   0.001269     0.00\n",
      "NOTE:     13   400 0.000262           0.9422     0.8485  1.494e+04       2668          0   0.001269     0.00\n",
      "NOTE:     14   400 0.000262           0.8296     0.8698  1.244e+04       1863          0   0.001269     0.00\n",
      "NOTE:     15   400 0.000262           0.9507     0.8419  1.274e+04       2392          0   0.001269     0.00\n",
      "NOTE:     16   400 0.000262           0.7985     0.8512  1.251e+04       2187          0   0.001269     0.00\n",
      "NOTE:     17   400 0.000262            1.086     0.8325  1.624e+04       3267          0   0.001269     0.00\n",
      "NOTE:     18   400 0.000262           0.8652     0.8639  1.428e+04       2250          0   0.001269     0.00\n",
      "NOTE:     19   400 0.000262           0.8413      0.875  1.474e+04       2105          0   0.001269     0.00\n",
      "NOTE:     20   400 0.000262           0.9487     0.8705  1.561e+04       2321          0   0.001269     0.00\n",
      "NOTE:     21   400 0.000262           0.8163     0.8422  1.107e+04       2074          0   0.001269     0.00\n",
      "NOTE:     22   400 0.000262           0.8664     0.7872  1.191e+04       3221          0   0.001269     0.00\n",
      "NOTE:     23   400 0.000262           0.9028     0.8894  1.486e+04       1848          0   0.001269     0.00\n",
      "NOTE:     24   400 0.000262           0.8056     0.8493  1.246e+04       2211          0   0.001269     0.00\n",
      "NOTE:     25   400 0.000262           0.9699     0.8357  1.427e+04       2807          0   0.001269     0.00\n",
      "NOTE:     26   400 0.000262           0.8349     0.8459  1.253e+04       2282          0   0.001269     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  76       0.0003          0.8777     0.8481  3.609e+05  6.466e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262            1.032     0.8768  1.531e+04       2152          0   0.001269     0.00\n",
      "NOTE:      1   400 0.000262           0.9098     0.8845  1.462e+04       1910          0   0.001269     0.00\n",
      "NOTE:      2   400 0.000262           0.9077     0.7974  1.132e+04       2878          0   0.001269     0.00\n",
      "NOTE:      3   400 0.000262           0.8349      0.866  1.359e+04       2103          0   0.001269     0.00\n",
      "NOTE:      4   400 0.000262           0.8409     0.8767  1.477e+04       2077          0    0.00127     0.00\n",
      "NOTE:      5   400 0.000262           0.9158     0.8394  1.298e+04       2485          0    0.00127     0.00\n",
      "NOTE:      6   400 0.000262           0.8735     0.8686  1.322e+04       2000          0    0.00127     0.00\n",
      "NOTE:      7   400 0.000262           0.8574     0.8452  1.278e+04       2341          0    0.00127     0.00\n",
      "NOTE:      8   400 0.000262           0.9181     0.8444  1.328e+04       2448          0    0.00127     0.00\n",
      "NOTE:      9   400 0.000262           0.8791     0.8858  1.442e+04       1859          0    0.00127     0.00\n",
      "NOTE:     10   400 0.000262           0.7612     0.8729  1.229e+04       1789          0    0.00127     0.00\n",
      "NOTE:     11   400 0.000262           0.9947     0.8519  1.446e+04       2514          0    0.00127     0.00\n",
      "NOTE:     12   400 0.000262            1.051     0.8402  1.511e+04       2874          0    0.00127     0.00\n",
      "NOTE:     13   400 0.000262           0.7284     0.8528  1.065e+04       1839          0    0.00127     0.00\n",
      "NOTE:     14   400 0.000262           0.9114     0.8207  1.361e+04       2973          0    0.00127     0.00\n",
      "NOTE:     15   400 0.000262           0.8156       0.87    1.3e+04       1942          0    0.00127     0.00\n",
      "NOTE:     16   400 0.000262           0.8067     0.8634  1.275e+04       2017          0    0.00127     0.00\n",
      "NOTE:     17   400 0.000262           0.7883     0.8786   1.45e+04       2004          0    0.00127     0.00\n",
      "NOTE:     18   400 0.000262           0.9522     0.8598  1.411e+04       2301          0    0.00127     0.00\n",
      "NOTE:     19   400 0.000262           0.9115     0.8774  1.437e+04       2008          0    0.00127     0.00\n",
      "NOTE:     20   400 0.000262           0.8534     0.8835  1.504e+04       1984          0    0.00127     0.00\n",
      "NOTE:     21   400 0.000262           0.8723     0.8239  1.393e+04       2976          0    0.00127     0.00\n",
      "NOTE:     22   400 0.000262           0.8053     0.8994  1.343e+04       1501          0    0.00127     0.00\n",
      "NOTE:     23   400 0.000262           0.9227     0.8557  1.357e+04       2289          0    0.00127     0.00\n",
      "NOTE:     24   400 0.000262           0.8577     0.8658  1.248e+04       1934          0    0.00127     0.00\n",
      "NOTE:     25   400 0.000262            0.802     0.8536  1.353e+04       2321          0    0.00127     0.00\n",
      "NOTE:     26   400 0.000262           0.8924     0.8433  1.289e+04       2395          0    0.00127     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  77       0.0003          0.8776     0.8593   3.66e+05  5.991e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262           0.8487     0.8009  1.158e+04       2879          0    0.00127     0.00\n",
      "NOTE:      1   400 0.000262           0.8331     0.8611  1.348e+04       2174          0    0.00127     0.00\n",
      "NOTE:      2   400 0.000262            0.938     0.8238  1.288e+04       2754          0    0.00127     0.00\n",
      "NOTE:      3   400 0.000262           0.8938     0.8795  1.558e+04       2135          0    0.00127     0.00\n",
      "NOTE:      4   400 0.000262           0.9268     0.8806  1.561e+04       2116          0    0.00127     0.00\n",
      "NOTE:      5   400 0.000262           0.8465     0.8583  1.309e+04       2161          0    0.00127     0.00\n",
      "NOTE:      6   400 0.000262           0.8722     0.8294   1.33e+04       2737          0    0.00127     0.00\n",
      "NOTE:      7   400 0.000262            0.885     0.8569  1.412e+04       2357          0    0.00127     0.00\n",
      "NOTE:      8   400 0.000262            1.035     0.8362  1.567e+04       3068          0    0.00127     0.00\n",
      "NOTE:      9   400 0.000262           0.9994     0.8828  1.557e+04       2067          0    0.00127     0.00\n",
      "NOTE:     10   400 0.000262           0.8687     0.8565  1.282e+04       2148          0    0.00127     0.00\n",
      "NOTE:     11   400 0.000262            0.788     0.8187  1.163e+04       2574          0    0.00127     0.00\n",
      "NOTE:     12   400 0.000262           0.8687     0.8685  1.352e+04       2046          0    0.00127     0.00\n",
      "NOTE:     13   400 0.000262           0.9131     0.9006  1.632e+04       1800          0    0.00127     0.00\n",
      "NOTE:     14   400 0.000262           0.9164     0.8097  1.169e+04       2748          0    0.00127     0.00\n",
      "NOTE:     15   400 0.000262           0.7968     0.8677  1.289e+04       1965          0    0.00127     0.00\n",
      "NOTE:     16   400 0.000262            0.773     0.8421  1.127e+04       2112          0    0.00127     0.00\n",
      "NOTE:     17   400 0.000262           0.9084     0.8197  1.341e+04       2949          0    0.00127     0.00\n",
      "NOTE:     18   400 0.000262           0.7978     0.8948  1.294e+04       1521          0    0.00127     0.00\n",
      "NOTE:     19   400 0.000262            1.105     0.8317  1.741e+04       3522          0    0.00127     0.00\n",
      "NOTE:     20   400 0.000262           0.7775      0.841  1.161e+04       2194          0    0.00127     0.00\n",
      "NOTE:     21   400 0.000262           0.9674      0.855   1.47e+04       2492          0    0.00127     0.00\n",
      "NOTE:     22   400 0.000262           0.7513     0.8627  1.253e+04       1995          0    0.00127     0.00\n",
      "NOTE:     23   400 0.000262           0.8004     0.9035  1.367e+04       1460          0    0.00127     0.00\n",
      "NOTE:     24   400 0.000262           0.8265     0.8861  1.456e+04       1872          0   0.001271     0.00\n",
      "NOTE:     25   400 0.000262           0.8793      0.909  1.516e+04       1518          0   0.001271     0.00\n",
      "NOTE:     26   400 0.000262           0.9803     0.8746  1.613e+04       2313          0   0.001271     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  78       0.0003          0.8814     0.8581  3.731e+05  6.168e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262            1.003     0.8503  1.449e+04       2551          0   0.001271     0.00\n",
      "NOTE:      1   400 0.000262           0.9023     0.9014  1.484e+04       1624          0   0.001271     0.00\n",
      "NOTE:      2   400 0.000262           0.8256     0.8351   1.26e+04       2488          0   0.001271     0.00\n",
      "NOTE:      3   400 0.000262           0.9872     0.8783  1.611e+04       2232          0   0.001271     0.00\n",
      "NOTE:      4   400 0.000262            1.024     0.8687  1.561e+04       2358          0   0.001271     0.00\n",
      "NOTE:      5   400 0.000262           0.7709      0.892   1.29e+04       1561          0   0.001271     0.00\n",
      "NOTE:      6   400 0.000262           0.8756     0.8394   1.27e+04       2430          0   0.001271     0.00\n",
      "NOTE:      7   400 0.000262           0.9684     0.8429  1.461e+04       2724          0   0.001271     0.00\n",
      "NOTE:      8   400 0.000262           0.9583     0.8591  1.304e+04       2138          0   0.001271     0.00\n",
      "NOTE:      9   400 0.000262           0.8561      0.851  1.399e+04       2450          0   0.001271     0.00\n",
      "NOTE:     10   400 0.000262            1.191      0.864  1.888e+04       2971          0   0.001272     0.00\n",
      "NOTE:     11   400 0.000262           0.9908     0.8399  1.444e+04       2751          0   0.001272     0.00\n",
      "NOTE:     12   400 0.000262           0.7716     0.8986  1.322e+04       1492          0   0.001272     0.00\n",
      "NOTE:     13   400 0.000262           0.8899     0.9058  1.544e+04       1606          0   0.001272     0.00\n",
      "NOTE:     14   400 0.000262           0.8924     0.8816  1.431e+04       1922          0   0.001272     0.00\n",
      "NOTE:     15   400 0.000262           0.8766     0.8408  1.326e+04       2511          0   0.001272     0.00\n",
      "NOTE:     16   400 0.000262           0.9054     0.8443  1.348e+04       2485          0   0.001272     0.00\n",
      "NOTE:     17   400 0.000262           0.8705     0.8979  1.448e+04       1646          0   0.001272     0.00\n",
      "NOTE:     18   400 0.000262           0.9021     0.8511  1.399e+04       2447          0   0.001272     0.00\n",
      "NOTE:     19   400 0.000262           0.8821     0.8158  1.242e+04       2805          0   0.001272     0.00\n",
      "NOTE:     20   400 0.000262           0.7675     0.8693  1.268e+04       1906          0   0.001272     0.00\n",
      "NOTE:     21   400 0.000262           0.8907     0.8161  1.233e+04       2778          0   0.001272     0.00\n",
      "NOTE:     22   400 0.000262             1.03     0.8466  1.593e+04       2886          0   0.001272     0.00\n",
      "NOTE:     23   400 0.000262           0.8642      0.857   1.39e+04       2319          0   0.001273     0.00\n",
      "NOTE:     24   400 0.000262            0.863     0.8727  1.361e+04       1985          0   0.001273     0.00\n",
      "NOTE:     25   400 0.000262           0.8162     0.8708  1.321e+04       1960          0   0.001273     0.00\n",
      "NOTE:     26   400 0.000262           0.9802     0.8462  1.546e+04       2809          0   0.001273     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  79       0.0003          0.9094     0.8607  3.819e+05  6.184e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262           0.7749     0.8849   1.27e+04       1652          0   0.001273     0.00\n",
      "NOTE:      1   400 0.000262           0.9715       0.87  1.489e+04       2225          0   0.001273     0.00\n",
      "NOTE:      2   400 0.000262           0.7691     0.8719  1.183e+04       1739          0   0.001273     0.00\n",
      "NOTE:      3   400 0.000262           0.7843      0.832  1.112e+04       2245          0   0.001273     0.00\n",
      "NOTE:      4   400 0.000262           0.9608     0.8587  1.304e+04       2146          0   0.001273     0.00\n",
      "NOTE:      5   400 0.000262           0.8601     0.9132  1.499e+04       1425          0   0.001273     0.00\n",
      "NOTE:      6   400 0.000262           0.8828     0.8524  1.251e+04       2166          0   0.001273     0.00\n",
      "NOTE:      7   400 0.000262           0.7407     0.8899  1.347e+04       1666          0   0.001273     0.00\n",
      "NOTE:      8   400 0.000262           0.9006     0.8833  1.409e+04       1862          0   0.001273     0.00\n",
      "NOTE:      9   400 0.000262            0.872     0.8689  1.411e+04       2129          0   0.001273     0.00\n",
      "NOTE:     10   400 0.000262           0.7242     0.8448  1.205e+04       2215          0   0.001273     0.00\n",
      "NOTE:     11   400 0.000262           0.8903     0.8347  1.379e+04       2730          0   0.001273     0.00\n",
      "NOTE:     12   400 0.000262            1.073     0.8862  1.679e+04       2157          0   0.001273     0.00\n",
      "NOTE:     13   400 0.000262           0.8915     0.8256  1.205e+04       2545          0   0.001273     0.00\n",
      "NOTE:     14   400 0.000262           0.7552     0.8772  1.186e+04       1660          0   0.001273     0.00\n",
      "NOTE:     15   400 0.000262           0.7753     0.8388  1.176e+04       2259          0   0.001273     0.00\n",
      "NOTE:     16   400 0.000262           0.8839     0.8684  1.348e+04       2042          0   0.001273     0.00\n",
      "NOTE:     17   400 0.000262           0.8581     0.8492  1.361e+04       2418          0   0.001273     0.00\n",
      "NOTE:     18   400 0.000262           0.8451     0.8148  1.093e+04       2484          0   0.001274     0.00\n",
      "NOTE:     19   400 0.000262           0.7276     0.8832  1.364e+04       1804          0   0.001274     0.00\n",
      "NOTE:     20   400 0.000262           0.8342     0.8562  1.291e+04       2168          0   0.001274     0.00\n",
      "NOTE:     21   400 0.000262            1.026     0.8635  1.605e+04       2537          0   0.001274     0.00\n",
      "NOTE:     22   400 0.000262           0.9814     0.8578   1.55e+04       2571          0   0.001274     0.00\n",
      "NOTE:     23   400 0.000262           0.8879     0.8467  1.453e+04       2632          0   0.001274     0.00\n",
      "NOTE:     24   400 0.000262           0.7962     0.8989  1.374e+04       1546          0   0.001274     0.00\n",
      "NOTE:     25   400 0.000262           0.6876     0.8669  1.171e+04       1797          0   0.001274     0.00\n",
      "NOTE:     26   400 0.000262           0.9119      0.884  1.477e+04       1937          0   0.001274     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  80       0.0003          0.8543     0.8644  3.619e+05  5.676e+04          0     0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262           0.9439     0.8778  1.429e+04       1989          0   0.001274     0.00\n",
      "NOTE:      1   400 0.000262           0.8931     0.8442  1.317e+04       2431          0   0.001274     0.00\n",
      "NOTE:      2   400 0.000262           0.8036     0.8417  1.188e+04       2234          0   0.001274     0.00\n",
      "NOTE:      3   400 0.000262           0.9846     0.8409  1.518e+04       2872          0   0.001274     0.00\n",
      "NOTE:      4   400 0.000262           0.8926     0.8468   1.36e+04       2461          0   0.001274     0.00\n",
      "NOTE:      5   400 0.000262           0.7913     0.8573  1.204e+04       2003          0   0.001274     0.00\n",
      "NOTE:      6   400 0.000262           0.7867     0.8482  1.141e+04       2041          0   0.001274     0.00\n",
      "NOTE:      7   400 0.000262           0.7844     0.8992  1.413e+04       1584          0   0.001274     0.00\n",
      "NOTE:      8   400 0.000262           0.8807     0.8103  1.279e+04       2994          0   0.001274     0.00\n",
      "NOTE:      9   400 0.000262           0.8778     0.8549   1.37e+04       2325          0   0.001274     0.00\n",
      "NOTE:     10   400 0.000262           0.9649     0.8222  1.459e+04       3157          0   0.001274     0.00\n",
      "NOTE:     11   400 0.000262           0.9457     0.8428  1.275e+04       2378          0   0.001274     0.00\n",
      "NOTE:     12   400 0.000262           0.9108     0.8487  1.346e+04       2398          0   0.001274     0.00\n",
      "NOTE:     13   400 0.000262           0.9508     0.8572  1.465e+04       2442          0   0.001274     0.00\n",
      "NOTE:     14   400 0.000262           0.8088     0.8587  1.226e+04       2017          0   0.001274     0.00\n",
      "NOTE:     15   400 0.000262           0.9254     0.8668  1.341e+04       2060          0   0.001274     0.00\n",
      "NOTE:     16   400 0.000262           0.9204      0.806   1.27e+04       3057          0   0.001274     0.00\n",
      "NOTE:     17   400 0.000262           0.8688     0.8879  1.497e+04       1890          0   0.001274     0.00\n",
      "NOTE:     18   400 0.000262            1.007     0.7999  1.362e+04       3407          0   0.001274     0.00\n",
      "NOTE:     19   400 0.000262            1.031     0.8709  1.677e+04       2487          0   0.001274     0.00\n",
      "NOTE:     20   400 0.000262           0.8537     0.8597  1.333e+04       2176          0   0.001274     0.00\n",
      "NOTE:     21   400 0.000262           0.9745     0.8578   1.45e+04       2403          0   0.001274     0.00\n",
      "NOTE:     22   400 0.000262           0.9978      0.827  1.469e+04       3072          0   0.001274     0.00\n",
      "NOTE:     23   400 0.000262           0.9298     0.8377  1.237e+04       2396          0   0.001274     0.00\n",
      "NOTE:     24   400 0.000262           0.8348     0.8716  1.196e+04       1762          0   0.001274     0.00\n",
      "NOTE:     25   400 0.000262           0.9277     0.8293  1.393e+04       2868          0   0.001274     0.00\n",
      "NOTE:     26   400 0.000262           0.7934     0.8586  1.218e+04       2005          0   0.001274     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  81       0.0003          0.8994     0.8488  3.643e+05  6.491e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262           0.8019     0.8241  1.173e+04       2503          0   0.001274     0.00\n",
      "NOTE:      1   400 0.000262            0.906     0.8427   1.35e+04       2521          0   0.001274     0.00\n",
      "NOTE:      2   400 0.000262           0.8505     0.8878  1.436e+04       1815          0   0.001274     0.00\n",
      "NOTE:      3   400 0.000262           0.9241     0.8553  1.407e+04       2380          0   0.001274     0.00\n",
      "NOTE:      4   400 0.000262           0.7425     0.8492  1.159e+04       2058          0   0.001274     0.00\n",
      "NOTE:      5   400 0.000262            0.965     0.8717  1.504e+04       2214          0   0.001274     0.00\n",
      "NOTE:      6   400 0.000262           0.9827     0.8556  1.467e+04       2475          0   0.001274     0.00\n",
      "NOTE:      7   400 0.000262           0.9793     0.8503  1.469e+04       2586          0   0.001274     0.00\n",
      "NOTE:      8   400 0.000262           0.9717     0.8688  1.518e+04       2291          0   0.001274     0.00\n",
      "NOTE:      9   400 0.000262           0.7723     0.8894  1.235e+04       1537          0   0.001274     0.00\n",
      "NOTE:     10   400 0.000262           0.8891     0.8278  1.265e+04       2631          0   0.001274     0.00\n",
      "NOTE:     11   400 0.000262             0.85       0.81  1.134e+04       2661          0   0.001274     0.00\n",
      "NOTE:     12   400 0.000262           0.8744     0.8657  1.504e+04       2334          0   0.001274     0.00\n",
      "NOTE:     13   400 0.000262           0.9138     0.8357  1.421e+04       2794          0   0.001274     0.00\n",
      "NOTE:     14   400 0.000262           0.9383     0.8434  1.308e+04       2427          0   0.001274     0.00\n",
      "NOTE:     15   400 0.000262           0.7649     0.8598  1.185e+04       1933          0   0.001274     0.00\n",
      "NOTE:     16   400 0.000262           0.7376     0.8787  1.186e+04       1637          0   0.001274     0.00\n",
      "NOTE:     17   400 0.000262           0.8467     0.8437  1.211e+04       2243          0   0.001274     0.00\n",
      "NOTE:     18   400 0.000262           0.8578     0.8645  1.371e+04       2149          0   0.001274     0.00\n",
      "NOTE:     19   400 0.000262           0.7917     0.8728  1.271e+04       1851          0   0.001274     0.00\n",
      "NOTE:     20   400 0.000262           0.7437     0.8729  1.193e+04       1737          0   0.001274     0.00\n",
      "NOTE:     21   400 0.000262           0.9972     0.8781  1.634e+04       2267          0   0.001274     0.00\n",
      "NOTE:     22   400 0.000262           0.9079     0.8468  1.358e+04       2456          0   0.001274     0.00\n",
      "NOTE:     23   400 0.000262            1.005     0.8934  1.608e+04       1919          0   0.001274     0.00\n",
      "NOTE:     24   400 0.000262           0.7794     0.8526  1.361e+04       2353          0   0.001274     0.00\n",
      "NOTE:     25   400 0.000262             0.93     0.8773  1.453e+04       2033          0   0.001274     0.00\n",
      "NOTE:     26   400 0.000262           0.8588     0.8471  1.294e+04       2337          0   0.001274     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  82       0.0003          0.8734     0.8585  3.647e+05  6.014e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262           0.8345     0.8897  1.411e+04       1748          0   0.001274     0.00\n",
      "NOTE:      1   400 0.000262           0.9403     0.8624  1.377e+04       2196          0   0.001274     0.00\n",
      "NOTE:      2   400 0.000262           0.9398     0.8575  1.369e+04       2274          0   0.001274     0.00\n",
      "NOTE:      3   400 0.000262           0.9467     0.8809  1.593e+04       2153          0   0.001274     0.00\n",
      "NOTE:      4   400 0.000262           0.8633     0.8572  1.307e+04       2177          0   0.001274     0.00\n",
      "NOTE:      5   400 0.000262           0.8957     0.8469  1.416e+04       2560          0   0.001274     0.00\n",
      "NOTE:      6   400 0.000262           0.8868     0.8809  1.347e+04       1822          0   0.001274     0.00\n",
      "NOTE:      7   400 0.000262           0.8459     0.8817  1.238e+04       1661          0   0.001274     0.00\n",
      "NOTE:      8   400 0.000262           0.8923      0.872  1.564e+04       2296          0   0.001274     0.00\n",
      "NOTE:      9   400 0.000262           0.8034     0.8363  1.235e+04       2416          0   0.001274     0.00\n",
      "NOTE:     10   400 0.000262           0.9983     0.8551    1.4e+04       2371          0   0.001274     0.00\n",
      "NOTE:     11   400 0.000262            0.896       0.86   1.54e+04       2506          0   0.001274     0.00\n",
      "NOTE:     12   400 0.000262           0.8288     0.8463  1.284e+04       2332          0   0.001274     0.00\n",
      "NOTE:     13   400 0.000262            0.866     0.8673  1.364e+04       2087          0   0.001274     0.00\n",
      "NOTE:     14   400 0.000262            0.953     0.8678  1.491e+04       2272          0   0.001274     0.00\n",
      "NOTE:     15   400 0.000262           0.8983     0.8948  1.459e+04       1716          0   0.001275     0.00\n",
      "NOTE:     16   400 0.000262           0.9957     0.8332  1.483e+04       2968          0   0.001275     0.00\n",
      "NOTE:     17   400 0.000262           0.7938     0.8995  1.339e+04       1496          0   0.001275     0.00\n",
      "NOTE:     18   400 0.000262           0.8671      0.845  1.325e+04       2429          0   0.001275     0.00\n",
      "NOTE:     19   400 0.000262           0.9149     0.8938  1.542e+04       1832          0   0.001275     0.00\n",
      "NOTE:     20   400 0.000262           0.8731     0.8275  1.262e+04       2631          0   0.001275     0.00\n",
      "NOTE:     21   400 0.000262           0.8874     0.8756  1.481e+04       2104          0   0.001275     0.00\n",
      "NOTE:     22   400 0.000262            1.031     0.8693  1.711e+04       2573          0   0.001275     0.00\n",
      "NOTE:     23   400 0.000262            1.011     0.8667  1.614e+04       2483          0   0.001275     0.00\n",
      "NOTE:     24   400 0.000262           0.9192     0.8737  1.422e+04       2056          0   0.001275     0.00\n",
      "NOTE:     25   400 0.000262           0.8308     0.8913  1.393e+04       1700          0   0.001275     0.00\n",
      "NOTE:     26   400 0.000262           0.6133     0.9108  1.104e+04       1082          0   0.001275     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  83       0.0003          0.8899     0.8679  3.807e+05  5.794e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262           0.9005     0.8616  1.293e+04       2077          0   0.001275     0.00\n",
      "NOTE:      1   400 0.000262           0.9201     0.8538  1.466e+04       2510          0   0.001275     0.00\n",
      "NOTE:      2   400 0.000262           0.9507     0.8726  1.466e+04       2140          0   0.001275     0.00\n",
      "NOTE:      3   400 0.000262           0.6852      0.912  1.219e+04       1177          0   0.001275     0.00\n",
      "NOTE:      4   400 0.000262            1.006      0.831  1.427e+04       2902          0   0.001275     0.00\n",
      "NOTE:      5   400 0.000262           0.7903      0.865   1.21e+04       1889          0   0.001276     0.00\n",
      "NOTE:      6   400 0.000262           0.8535     0.8425  1.237e+04       2311          0   0.001276     0.00\n",
      "NOTE:      7   400 0.000262            0.867     0.8611  1.364e+04       2199          0   0.001276     0.00\n",
      "NOTE:      8   400 0.000262           0.7828     0.8742  1.289e+04       1854          0   0.001276     0.00\n",
      "NOTE:      9   400 0.000262           0.8342     0.8816  1.474e+04       1979          0   0.001276     0.00\n",
      "NOTE:     10   400 0.000262            1.035     0.8304  1.512e+04       3089          0   0.001276     0.00\n",
      "NOTE:     11   400 0.000262           0.8799     0.8785  1.284e+04       1776          0   0.001276     0.00\n",
      "NOTE:     12   400 0.000262           0.8796      0.899  1.463e+04       1643          0   0.001276     0.00\n",
      "NOTE:     13   400 0.000262           0.7428     0.8727   1.17e+04       1706          0   0.001276     0.00\n",
      "NOTE:     14   400 0.000262           0.8415     0.8347  1.259e+04       2493          0   0.001276     0.00\n",
      "NOTE:     15   400 0.000262           0.9145     0.8538  1.483e+04       2539          0   0.001276     0.00\n",
      "NOTE:     16   400 0.000262            1.042     0.8668  1.715e+04       2636          0   0.001276     0.00\n",
      "NOTE:     17   400 0.000262           0.8137     0.8591  1.208e+04       1981          0   0.001276     0.00\n",
      "NOTE:     18   400 0.000262            0.957     0.8667    1.6e+04       2461          0   0.001276     0.00\n",
      "NOTE:     19   400 0.000262           0.8825     0.8959  1.558e+04       1811          0   0.001277     0.00\n",
      "NOTE:     20   400 0.000262           0.8743     0.8578  1.373e+04       2277          0   0.001277     0.00\n",
      "NOTE:     21   400 0.000262           0.9054     0.8391  1.461e+04       2801          0   0.001277     0.00\n",
      "NOTE:     22   400 0.000262           0.7289     0.8351  1.143e+04       2256          0   0.001277     0.00\n",
      "NOTE:     23   400 0.000262           0.9221     0.9102  1.538e+04       1516          0   0.001277     0.00\n",
      "NOTE:     24   400 0.000262           0.6948     0.8155  1.013e+04       2291          0   0.001277     0.00\n",
      "NOTE:     25   400 0.000262           0.9476     0.8415  1.386e+04       2610          0   0.001277     0.00\n",
      "NOTE:     26   400 0.000262           0.9405     0.8621  1.575e+04       2519          0   0.001277     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  84       0.0003          0.8738     0.8622  3.718e+05  5.944e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262           0.9328     0.9065  1.583e+04       1634          0   0.001277     0.00\n",
      "NOTE:      1   400 0.000262            0.969     0.8907   1.56e+04       1914          0   0.001277     0.00\n",
      "NOTE:      2   400 0.000262           0.8128     0.8538  1.188e+04       2035          0   0.001277     0.00\n",
      "NOTE:      3   400 0.000262           0.8896     0.8411  1.386e+04       2618          0   0.001277     0.00\n",
      "NOTE:      4   400 0.000262           0.9275     0.8368  1.343e+04       2619          0   0.001277     0.00\n",
      "NOTE:      5   400 0.000262           0.9656     0.8624  1.487e+04       2372          0   0.001277     0.00\n",
      "NOTE:      6   400 0.000262            1.021     0.8649  1.624e+04       2537          0   0.001277     0.00\n",
      "NOTE:      7   400 0.000262            0.912     0.8304  1.265e+04       2583          0   0.001277     0.00\n",
      "NOTE:      8   400 0.000262            1.011     0.8574   1.58e+04       2629          0   0.001277     0.00\n",
      "NOTE:      9   400 0.000262           0.8581     0.8818   1.41e+04       1889          0   0.001277     0.00\n",
      "NOTE:     10   400 0.000262           0.9609     0.8602  1.539e+04       2501          0   0.001277     0.00\n",
      "NOTE:     11   400 0.000262           0.8584     0.8109  1.131e+04       2638          0   0.001277     0.00\n",
      "NOTE:     12   400 0.000262           0.9642     0.8618  1.516e+04       2431          0   0.001277     0.00\n",
      "NOTE:     13   400 0.000262           0.8527     0.8481  1.368e+04       2451          0   0.001277     0.00\n",
      "NOTE:     14   400 0.000262           0.8494     0.8597  1.354e+04       2209          0   0.001277     0.00\n",
      "NOTE:     15   400 0.000262           0.9382     0.8395  1.459e+04       2790          0   0.001277     0.00\n",
      "NOTE:     16   400 0.000262            1.005     0.8631  1.524e+04       2417          0   0.001278     0.00\n",
      "NOTE:     17   400 0.000262           0.8032     0.8866   1.17e+04       1496          0   0.001278     0.00\n",
      "NOTE:     18   400 0.000262           0.7325     0.8644  1.149e+04       1803          0   0.001277     0.00\n",
      "NOTE:     19   400 0.000262           0.9987     0.7811  1.277e+04       3579          0   0.001277     0.00\n",
      "NOTE:     20   400 0.000262           0.7975     0.8728  1.377e+04       2007          0   0.001277     0.00\n",
      "NOTE:     21   400 0.000262           0.9001      0.872  1.354e+04       1987          0   0.001277     0.00\n",
      "NOTE:     22   400 0.000262             1.07     0.8461  1.634e+04       2973          0   0.001277     0.00\n",
      "NOTE:     23   400 0.000262           0.9408     0.8159  1.263e+04       2848          0   0.001277     0.00\n",
      "NOTE:     24   400 0.000262            1.001     0.8788  1.628e+04       2244          0   0.001277     0.00\n",
      "NOTE:     25   400 0.000262           0.8813     0.8666  1.364e+04       2100          0   0.001277     0.00\n",
      "NOTE:     26   400 0.000262           0.8472     0.8581  1.246e+04       2060          0   0.001277     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  85       0.0003          0.9148     0.8564  3.778e+05  6.336e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000262           0.8601     0.8447  1.366e+04       2510          0   0.001277     0.00\n",
      "NOTE:      1   400 0.000262           0.8519     0.8663  1.291e+04       1992          0   0.001277     0.00\n",
      "NOTE:      2   400 0.000262           0.7725     0.8474  1.272e+04       2290          0   0.001277     0.00\n",
      "NOTE:      3   400 0.000262           0.8459     0.8496  1.295e+04       2292          0   0.001277     0.00\n",
      "NOTE:      4   400 0.000262           0.9002     0.8168  1.205e+04       2704          0   0.001277     0.00\n",
      "NOTE:      5   400 0.000262           0.8457     0.8811   1.43e+04       1930          0   0.001277     0.00\n",
      "NOTE:      6   400 0.000262           0.8604     0.8725  1.321e+04       1930          0   0.001277     0.00\n",
      "NOTE:      7   400 0.000262           0.7711     0.8584  1.127e+04       1859          0   0.001277     0.00\n",
      "NOTE:      8   400 0.000262           0.7821     0.8849  1.308e+04       1700          0   0.001277     0.00\n",
      "NOTE:      9   400 0.000262           0.8706     0.8334  1.307e+04       2612          0   0.001277     0.00\n",
      "NOTE:     10   400 0.000262           0.8821      0.886   1.42e+04       1827          0   0.001277     0.00\n",
      "NOTE:     11   400 0.000262           0.9687     0.8676   1.46e+04       2228          0   0.001277     0.00\n",
      "NOTE:     12   400 0.000262           0.8347     0.8511  1.159e+04       2028          0   0.001277     0.00\n",
      "NOTE:     13   400 0.000262           0.8337     0.8771  1.391e+04       1949          0   0.001277     0.00\n",
      "NOTE:     14   400 0.000262           0.8529     0.8709  1.369e+04       2028          0   0.001277     0.00\n",
      "NOTE:     15   400 0.000262           0.8995     0.8476  1.384e+04       2488          0   0.001277     0.00\n",
      "NOTE:     16   400 0.000262           0.9083     0.8823  1.472e+04       1963          0   0.001277     0.00\n",
      "NOTE:     17   400 0.000262            1.006     0.8472  1.591e+04       2869          0   0.001277     0.00\n",
      "NOTE:     18   400 0.000262           0.8844     0.8746  1.452e+04       2082          0   0.001277     0.00\n",
      "NOTE:     19   400 0.000262           0.9364     0.8318   1.31e+04       2649          0   0.001278     0.00\n",
      "NOTE:     20   400 0.000262           0.9426     0.8347  1.389e+04       2750          0   0.001278     0.00\n",
      "NOTE:     21   400 0.000262           0.8289     0.8665  1.363e+04       2100          0   0.001278     0.00\n",
      "NOTE:     22   400 0.000262           0.9007     0.8338  1.312e+04       2615          0   0.001278     0.00\n",
      "NOTE:     23   400 0.000262           0.9563     0.8459  1.388e+04       2528          0   0.001278     0.00\n",
      "NOTE:     24   400 0.000262           0.9575     0.8692  1.489e+04       2239          0   0.001278     0.00\n",
      "NOTE:     25   400 0.000262           0.9085     0.8906  1.563e+04       1921          0   0.001278     0.00\n",
      "NOTE:     26   400 0.000262            0.938     0.8691  1.472e+04       2216          0   0.001278     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  86       0.0003          0.8815     0.8595   3.69e+05   6.03e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00021           0.9028     0.8798  1.355e+04       1851          0   0.001278     0.00\n",
      "NOTE:      1   400  0.00021           0.9418     0.8137  1.334e+04       3053          0   0.001278     0.00\n",
      "NOTE:      2   400  0.00021           0.7915     0.8635  1.263e+04       1997          0   0.001278     0.00\n",
      "NOTE:      3   400  0.00021           0.7909      0.851  1.248e+04       2184          0   0.001278     0.00\n",
      "NOTE:      4   400  0.00021           0.9603      0.893  1.538e+04       1843          0   0.001278     0.00\n",
      "NOTE:      5   400  0.00021           0.6915      0.874  1.226e+04       1767          0   0.001278     0.00\n",
      "NOTE:      6   400  0.00021           0.9118     0.9033  1.507e+04       1613          0   0.001278     0.00\n",
      "NOTE:      7   400  0.00021           0.8045     0.8495  1.071e+04       1896          0   0.001278     0.00\n",
      "NOTE:      8   400  0.00021            1.024     0.8816  1.694e+04       2275          0   0.001278     0.00\n",
      "NOTE:      9   400  0.00021            0.861     0.8868  1.473e+04       1881          0   0.001278     0.00\n",
      "NOTE:     10   400  0.00021           0.8887     0.8821  1.483e+04       1982          0   0.001278     0.00\n",
      "NOTE:     11   400  0.00021           0.7452     0.8712   1.24e+04       1833          0   0.001278     0.00\n",
      "NOTE:     12   400  0.00021           0.8803     0.8169  1.244e+04       2789          0   0.001278     0.00\n",
      "NOTE:     13   400  0.00021           0.8573     0.8725  1.273e+04       1860          0   0.001278     0.00\n",
      "NOTE:     14   400  0.00021           0.7594     0.9164  1.435e+04       1309          0   0.001278     0.00\n",
      "NOTE:     15   400  0.00021           0.8999     0.8388  1.421e+04       2731          0   0.001278     0.00\n",
      "NOTE:     16   400  0.00021           0.8513     0.8702  1.385e+04       2067          0   0.001278     0.00\n",
      "NOTE:     17   400  0.00021           0.8819     0.8031  1.189e+04       2915          0   0.001278     0.00\n",
      "NOTE:     18   400  0.00021           0.7397       0.87    1.3e+04       1941          0   0.001279     0.00\n",
      "NOTE:     19   400  0.00021           0.7861     0.8514  1.224e+04       2136          0   0.001279     0.00\n",
      "NOTE:     20   400  0.00021            1.003     0.8427  1.558e+04       2908          0   0.001279     0.00\n",
      "NOTE:     21   400  0.00021           0.9319     0.8323  1.357e+04       2734          0   0.001279     0.00\n",
      "NOTE:     22   400  0.00021           0.8793     0.8914  1.555e+04       1895          0   0.001279     0.00\n",
      "NOTE:     23   400  0.00021           0.7921     0.8444   1.29e+04       2376          0   0.001279     0.00\n",
      "NOTE:     24   400  0.00021           0.7297     0.7972       9581       2437          0   0.001279     0.00\n",
      "NOTE:     25   400  0.00021           0.9193     0.9032  1.642e+04       1759          0   0.001279     0.00\n",
      "NOTE:     26   400  0.00021           0.9059     0.8976  1.424e+04       1624          0   0.001279     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  87       0.0002          0.8567     0.8642  3.669e+05  5.766e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00021           0.7929     0.8875  1.303e+04       1651          0   0.001279     0.00\n",
      "NOTE:      1   400  0.00021           0.8405      0.824  1.258e+04       2687          0   0.001279     0.00\n",
      "NOTE:      2   400  0.00021           0.8681     0.8713  1.268e+04       1873          0   0.001279     0.00\n",
      "NOTE:      3   400  0.00021           0.8807     0.8818  1.469e+04       1968          0   0.001279     0.00\n",
      "NOTE:      4   400  0.00021           0.9559     0.8965  1.612e+04       1861          0   0.001279     0.00\n",
      "NOTE:      5   400  0.00021           0.8737     0.8418   1.31e+04       2462          0   0.001279     0.00\n",
      "NOTE:      6   400  0.00021            0.851      0.863  1.341e+04       2129          0   0.001279     0.00\n",
      "NOTE:      7   400  0.00021           0.8876     0.8568  1.498e+04       2503          0    0.00128     0.00\n",
      "NOTE:      8   400  0.00021            1.033     0.8497  1.524e+04       2694          0    0.00128     0.00\n",
      "NOTE:      9   400  0.00021            1.001     0.8434  1.502e+04       2789          0    0.00128     0.00\n",
      "NOTE:     10   400  0.00021           0.9181     0.8842  1.641e+04       2148          0    0.00128     0.00\n",
      "NOTE:     11   400  0.00021           0.9963     0.8513  1.607e+04       2807          0    0.00128     0.00\n",
      "NOTE:     12   400  0.00021           0.8803     0.8607  1.396e+04       2259          0    0.00128     0.00\n",
      "NOTE:     13   400  0.00021           0.9218     0.8021   1.26e+04       3109          0    0.00128     0.00\n",
      "NOTE:     14   400  0.00021           0.8631     0.8664  1.417e+04       2184          0    0.00128     0.00\n",
      "NOTE:     15   400  0.00021            0.917     0.8788  1.438e+04       1984          0    0.00128     0.00\n",
      "NOTE:     16   400  0.00021             1.04      0.835  1.436e+04       2836          0    0.00128     0.00\n",
      "NOTE:     17   400  0.00021           0.9597     0.8181  1.445e+04       3212          0    0.00128     0.00\n",
      "NOTE:     18   400  0.00021           0.9275     0.8198  1.384e+04       3042          0    0.00128     0.00\n",
      "NOTE:     19   400  0.00021           0.8316     0.8966  1.496e+04       1724          0    0.00128     0.00\n",
      "NOTE:     20   400  0.00021           0.8436     0.8735  1.272e+04       1843          0    0.00128     0.00\n",
      "NOTE:     21   400  0.00021           0.9625      0.871  1.576e+04       2334          0    0.00128     0.00\n",
      "NOTE:     22   400  0.00021            1.044     0.8941  1.759e+04       2084          0    0.00128     0.00\n",
      "NOTE:     23   400  0.00021           0.8608     0.8492  1.337e+04       2375          0    0.00128     0.00\n",
      "NOTE:     24   400  0.00021           0.8493     0.8492  1.174e+04       2084          0    0.00128     0.00\n",
      "NOTE:     25   400  0.00021           0.7463     0.8305  1.066e+04       2174          0    0.00128     0.00\n",
      "NOTE:     26   400  0.00021           0.8433     0.8648   1.29e+04       2016          0    0.00128     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  88       0.0002          0.9033     0.8584  3.807e+05  6.283e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00021           0.9133     0.8242  1.421e+04       3031          0    0.00128     0.00\n",
      "NOTE:      1   400  0.00021             1.09     0.8594  1.662e+04       2719          0    0.00128     0.00\n",
      "NOTE:      2   400  0.00021            0.827     0.8319  1.175e+04       2375          0    0.00128     0.00\n",
      "NOTE:      3   400  0.00021           0.9348     0.8285  1.328e+04       2748          0    0.00128     0.00\n",
      "NOTE:      4   400  0.00021           0.9155     0.8367  1.298e+04       2534          0    0.00128     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      5   400  0.00021           0.9577     0.8464  1.476e+04       2677          0    0.00128     0.00\n",
      "NOTE:      6   400  0.00021           0.8436     0.8631   1.32e+04       2093          0    0.00128     0.00\n",
      "NOTE:      7   400  0.00021           0.8291     0.8833  1.402e+04       1853          0    0.00128     0.00\n",
      "NOTE:      8   400  0.00021           0.8875      0.867  1.358e+04       2084          0    0.00128     0.00\n",
      "NOTE:      9   400  0.00021            1.027     0.8826  1.575e+04       2095          0    0.00128     0.00\n",
      "NOTE:     10   400  0.00021           0.8129     0.8719  1.318e+04       1937          0    0.00128     0.00\n",
      "NOTE:     11   400  0.00021            1.042     0.8582  1.582e+04       2614          0    0.00128     0.00\n",
      "NOTE:     12   400  0.00021           0.8709     0.8349  1.332e+04       2634          0    0.00128     0.00\n",
      "NOTE:     13   400  0.00021            0.896     0.8374    1.3e+04       2524          0    0.00128     0.00\n",
      "NOTE:     14   400  0.00021            1.022     0.7961  1.351e+04       3459          0    0.00128     0.00\n",
      "NOTE:     15   400  0.00021            1.003     0.8581  1.627e+04       2689          0    0.00128     0.00\n",
      "NOTE:     16   400  0.00021           0.7746     0.8647  1.286e+04       2011          0    0.00128     0.00\n",
      "NOTE:     17   400  0.00021           0.8534     0.8418   1.32e+04       2482          0    0.00128     0.00\n",
      "NOTE:     18   400  0.00021           0.8788     0.8578  1.467e+04       2431          0    0.00128     0.00\n",
      "NOTE:     19   400  0.00021           0.7646     0.8541  1.162e+04       1985          0    0.00128     0.00\n",
      "NOTE:     20   400  0.00021           0.9101     0.8853  1.617e+04       2096          0    0.00128     0.00\n",
      "NOTE:     21   400  0.00021           0.8612     0.8589  1.272e+04       2089          0    0.00128     0.00\n",
      "NOTE:     22   400  0.00021            1.039     0.8877   1.81e+04       2290          0    0.00128     0.00\n",
      "NOTE:     23   400  0.00021           0.8423     0.7954  1.162e+04       2989          0    0.00128     0.00\n",
      "NOTE:     24   400  0.00021           0.7989     0.8944  1.322e+04       1561          0    0.00128     0.00\n",
      "NOTE:     25   400  0.00021           0.8963      0.867  1.394e+04       2138          0   0.001281     0.00\n",
      "NOTE:     26   400  0.00021           0.8326     0.8956  1.363e+04       1588          0   0.001281     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  89       0.0002          0.9009     0.8554   3.77e+05  6.373e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00021           0.9092     0.8653  1.394e+04       2171          0   0.001281     0.00\n",
      "NOTE:      1   400  0.00021           0.9391     0.8443  1.457e+04       2687          0   0.001281     0.00\n",
      "NOTE:      2   400  0.00021           0.9675     0.8259  1.403e+04       2958          0   0.001281     0.00\n",
      "NOTE:      3   400  0.00021           0.7549     0.9063  1.229e+04       1270          0   0.001281     0.00\n",
      "NOTE:      4   400  0.00021           0.8145     0.8752  1.299e+04       1852          0   0.001281     0.00\n",
      "NOTE:      5   400  0.00021           0.8796     0.8512  1.401e+04       2450          0   0.001281     0.00\n",
      "NOTE:      6   400  0.00021           0.8774     0.8135  1.221e+04       2798          0   0.001281     0.00\n",
      "NOTE:      7   400  0.00021           0.9129     0.8309  1.208e+04       2458          0   0.001281     0.00\n",
      "NOTE:      8   400  0.00021           0.9253     0.8212  1.306e+04       2843          0   0.001281     0.00\n",
      "NOTE:      9   400  0.00021           0.9393     0.8491  1.466e+04       2606          0   0.001281     0.00\n",
      "NOTE:     10   400  0.00021           0.8535     0.9128  1.456e+04       1391          0   0.001281     0.00\n",
      "NOTE:     11   400  0.00021           0.9359     0.8592  1.492e+04       2445          0   0.001281     0.00\n",
      "NOTE:     12   400  0.00021           0.8953     0.8796  1.457e+04       1995          0   0.001281     0.00\n",
      "NOTE:     13   400  0.00021           0.7763     0.8704   1.31e+04       1950          0   0.001281     0.00\n",
      "NOTE:     14   400  0.00021           0.8064     0.8397  1.112e+04       2123          0   0.001281     0.00\n",
      "NOTE:     15   400  0.00021           0.8573     0.8384  1.268e+04       2444          0   0.001281     0.00\n",
      "NOTE:     16   400  0.00021           0.8318     0.8733   1.38e+04       2002          0   0.001281     0.00\n",
      "NOTE:     17   400  0.00021           0.8908     0.8759  1.442e+04       2043          0   0.001281     0.00\n",
      "NOTE:     18   400  0.00021           0.8169     0.8571  1.234e+04       2057          0   0.001281     0.00\n",
      "NOTE:     19   400  0.00021           0.9101     0.8966    1.5e+04       1729          0   0.001281     0.00\n",
      "NOTE:     20   400  0.00021           0.8727     0.8845  1.448e+04       1890          0   0.001281     0.00\n",
      "NOTE:     21   400  0.00021           0.8379     0.8664  1.378e+04       2125          0   0.001281     0.00\n",
      "NOTE:     22   400  0.00021            0.844     0.8519  1.285e+04       2234          0   0.001281     0.00\n",
      "NOTE:     23   400  0.00021           0.8401     0.8443  1.148e+04       2117          0   0.001281     0.00\n",
      "NOTE:     24   400  0.00021            0.844     0.8587  1.269e+04       2088          0   0.001281     0.00\n",
      "NOTE:     25   400  0.00021           0.9659     0.8478  1.488e+04       2672          0   0.001281     0.00\n",
      "NOTE:     26   400  0.00021           0.8614     0.8787  1.416e+04       1954          0   0.001281     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  90       0.0002          0.8726       0.86  3.647e+05  5.935e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00021           0.8497     0.8571  1.319e+04       2199          0   0.001281     0.00\n",
      "NOTE:      1   400  0.00021           0.9739     0.8616  1.448e+04       2325          0   0.001281     0.00\n",
      "NOTE:      2   400  0.00021           0.8963     0.8176  1.215e+04       2711          0   0.001281     0.00\n",
      "NOTE:      3   400  0.00021           0.9358     0.8467  1.475e+04       2670          0   0.001281     0.00\n",
      "NOTE:      4   400  0.00021           0.8416     0.8407  1.183e+04       2242          0   0.001281     0.00\n",
      "NOTE:      5   400  0.00021           0.9317     0.8532  1.428e+04       2456          0   0.001281     0.00\n",
      "NOTE:      6   400  0.00021           0.9228     0.8547  1.474e+04       2506          0   0.001281     0.00\n",
      "NOTE:      7   400  0.00021           0.8351     0.7867  1.122e+04       3043          0   0.001281     0.00\n",
      "NOTE:      8   400  0.00021           0.8819     0.8655  1.442e+04       2241          0   0.001281     0.00\n",
      "NOTE:      9   400  0.00021           0.9411     0.8594  1.475e+04       2412          0   0.001281     0.00\n",
      "NOTE:     10   400  0.00021           0.7967     0.8581  1.241e+04       2051          0   0.001281     0.00\n",
      "NOTE:     11   400  0.00021           0.7236     0.8712  1.147e+04       1696          0   0.001281     0.00\n",
      "NOTE:     12   400  0.00021           0.8124     0.8138  1.184e+04       2710          0   0.001282     0.00\n",
      "NOTE:     13   400  0.00021           0.9521     0.8861  1.541e+04       1981          0   0.001282     0.00\n",
      "NOTE:     14   400  0.00021           0.6947     0.8315  1.014e+04       2055          0   0.001282     0.00\n",
      "NOTE:     15   400  0.00021           0.9753     0.8086  1.416e+04       3350          0   0.001282     0.00\n",
      "NOTE:     16   400  0.00021            1.052     0.8681  1.588e+04       2413          0   0.001282     0.00\n",
      "NOTE:     17   400  0.00021           0.8564     0.8617  1.363e+04       2188          0   0.001282     0.00\n",
      "NOTE:     18   400  0.00021           0.7814     0.8417  1.101e+04       2070          0   0.001282     0.00\n",
      "NOTE:     19   400  0.00021           0.7592     0.8669  1.272e+04       1954          0   0.001282     0.00\n",
      "NOTE:     20   400  0.00021           0.8595      0.857  1.291e+04       2154          0   0.001282     0.00\n",
      "NOTE:     21   400  0.00021           0.8192     0.8445  1.209e+04       2226          0   0.001282     0.00\n",
      "NOTE:     22   400  0.00021            1.003     0.8801  1.664e+04       2266          0   0.001282     0.00\n",
      "NOTE:     23   400  0.00021           0.8582     0.8655  1.261e+04       1959          0   0.001282     0.00\n",
      "NOTE:     24   400  0.00021           0.9427     0.8654  1.338e+04       2080          0   0.001282     0.00\n",
      "NOTE:     25   400  0.00021            1.022     0.8514  1.689e+04       2949          0   0.001282     0.00\n",
      "NOTE:     26   400  0.00021           0.8664     0.8433  1.274e+04       2368          0   0.001282     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  91       0.0002          0.8809     0.8511  3.617e+05  6.328e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00021           0.9041     0.8719  1.416e+04       2080          0   0.001282     0.00\n",
      "NOTE:      1   400  0.00021            1.072     0.8697  1.686e+04       2527          0   0.001282     0.00\n",
      "NOTE:      2   400  0.00021           0.9504     0.7988  1.241e+04       3127          0   0.001282     0.00\n",
      "NOTE:      3   400  0.00021           0.9052     0.8894  1.469e+04       1826          0   0.001282     0.00\n",
      "NOTE:      4   400  0.00021           0.9298     0.8567  1.313e+04       2197          0   0.001282     0.00\n",
      "NOTE:      5   400  0.00021           0.9014     0.8808  1.439e+04       1948          0   0.001282     0.00\n",
      "NOTE:      6   400  0.00021            0.752     0.8731   1.18e+04       1714          0   0.001282     0.00\n",
      "NOTE:      7   400  0.00021           0.8833     0.8587  1.454e+04       2392          0   0.001282     0.00\n",
      "NOTE:      8   400  0.00021            1.026      0.852  1.553e+04       2698          0   0.001282     0.00\n",
      "NOTE:      9   400  0.00021           0.8739     0.8851  1.502e+04       1949          0   0.001282     0.00\n",
      "NOTE:     10   400  0.00021           0.8645     0.8794  1.322e+04       1813          0   0.001282     0.00\n",
      "NOTE:     11   400  0.00021           0.8681     0.8642  1.257e+04       1975          0   0.001282     0.00\n",
      "NOTE:     12   400  0.00021            1.003      0.857  1.501e+04       2505          0   0.001282     0.00\n",
      "NOTE:     13   400  0.00021           0.9886     0.8584  1.565e+04       2582          0   0.001282     0.00\n",
      "NOTE:     14   400  0.00021           0.8069     0.8806  1.253e+04       1700          0   0.001282     0.00\n",
      "NOTE:     15   400  0.00021           0.8993     0.8518  1.378e+04       2396          0   0.001282     0.00\n",
      "NOTE:     16   400  0.00021            0.941     0.8122  1.301e+04       3008          0   0.001283     0.00\n",
      "NOTE:     17   400  0.00021           0.8945     0.8395  1.324e+04       2532          0   0.001283     0.00\n",
      "NOTE:     18   400  0.00021           0.8234     0.8826  1.316e+04       1750          0   0.001283     0.00\n",
      "NOTE:     19   400  0.00021           0.9101     0.8665  1.429e+04       2201          0   0.001283     0.00\n",
      "NOTE:     20   400  0.00021           0.9078     0.8698  1.529e+04       2288          0   0.001283     0.00\n",
      "NOTE:     21   400  0.00021           0.8324     0.8802  1.339e+04       1822          0   0.001283     0.00\n",
      "NOTE:     22   400  0.00021           0.8835     0.8562  1.411e+04       2369          0   0.001283     0.00\n",
      "NOTE:     23   400  0.00021           0.8323     0.8718  1.324e+04       1947          0   0.001283     0.00\n",
      "NOTE:     24   400  0.00021           0.7009      0.879  1.305e+04       1795          0   0.001283     0.00\n",
      "NOTE:     25   400  0.00021           0.7671     0.8813  1.325e+04       1785          0   0.001283     0.00\n",
      "NOTE:     26   400  0.00021           0.8977     0.8693  1.483e+04       2230          0   0.001283     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  92       0.0002          0.8896     0.8641  3.761e+05  5.916e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00021           0.9161     0.8605  1.354e+04       2194          0   0.001283     0.00\n",
      "NOTE:      1   400  0.00021           0.9385     0.8609  1.524e+04       2462          0   0.001283     0.00\n",
      "NOTE:      2   400  0.00021           0.8981     0.8675  1.409e+04       2153          0   0.001283     0.00\n",
      "NOTE:      3   400  0.00021           0.9438     0.8639  1.551e+04       2442          0   0.001283     0.00\n",
      "NOTE:      4   400  0.00021           0.7736     0.8257  1.068e+04       2255          0   0.001283     0.00\n",
      "NOTE:      5   400  0.00021           0.8068     0.8352  1.208e+04       2383          0   0.001283     0.00\n",
      "NOTE:      6   400  0.00021           0.9743     0.8555   1.45e+04       2449          0   0.001283     0.00\n",
      "NOTE:      7   400  0.00021           0.9703     0.8484  1.504e+04       2686          0   0.001283     0.00\n",
      "NOTE:      8   400  0.00021           0.9181     0.9036  1.676e+04       1789          0   0.001284     0.00\n",
      "NOTE:      9   400  0.00021           0.8938       0.88  1.413e+04       1926          0   0.001284     0.00\n",
      "NOTE:     10   400  0.00021           0.8503     0.8706  1.413e+04       2100          0   0.001284     0.00\n",
      "NOTE:     11   400  0.00021            0.985     0.8717  1.585e+04       2332          0   0.001284     0.00\n",
      "NOTE:     12   400  0.00021            0.964     0.8238   1.38e+04       2951          0   0.001284     0.00\n",
      "NOTE:     13   400  0.00021           0.8951     0.8688  1.521e+04       2296          0   0.001284     0.00\n",
      "NOTE:     14   400  0.00021           0.9813     0.8282  1.369e+04       2839          0   0.001284     0.00\n",
      "NOTE:     15   400  0.00021            1.033     0.8728  1.591e+04       2318          0   0.001284     0.00\n",
      "NOTE:     16   400  0.00021           0.8687     0.8691  1.352e+04       2035          0   0.001284     0.00\n",
      "NOTE:     17   400  0.00021           0.9511     0.8814  1.526e+04       2052          0   0.001284     0.00\n",
      "NOTE:     18   400  0.00021           0.9139     0.8324  1.385e+04       2789          0   0.001284     0.00\n",
      "NOTE:     19   400  0.00021           0.7223     0.8419  1.117e+04       2098          0   0.001284     0.00\n",
      "NOTE:     20   400  0.00021            1.045     0.8633  1.585e+04       2510          0   0.001284     0.00\n",
      "NOTE:     21   400  0.00021           0.7876      0.871  1.258e+04       1863          0   0.001284     0.00\n",
      "NOTE:     22   400  0.00021           0.8126      0.876  1.298e+04       1838          0   0.001284     0.00\n",
      "NOTE:     23   400  0.00021           0.8344     0.8792  1.407e+04       1934          0   0.001284     0.00\n",
      "NOTE:     24   400  0.00021           0.8969     0.8742   1.42e+04       2044          0   0.001284     0.00\n",
      "NOTE:     25   400  0.00021           0.8946     0.7984  1.155e+04       2916          0   0.001284     0.00\n",
      "NOTE:     26   400  0.00021           0.9863     0.8542  1.418e+04       2420          0   0.001284     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  93       0.0002          0.9058     0.8594  3.793e+05  6.207e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00021           0.9032     0.8774  1.494e+04       2088          0   0.001284     0.00\n",
      "NOTE:      1   400  0.00021           0.9728     0.8776  1.591e+04       2218          0   0.001284     0.00\n",
      "NOTE:      2   400  0.00021             1.08     0.8667  1.647e+04       2533          0   0.001284     0.00\n",
      "NOTE:      3   400  0.00021           0.9144      0.843  1.361e+04       2535          0   0.001284     0.00\n",
      "NOTE:      4   400  0.00021           0.9556     0.8413  1.382e+04       2608          0   0.001284     0.00\n",
      "NOTE:      5   400  0.00021            1.018     0.8704  1.522e+04       2265          0   0.001285     0.00\n",
      "NOTE:      6   400  0.00021             1.02     0.8751  1.607e+04       2294          0   0.001285     0.00\n",
      "NOTE:      7   400  0.00021           0.8649     0.8393  1.133e+04       2168          0   0.001285     0.00\n",
      "NOTE:      8   400  0.00021           0.8394     0.8167    1.2e+04       2694          0   0.001285     0.00\n",
      "NOTE:      9   400  0.00021           0.8879     0.8594  1.465e+04       2396          0   0.001285     0.00\n",
      "NOTE:     10   400  0.00021           0.9933     0.8361   1.48e+04       2901          0   0.001285     0.00\n",
      "NOTE:     11   400  0.00021           0.7885     0.8242  1.241e+04       2646          0   0.001285     0.00\n",
      "NOTE:     12   400  0.00021           0.7019     0.8327       9757       1961          0   0.001285     0.00\n",
      "NOTE:     13   400  0.00021           0.8435     0.8553  1.288e+04       2179          0   0.001285     0.00\n",
      "NOTE:     14   400  0.00021           0.8715     0.8691  1.377e+04       2074          0   0.001285     0.00\n",
      "NOTE:     15   400  0.00021           0.8552     0.8343  1.261e+04       2504          0   0.001285     0.00\n",
      "NOTE:     16   400  0.00021           0.8294     0.8646  1.243e+04       1946          0   0.001285     0.00\n",
      "NOTE:     17   400  0.00021           0.9294     0.8414  1.342e+04       2530          0   0.001285     0.00\n",
      "NOTE:     18   400  0.00021           0.9168     0.8445  1.437e+04       2647          0   0.001285     0.00\n",
      "NOTE:     19   400  0.00021           0.8665      0.852  1.293e+04       2246          0   0.001285     0.00\n",
      "NOTE:     20   400  0.00021                1     0.8877  1.628e+04       2059          0   0.001285     0.00\n",
      "NOTE:     21   400  0.00021           0.8797     0.8675  1.479e+04       2258          0   0.001285     0.00\n",
      "NOTE:     22   400  0.00021           0.8941     0.8743  1.442e+04       2073          0   0.001285     0.00\n",
      "NOTE:     23   400  0.00021           0.8591     0.8252  1.259e+04       2667          0   0.001285     0.00\n",
      "NOTE:     24   400  0.00021             0.92     0.8818  1.601e+04       2146          0   0.001285     0.00\n",
      "NOTE:     25   400  0.00021           0.8858     0.8197  1.439e+04       3166          0   0.001285     0.00\n",
      "NOTE:     26   400  0.00021           0.9321     0.8545  1.439e+04       2450          0   0.001285     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  94       0.0002          0.9046     0.8541  3.763e+05  6.425e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400  0.00021            0.759     0.8557  1.178e+04       1987          0   0.001285     0.00\n",
      "NOTE:      1   400  0.00021           0.8891     0.8782  1.316e+04       1824          0   0.001285     0.00\n",
      "NOTE:      2   400  0.00021            1.027     0.8689  1.591e+04       2401          0   0.001285     0.00\n",
      "NOTE:      3   400  0.00021           0.8652       0.87  1.454e+04       2173          0   0.001285     0.00\n",
      "NOTE:      4   400  0.00021           0.8719     0.8593  1.339e+04       2192          0   0.001285     0.00\n",
      "NOTE:      5   400  0.00021           0.8387     0.8485  1.292e+04       2307          0   0.001285     0.00\n",
      "NOTE:      6   400  0.00021            0.842      0.864  1.386e+04       2181          0   0.001285     0.00\n",
      "NOTE:      7   400  0.00021           0.8268     0.8993  1.387e+04       1553          0   0.001285     0.00\n",
      "NOTE:      8   400  0.00021           0.8477     0.8547  1.382e+04       2350          0   0.001285     0.00\n",
      "NOTE:      9   400  0.00021           0.8796     0.8959  1.436e+04       1668          0   0.001285     0.00\n",
      "NOTE:     10   400  0.00021           0.8284     0.8742  1.439e+04       2071          0   0.001285     0.00\n",
      "NOTE:     11   400  0.00021           0.8308     0.8742  1.252e+04       1802          0   0.001285     0.00\n",
      "NOTE:     12   400  0.00021           0.8303     0.8486   1.31e+04       2338          0   0.001286     0.00\n",
      "NOTE:     13   400  0.00021            0.833     0.8862  1.399e+04       1796          0   0.001286     0.00\n",
      "NOTE:     14   400  0.00021           0.9346     0.8558  1.441e+04       2428          0   0.001286     0.00\n",
      "NOTE:     15   400  0.00021           0.9539     0.8575   1.38e+04       2292          0   0.001286     0.00\n",
      "NOTE:     16   400  0.00021           0.9095     0.8684  1.493e+04       2263          0   0.001286     0.00\n",
      "NOTE:     17   400  0.00021           0.9192     0.8711  1.538e+04       2275          0   0.001286     0.00\n",
      "NOTE:     18   400  0.00021           0.7774     0.8681  1.192e+04       1812          0   0.001286     0.00\n",
      "NOTE:     19   400  0.00021           0.8157     0.8651  1.383e+04       2157          0   0.001286     0.00\n",
      "NOTE:     20   400  0.00021           0.9645     0.8537  1.359e+04       2330          0   0.001286     0.00\n",
      "NOTE:     21   400  0.00021           0.9162     0.8029  1.282e+04       3146          0   0.001286     0.00\n",
      "NOTE:     22   400  0.00021           0.8823     0.8801  1.388e+04       1892          0   0.001286     0.00\n",
      "NOTE:     23   400  0.00021           0.8793     0.8438   1.38e+04       2555          0   0.001286     0.00\n",
      "NOTE:     24   400  0.00021           0.8333     0.8839  1.398e+04       1837          0   0.001286     0.00\n",
      "NOTE:     25   400  0.00021           0.8851     0.8697  1.405e+04       2105          0   0.001286     0.00\n",
      "NOTE:     26   400  0.00021           0.7058     0.8494  1.144e+04       2028          0   0.001286     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  95       0.0002          0.8647     0.8648  3.695e+05  5.776e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000168           0.9444     0.8239  1.354e+04       2894          0   0.001286     0.00\n",
      "NOTE:      1   400 0.000168           0.8513     0.8991  1.577e+04       1769          0   0.001286     0.00\n",
      "NOTE:      2   400 0.000168            1.016     0.8447   1.44e+04       2649          0   0.001287     0.00\n",
      "NOTE:      3   400 0.000168           0.9593     0.8556  1.519e+04       2563          0   0.001287     0.00\n",
      "NOTE:      4   400 0.000168           0.8563     0.8434  1.312e+04       2435          0   0.001287     0.00\n",
      "NOTE:      5   400 0.000168           0.8423     0.8166  1.185e+04       2661          0   0.001287     0.00\n",
      "NOTE:      6   400 0.000168           0.9857     0.8699  1.515e+04       2266          0   0.001287     0.00\n",
      "NOTE:      7   400 0.000168           0.9873     0.8543  1.503e+04       2563          0   0.001287     0.00\n",
      "NOTE:      8   400 0.000168           0.9551     0.8401  1.495e+04       2845          0   0.001287     0.00\n",
      "NOTE:      9   400 0.000168           0.8354     0.8826  1.358e+04       1807          0   0.001287     0.00\n",
      "NOTE:     10   400 0.000168           0.8832     0.8497  1.385e+04       2451          0   0.001287     0.00\n",
      "NOTE:     11   400 0.000168           0.9077     0.8541  1.498e+04       2558          0   0.001287     0.00\n",
      "NOTE:     12   400 0.000168           0.8403     0.8298   1.18e+04       2421          0   0.001287     0.00\n",
      "NOTE:     13   400 0.000168           0.8456     0.8516  1.267e+04       2208          0   0.001287     0.00\n",
      "NOTE:     14   400 0.000168           0.9354     0.8649  1.507e+04       2354          0   0.001287     0.00\n",
      "NOTE:     15   400 0.000168             0.69     0.8736  1.181e+04       1709          0   0.001287     0.00\n",
      "NOTE:     16   400 0.000168           0.9795     0.8689  1.455e+04       2195          0   0.001287     0.00\n",
      "NOTE:     17   400 0.000168           0.8191     0.7705  1.157e+04       3446          0   0.001287     0.00\n",
      "NOTE:     18   400 0.000168           0.7481     0.8967  1.299e+04       1496          0   0.001287     0.00\n",
      "NOTE:     19   400 0.000168            1.017     0.8717  1.755e+04       2583          0   0.001287     0.00\n",
      "NOTE:     20   400 0.000168           0.8357     0.8267  1.268e+04       2659          0   0.001287     0.00\n",
      "NOTE:     21   400 0.000168           0.7079     0.8651  1.132e+04       1764          0   0.001287     0.00\n",
      "NOTE:     22   400 0.000168           0.9182     0.8491  1.347e+04       2393          0   0.001287     0.00\n",
      "NOTE:     23   400 0.000168           0.7771     0.8323  1.127e+04       2269          0   0.001287     0.00\n",
      "NOTE:     24   400 0.000168           0.9731     0.8961  1.523e+04       1767          0   0.001287     0.00\n",
      "NOTE:     25   400 0.000168           0.8281     0.8765  1.283e+04       1808          0   0.001287     0.00\n",
      "NOTE:     26   400 0.000168           0.9101     0.8189  1.421e+04       3143          0   0.001287     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  96       0.0002          0.8833     0.8533  3.704e+05  6.368e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000168           0.9002     0.8783  1.354e+04       1876          0   0.001287     0.00\n",
      "NOTE:      1   400 0.000168           0.7663     0.9016   1.33e+04       1451          0   0.001287     0.00\n",
      "NOTE:      2   400 0.000168           0.8258     0.8542  1.272e+04       2170          0   0.001287     0.00\n",
      "NOTE:      3   400 0.000168           0.8332     0.8677  1.256e+04       1915          0   0.001287     0.00\n",
      "NOTE:      4   400 0.000168           0.8997     0.8568  1.484e+04       2480          0   0.001287     0.00\n",
      "NOTE:      5   400 0.000168           0.8389      0.878   1.45e+04       2015          0   0.001287     0.00\n",
      "NOTE:      6   400 0.000168           0.9396     0.8401  1.308e+04       2490          0   0.001287     0.00\n",
      "NOTE:      7   400 0.000168           0.8412     0.8492  1.186e+04       2107          0   0.001288     0.00\n",
      "NOTE:      8   400 0.000168           0.8696     0.8697  1.343e+04       2012          0   0.001288     0.00\n",
      "NOTE:      9   400 0.000168           0.9139     0.8615  1.367e+04       2198          0   0.001288     0.00\n",
      "NOTE:     10   400 0.000168           0.9056     0.8659   1.36e+04       2106          0   0.001288     0.00\n",
      "NOTE:     11   400 0.000168           0.8782     0.8677  1.278e+04       1949          0   0.001288     0.00\n",
      "NOTE:     12   400 0.000168           0.8758     0.8562  1.233e+04       2070          0   0.001288     0.00\n",
      "NOTE:     13   400 0.000168           0.8558     0.8738  1.463e+04       2113          0   0.001288     0.00\n",
      "NOTE:     14   400 0.000168           0.7823      0.845  1.265e+04       2320          0   0.001288     0.00\n",
      "NOTE:     15   400 0.000168           0.7957     0.8842  1.447e+04       1895          0   0.001288     0.00\n",
      "NOTE:     16   400 0.000168           0.8364     0.8759  1.238e+04       1754          0   0.001288     0.00\n",
      "NOTE:     17   400 0.000168           0.9008     0.8609  1.343e+04       2170          0   0.001288     0.00\n",
      "NOTE:     18   400 0.000168           0.9288     0.8499  1.427e+04       2521          0   0.001288     0.00\n",
      "NOTE:     19   400 0.000168            1.085     0.8995  1.758e+04       1964          0   0.001288     0.00\n",
      "NOTE:     20   400 0.000168           0.8621     0.8365  1.299e+04       2539          0   0.001288     0.00\n",
      "NOTE:     21   400 0.000168           0.6238     0.8191       9269       2047          0   0.001288     0.00\n",
      "NOTE:     22   400 0.000168           0.8922     0.8873  1.496e+04       1899          0   0.001288     0.00\n",
      "NOTE:     23   400 0.000168           0.9201     0.8615  1.347e+04       2166          0   0.001288     0.00\n",
      "NOTE:     24   400 0.000168            1.062     0.8511  1.619e+04       2832          0   0.001288     0.00\n",
      "NOTE:     25   400 0.000168           0.9624     0.8859  1.568e+04       2019          0   0.001288     0.00\n",
      "NOTE:     26   400 0.000168            1.017     0.8276  1.535e+04       3197          0   0.001288     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  97       0.0002           0.882     0.8638  3.695e+05  5.828e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000168           0.8207     0.8844  1.318e+04       1723          0   0.001288     0.00\n",
      "NOTE:      1   400 0.000168           0.7713     0.8659   1.27e+04       1966          0   0.001288     0.00\n",
      "NOTE:      2   400 0.000168           0.8225     0.8655  1.309e+04       2035          0   0.001288     0.00\n",
      "NOTE:      3   400 0.000168           0.7406     0.8909  1.147e+04       1405          0   0.001288     0.00\n",
      "NOTE:      4   400 0.000168            1.051     0.8655  1.674e+04       2602          0   0.001288     0.00\n",
      "NOTE:      5   400 0.000168           0.8548     0.8232  1.089e+04       2339          0   0.001288     0.00\n",
      "NOTE:      6   400 0.000168            0.784     0.8226  1.092e+04       2355          0   0.001288     0.00\n",
      "NOTE:      7   400 0.000168           0.9397     0.8497  1.369e+04       2421          0   0.001288     0.00\n",
      "NOTE:      8   400 0.000168           0.7199     0.8891  1.178e+04       1469          0   0.001288     0.00\n",
      "NOTE:      9   400 0.000168            0.883     0.8905  1.569e+04       1929          0   0.001288     0.00\n",
      "NOTE:     10   400 0.000168           0.9701     0.8361  1.349e+04       2643          0   0.001288     0.00\n",
      "NOTE:     11   400 0.000168            0.828     0.8765  1.441e+04       2030          0   0.001288     0.00\n",
      "NOTE:     12   400 0.000168           0.9311     0.8463  1.334e+04       2423          0   0.001288     0.00\n",
      "NOTE:     13   400 0.000168            0.935     0.8654  1.422e+04       2212          0   0.001288     0.00\n",
      "NOTE:     14   400 0.000168           0.8388     0.8768  1.366e+04       1918          0   0.001288     0.00\n",
      "NOTE:     15   400 0.000168           0.9272     0.9006  1.591e+04       1755          0   0.001288     0.00\n",
      "NOTE:     16   400 0.000168           0.8476     0.8552  1.351e+04       2287          0   0.001288     0.00\n",
      "NOTE:     17   400 0.000168           0.8065     0.8459  1.261e+04       2297          0   0.001288     0.00\n",
      "NOTE:     18   400 0.000168           0.8169     0.8168  1.177e+04       2640          0   0.001288     0.00\n",
      "NOTE:     19   400 0.000168           0.8266     0.8653  1.379e+04       2147          0   0.001288     0.00\n",
      "NOTE:     20   400 0.000168           0.8472     0.8598  1.332e+04       2171          0   0.001288     0.00\n",
      "NOTE:     21   400 0.000168           0.9393     0.8327  1.401e+04       2815          0   0.001288     0.00\n",
      "NOTE:     22   400 0.000168            1.002     0.8388  1.517e+04       2917          0   0.001289     0.00\n",
      "NOTE:     23   400 0.000168           0.9617     0.8294  1.444e+04       2971          0   0.001289     0.00\n",
      "NOTE:     24   400 0.000168           0.7255     0.8361  1.069e+04       2095          0   0.001289     0.00\n",
      "NOTE:     25   400 0.000168           0.9496     0.8947  1.579e+04       1859          0   0.001289     0.00\n",
      "NOTE:     26   400 0.000168           0.9467     0.8526  1.418e+04       2452          0   0.001289     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  98       0.0002          0.8699     0.8589  3.645e+05  5.988e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000168           0.8015     0.9121  1.357e+04       1308          0   0.001289     0.00\n",
      "NOTE:      1   400 0.000168           0.8505     0.8518  1.296e+04       2255          0   0.001289     0.00\n",
      "NOTE:      2   400 0.000168            0.851     0.8342  1.242e+04       2467          0   0.001289     0.00\n",
      "NOTE:      3   400 0.000168           0.8465     0.8586  1.285e+04       2117          0   0.001289     0.00\n",
      "NOTE:      4   400 0.000168           0.7996     0.8885  1.486e+04       1865          0   0.001289     0.00\n",
      "NOTE:      5   400 0.000168           0.8473     0.8712   1.47e+04       2173          0   0.001289     0.00\n",
      "NOTE:      6   400 0.000168           0.9092     0.8654    1.4e+04       2177          0   0.001289     0.00\n",
      "NOTE:      7   400 0.000168           0.9415     0.8941  1.504e+04       1782          0   0.001289     0.00\n",
      "NOTE:      8   400 0.000168           0.7729     0.7871  1.001e+04       2707          0   0.001289     0.00\n",
      "NOTE:      9   400 0.000168           0.8331     0.8291  1.172e+04       2416          0   0.001289     0.00\n",
      "NOTE:     10   400 0.000168           0.9192     0.8773  1.377e+04       1926          0   0.001289     0.00\n",
      "NOTE:     11   400 0.000168           0.8831     0.8639  1.368e+04       2156          0   0.001289     0.00\n",
      "NOTE:     12   400 0.000168            1.026      0.806  1.394e+04       3356          0   0.001289     0.00\n",
      "NOTE:     13   400 0.000168           0.8233     0.8571  1.328e+04       2214          0   0.001289     0.00\n",
      "NOTE:     14   400 0.000168           0.8822     0.8882  1.462e+04       1840          0   0.001289     0.00\n",
      "NOTE:     15   400 0.000168           0.9022     0.8659  1.464e+04       2267          0   0.001289     0.00\n",
      "NOTE:     16   400 0.000168           0.8103      0.878  1.369e+04       1901          0   0.001289     0.00\n",
      "NOTE:     17   400 0.000168           0.7846     0.8526  1.262e+04       2182          0   0.001289     0.00\n",
      "NOTE:     18   400 0.000168           0.9476     0.8505  1.488e+04       2616          0   0.001289     0.00\n",
      "NOTE:     19   400 0.000168           0.9171      0.879  1.549e+04       2133          0   0.001289     0.00\n",
      "NOTE:     20   400 0.000168           0.7721     0.8431  1.156e+04       2151          0   0.001289     0.00\n",
      "NOTE:     21   400 0.000168           0.8322     0.8373  1.291e+04       2509          0   0.001289     0.00\n",
      "NOTE:     22   400 0.000168           0.8287     0.8937  1.339e+04       1592          0   0.001289     0.00\n",
      "NOTE:     23   400 0.000168           0.8393     0.8416  1.317e+04       2478          0   0.001289     0.00\n",
      "NOTE:     24   400 0.000168           0.9229     0.8364  1.437e+04       2810          0   0.001289     0.00\n",
      "NOTE:     25   400 0.000168           0.8343       0.87  1.364e+04       2038          0   0.001289     0.00\n",
      "NOTE:     26   400 0.000168           0.8936     0.8221  1.352e+04       2926          0   0.001289     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  99       0.0002          0.8619     0.8582  3.653e+05  6.036e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000168           0.9185     0.8438  1.334e+04       2470          0    0.00129     0.00\n",
      "NOTE:      1   400 0.000168           0.9695     0.7999  1.257e+04       3146          0    0.00129     0.00\n",
      "NOTE:      2   400 0.000168             0.98     0.8545  1.514e+04       2577          0    0.00129     0.00\n",
      "NOTE:      3   400 0.000168           0.9767     0.8954  1.645e+04       1921          0    0.00129     0.00\n",
      "NOTE:      4   400 0.000168           0.8289     0.8174  1.148e+04       2564          0    0.00129     0.00\n",
      "NOTE:      5   400 0.000168            1.024     0.8744  1.542e+04       2214          0    0.00129     0.00\n",
      "NOTE:      6   400 0.000168           0.7701     0.8432  1.067e+04       1984          0    0.00129     0.00\n",
      "NOTE:      7   400 0.000168           0.8215     0.8347  1.229e+04       2433          0    0.00129     0.00\n",
      "NOTE:      8   400 0.000168           0.8154      0.887  1.405e+04       1790          0    0.00129     0.00\n",
      "NOTE:      9   400 0.000168           0.9316     0.8192  1.311e+04       2893          0    0.00129     0.00\n",
      "NOTE:     10   400 0.000168             1.02     0.8864  1.772e+04       2271          0    0.00129     0.00\n",
      "NOTE:     11   400 0.000168           0.9121     0.8516  1.353e+04       2357          0    0.00129     0.00\n",
      "NOTE:     12   400 0.000168           0.9032     0.8543  1.437e+04       2451          0    0.00129     0.00\n",
      "NOTE:     13   400 0.000168           0.8247     0.8265  1.262e+04       2650          0    0.00129     0.00\n",
      "NOTE:     14   400 0.000168           0.9645     0.8635    1.5e+04       2371          0    0.00129     0.00\n",
      "NOTE:     15   400 0.000168           0.9832     0.8472  1.438e+04       2592          0    0.00129     0.00\n",
      "NOTE:     16   400 0.000168            1.062     0.8616  1.483e+04       2382          0    0.00129     0.00\n",
      "NOTE:     17   400 0.000168           0.8527     0.8543  1.389e+04       2368          0    0.00129     0.00\n",
      "NOTE:     18   400 0.000168           0.8072     0.8563  1.213e+04       2035          0    0.00129     0.00\n",
      "NOTE:     19   400 0.000168           0.9258     0.8628  1.442e+04       2292          0    0.00129     0.00\n",
      "NOTE:     20   400 0.000168           0.9417      0.878  1.573e+04       2185          0    0.00129     0.00\n",
      "NOTE:     21   400 0.000168           0.9588     0.8578  1.422e+04       2357          0    0.00129     0.00\n",
      "NOTE:     22   400 0.000168           0.8185     0.8713  1.437e+04       2122          0    0.00129     0.00\n",
      "NOTE:     23   400 0.000168           0.8731     0.8368   1.38e+04       2692          0    0.00129     0.00\n",
      "NOTE:     24   400 0.000168           0.8049     0.8835  1.325e+04       1747          0    0.00129     0.00\n",
      "NOTE:     25   400 0.000168           0.8365      0.883  1.368e+04       1812          0    0.00129     0.00\n",
      "NOTE:     26   400 0.000168           0.8554     0.8853  1.363e+04       1765          0    0.00129     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  100      0.0002           0.903     0.8576  3.761e+05  6.244e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000168           0.9592     0.8584  1.497e+04       2470          0    0.00129     0.00\n",
      "NOTE:      1   400 0.000168           0.9463     0.8676  1.579e+04       2410          0    0.00129     0.00\n",
      "NOTE:      2   400 0.000168           0.8293     0.8722  1.407e+04       2061          0    0.00129     0.00\n",
      "NOTE:      3   400 0.000168           0.8881     0.8695  1.472e+04       2209          0    0.00129     0.00\n",
      "NOTE:      4   400 0.000168           0.8278     0.8502  1.262e+04       2224          0    0.00129     0.00\n",
      "NOTE:      5   400 0.000168           0.9201     0.8492  1.415e+04       2512          0    0.00129     0.00\n",
      "NOTE:      6   400 0.000168           0.8726     0.8277  1.329e+04       2767          0    0.00129     0.00\n",
      "NOTE:      7   400 0.000168           0.7871      0.833  1.182e+04       2369          0    0.00129     0.00\n",
      "NOTE:      8   400 0.000168           0.8943      0.851  1.366e+04       2391          0    0.00129     0.00\n",
      "NOTE:      9   400 0.000168           0.8309     0.8471  1.301e+04       2347          0    0.00129     0.00\n",
      "NOTE:     10   400 0.000168           0.8335     0.8491  1.366e+04       2428          0    0.00129     0.00\n",
      "NOTE:     11   400 0.000168            0.823     0.8533  1.343e+04       2309          0    0.00129     0.00\n",
      "NOTE:     12   400 0.000168           0.8449     0.8104  1.042e+04       2439          0    0.00129     0.00\n",
      "NOTE:     13   400 0.000168           0.9546     0.8218  1.373e+04       2977          0    0.00129     0.00\n",
      "NOTE:     14   400 0.000168           0.8253      0.885   1.53e+04       1988          0    0.00129     0.00\n",
      "NOTE:     15   400 0.000168            1.034     0.8401  1.483e+04       2824          0    0.00129     0.00\n",
      "NOTE:     16   400 0.000168           0.9937     0.8853   1.56e+04       2021          0    0.00129     0.00\n",
      "NOTE:     17   400 0.000168           0.7915     0.8236  1.145e+04       2453          0    0.00129     0.00\n",
      "NOTE:     18   400 0.000168           0.8982     0.8499   1.44e+04       2543          0    0.00129     0.00\n",
      "NOTE:     19   400 0.000168           0.7786     0.8149  1.083e+04       2460          0    0.00129     0.00\n",
      "NOTE:     20   400 0.000168            1.035     0.8837  1.685e+04       2217          0    0.00129     0.00\n",
      "NOTE:     21   400 0.000168           0.8867     0.8855  1.454e+04       1881          0    0.00129     0.00\n",
      "NOTE:     22   400 0.000168            0.859     0.8471  1.299e+04       2344          0    0.00129     0.00\n",
      "NOTE:     23   400 0.000168           0.8608     0.8519  1.347e+04       2342          0    0.00129     0.00\n",
      "NOTE:     24   400 0.000168           0.9815     0.8293  1.457e+04       3000          0    0.00129     0.00\n",
      "NOTE:     25   400 0.000168           0.8663      0.839  1.436e+04       2755          0    0.00129     0.00\n",
      "NOTE:     26   400 0.000168           0.9601     0.8745  1.448e+04       2079          0    0.00129     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  101      0.0002          0.8883      0.852   3.73e+05  6.482e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000168            0.808     0.8669  1.231e+04       1890          0    0.00129     0.00\n",
      "NOTE:      1   400 0.000168           0.9188     0.8373  1.359e+04       2640          0    0.00129     0.00\n",
      "NOTE:      2   400 0.000168           0.8771     0.8504  1.252e+04       2203          0    0.00129     0.00\n",
      "NOTE:      3   400 0.000168           0.9111     0.8764  1.591e+04       2244          0    0.00129     0.00\n",
      "NOTE:      4   400 0.000168           0.8418      0.893  1.449e+04       1736          0    0.00129     0.00\n",
      "NOTE:      5   400 0.000168           0.9675     0.8285  1.428e+04       2954          0    0.00129     0.00\n",
      "NOTE:      6   400 0.000168           0.8918     0.8685  1.356e+04       2053          0    0.00129     0.00\n",
      "NOTE:      7   400 0.000168             0.85     0.8713  1.467e+04       2167          0    0.00129     0.00\n",
      "NOTE:      8   400 0.000168            1.028     0.8291  1.571e+04       3238          0    0.00129     0.00\n",
      "NOTE:      9   400 0.000168           0.7339      0.808  1.081e+04       2569          0    0.00129     0.00\n",
      "NOTE:     10   400 0.000168           0.8371     0.8326   1.15e+04       2313          0    0.00129     0.00\n",
      "NOTE:     11   400 0.000168           0.9903     0.8654  1.595e+04       2480          0    0.00129     0.00\n",
      "NOTE:     12   400 0.000168           0.8131     0.8126  1.128e+04       2600          0    0.00129     0.00\n",
      "NOTE:     13   400 0.000168           0.8041     0.8633  1.252e+04       1982          0    0.00129     0.00\n",
      "NOTE:     14   400 0.000168           0.8738     0.8675   1.42e+04       2168          0    0.00129     0.00\n",
      "NOTE:     15   400 0.000168           0.8317     0.8389  1.235e+04       2372          0    0.00129     0.00\n",
      "NOTE:     16   400 0.000168           0.9343      0.864  1.427e+04       2247          0    0.00129     0.00\n",
      "NOTE:     17   400 0.000168           0.9467     0.8449  1.459e+04       2678          0    0.00129     0.00\n",
      "NOTE:     18   400 0.000168           0.9303     0.8544  1.258e+04       2144          0    0.00129     0.00\n",
      "NOTE:     19   400 0.000168           0.9193     0.8667  1.437e+04       2210          0    0.00129     0.00\n",
      "NOTE:     20   400 0.000168           0.7818     0.7884  1.156e+04       3104          0    0.00129     0.00\n",
      "NOTE:     21   400 0.000168            0.918     0.8509  1.389e+04       2435          0    0.00129     0.00\n",
      "NOTE:     22   400 0.000168             1.05     0.8054  1.359e+04       3283          0    0.00129     0.00\n",
      "NOTE:     23   400 0.000168           0.8033     0.8818  1.357e+04       1819          0    0.00129     0.00\n",
      "NOTE:     24   400 0.000168             0.88       0.86  1.278e+04       2080          0    0.00129     0.00\n",
      "NOTE:     25   400 0.000168           0.8656     0.8367  1.235e+04       2410          0    0.00129     0.00\n",
      "NOTE:     26   400 0.000168           0.9239     0.8184  1.271e+04       2820          0    0.00129     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  102      0.0002          0.8864     0.8481  3.619e+05  6.484e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000168           0.9077      0.877  1.509e+04       2115          0    0.00129     0.00\n",
      "NOTE:      1   400 0.000168           0.7847     0.8701  1.269e+04       1895          0    0.00129     0.00\n",
      "NOTE:      2   400 0.000168           0.9044      0.867  1.402e+04       2151          0    0.00129     0.00\n",
      "NOTE:      3   400 0.000168           0.7201      0.858  1.099e+04       1818          0    0.00129     0.00\n",
      "NOTE:      4   400 0.000168           0.6584     0.8781  1.155e+04       1604          0    0.00129     0.00\n",
      "NOTE:      5   400 0.000168            0.765      0.864  1.225e+04       1929          0    0.00129     0.00\n",
      "NOTE:      6   400 0.000168           0.7764      0.873  1.305e+04       1898          0    0.00129     0.00\n",
      "NOTE:      7   400 0.000168           0.9132      0.834  1.374e+04       2735          0    0.00129     0.00\n",
      "NOTE:      8   400 0.000168           0.9715     0.8646  1.432e+04       2242          0    0.00129     0.00\n",
      "NOTE:      9   400 0.000168           0.8002     0.8607  1.332e+04       2155          0    0.00129     0.00\n",
      "NOTE:     10   400 0.000168           0.9821      0.847  1.464e+04       2644          0    0.00129     0.00\n",
      "NOTE:     11   400 0.000168           0.8906      0.852   1.47e+04       2552          0    0.00129     0.00\n",
      "NOTE:     12   400 0.000168           0.8387     0.8635  1.294e+04       2044          0    0.00129     0.00\n",
      "NOTE:     13   400 0.000168           0.6486     0.8983  1.123e+04       1272          0    0.00129     0.00\n",
      "NOTE:     14   400 0.000168           0.8628     0.8431  1.391e+04       2589          0    0.00129     0.00\n",
      "NOTE:     15   400 0.000168           0.9519     0.8542  1.469e+04       2508          0    0.00129     0.00\n",
      "NOTE:     16   400 0.000168           0.9642     0.8643  1.443e+04       2266          0    0.00129     0.00\n",
      "NOTE:     17   400 0.000168            1.066      0.859  1.528e+04       2507          0    0.00129     0.00\n",
      "NOTE:     18   400 0.000168           0.9507     0.8229  1.323e+04       2848          0    0.00129     0.00\n",
      "NOTE:     19   400 0.000168           0.8504     0.8424   1.22e+04       2283          0    0.00129     0.00\n",
      "NOTE:     20   400 0.000168            1.024     0.8845   1.74e+04       2271          0    0.00129     0.00\n",
      "NOTE:     21   400 0.000168           0.7131     0.8633  1.162e+04       1840          0    0.00129     0.00\n",
      "NOTE:     22   400 0.000168            1.076      0.841  1.607e+04       3038          0    0.00129     0.00\n",
      "NOTE:     23   400 0.000168           0.7802     0.8903  1.259e+04       1551          0    0.00129     0.00\n",
      "NOTE:     24   400 0.000168           0.9682     0.8591  1.483e+04       2433          0    0.00129     0.00\n",
      "NOTE:     25   400 0.000168           0.9589     0.9052  1.617e+04       1693          0    0.00129     0.00\n",
      "NOTE:     26   400 0.000168             0.91      0.802   1.25e+04       3085          0    0.00129     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  103      0.0002          0.8755     0.8603  3.694e+05  5.997e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000168            1.037     0.8337    1.6e+04       3193          0    0.00129     0.00\n",
      "NOTE:      1   400 0.000168           0.8206     0.8712   1.39e+04       2055          0   0.001291     0.00\n",
      "NOTE:      2   400 0.000168           0.7879     0.8837  1.332e+04       1752          0   0.001291     0.00\n",
      "NOTE:      3   400 0.000168           0.8609     0.8622   1.38e+04       2205          0   0.001291     0.00\n",
      "NOTE:      4   400 0.000168           0.8857     0.8667  1.435e+04       2207          0   0.001291     0.00\n",
      "NOTE:      5   400 0.000168            1.013     0.8719  1.662e+04       2443          0   0.001291     0.00\n",
      "NOTE:      6   400 0.000168            1.043     0.8685  1.691e+04       2560          0   0.001291     0.00\n",
      "NOTE:      7   400 0.000168           0.9632     0.8886   1.56e+04       1955          0   0.001291     0.00\n",
      "NOTE:      8   400 0.000168           0.7956     0.8714  1.205e+04       1777          0   0.001291     0.00\n",
      "NOTE:      9   400 0.000168           0.8666      0.882  1.504e+04       2013          0   0.001291     0.00\n",
      "NOTE:     10   400 0.000168           0.8633     0.8889  1.434e+04       1793          0   0.001291     0.00\n",
      "NOTE:     11   400 0.000168           0.9709     0.8928  1.685e+04       2023          0   0.001291     0.00\n",
      "NOTE:     12   400 0.000168            0.756     0.8979  1.253e+04       1425          0   0.001291     0.00\n",
      "NOTE:     13   400 0.000168           0.9576     0.8492  1.408e+04       2501          0   0.001291     0.00\n",
      "NOTE:     14   400 0.000168           0.8066     0.8816  1.381e+04       1854          0   0.001291     0.00\n",
      "NOTE:     15   400 0.000168           0.8872     0.8611  1.439e+04       2321          0   0.001291     0.00\n",
      "NOTE:     16   400 0.000168           0.8059     0.8016  1.114e+04       2758          0   0.001291     0.00\n",
      "NOTE:     17   400 0.000168           0.9731     0.8319   1.42e+04       2868          0   0.001291     0.00\n",
      "NOTE:     18   400 0.000168           0.8348     0.8387   1.25e+04       2404          0   0.001291     0.00\n",
      "NOTE:     19   400 0.000168            0.904     0.9272   1.65e+04       1296          0   0.001291     0.00\n",
      "NOTE:     20   400 0.000168                1     0.8599  1.562e+04       2545          0   0.001292     0.00\n",
      "NOTE:     21   400 0.000168           0.9096     0.8052   1.32e+04       3192          0   0.001292     0.00\n",
      "NOTE:     22   400 0.000168           0.9834      0.823  1.452e+04       3122          0   0.001292     0.00\n",
      "NOTE:     23   400 0.000168           0.8235     0.8425  1.311e+04       2449          0   0.001292     0.00\n",
      "NOTE:     24   400 0.000168           0.7658     0.8405  1.083e+04       2055          0   0.001292     0.00\n",
      "NOTE:     25   400 0.000168           0.9728     0.8554  1.457e+04       2462          0   0.001292     0.00\n",
      "NOTE:     26   400 0.000168            1.001     0.8191  1.462e+04       3229          0   0.001292     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  104      0.0002          0.8996     0.8602  3.844e+05  6.246e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000134           0.8386     0.8852   1.37e+04       1776          0   0.001292     0.00\n",
      "NOTE:      1   400 0.000134           0.8231      0.868  1.316e+04       2001          0   0.001292     0.00\n",
      "NOTE:      2   400 0.000134            0.853     0.8287   1.23e+04       2543          0   0.001292     0.00\n",
      "NOTE:      3   400 0.000134            0.877     0.8986   1.46e+04       1648          0   0.001292     0.00\n",
      "NOTE:      4   400 0.000134           0.8778     0.8261  1.313e+04       2764          0   0.001292     0.00\n",
      "NOTE:      5   400 0.000134            0.903     0.8712  1.409e+04       2083          0   0.001292     0.00\n",
      "NOTE:      6   400 0.000134           0.8227     0.8304  1.271e+04       2596          0   0.001292     0.00\n",
      "NOTE:      7   400 0.000134           0.9857     0.8421  1.469e+04       2753          0   0.001292     0.00\n",
      "NOTE:      8   400 0.000134           0.8932     0.8737  1.385e+04       2003          0   0.001292     0.00\n",
      "NOTE:      9   400 0.000134           0.8401     0.8448  1.284e+04       2359          0   0.001292     0.00\n",
      "NOTE:     10   400 0.000134           0.8441     0.8706  1.464e+04       2177          0   0.001292     0.00\n",
      "NOTE:     11   400 0.000134           0.8398     0.8556  1.329e+04       2243          0   0.001292     0.00\n",
      "NOTE:     12   400 0.000134             1.08     0.8223  1.608e+04       3475          0   0.001292     0.00\n",
      "NOTE:     13   400 0.000134           0.9968     0.8506  1.405e+04       2468          0   0.001292     0.00\n",
      "NOTE:     14   400 0.000134           0.8405     0.8518  1.414e+04       2460          0   0.001292     0.00\n",
      "NOTE:     15   400 0.000134           0.8703     0.8606  1.458e+04       2361          0   0.001292     0.00\n",
      "NOTE:     16   400 0.000134           0.8624     0.8802  1.368e+04       1863          0   0.001292     0.00\n",
      "NOTE:     17   400 0.000134           0.9243     0.8588  1.506e+04       2476          0   0.001292     0.00\n",
      "NOTE:     18   400 0.000134           0.8975     0.8527  1.357e+04       2344          0   0.001292     0.00\n",
      "NOTE:     19   400 0.000134            0.751     0.8178  1.101e+04       2453          0   0.001292     0.00\n",
      "NOTE:     20   400 0.000134           0.9197     0.8626  1.518e+04       2418          0   0.001292     0.00\n",
      "NOTE:     21   400 0.000134           0.8472     0.8707  1.336e+04       1984          0   0.001292     0.00\n",
      "NOTE:     22   400 0.000134            0.838      0.889  1.387e+04       1732          0   0.001292     0.00\n",
      "NOTE:     23   400 0.000134           0.8243     0.8709  1.339e+04       1984          0   0.001292     0.00\n",
      "NOTE:     24   400 0.000134           0.7898     0.8609  1.257e+04       2031          0   0.001292     0.00\n",
      "NOTE:     25   400 0.000134           0.9337     0.8624  1.403e+04       2239          0   0.001292     0.00\n",
      "NOTE:     26   400 0.000134            0.966     0.8568  1.546e+04       2584          0   0.001292     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  105      0.0001          0.8792     0.8578   3.73e+05  6.182e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000134           0.8927     0.8784  1.532e+04       2121          0   0.001292     0.00\n",
      "NOTE:      1   400 0.000134           0.8894     0.8507  1.386e+04       2433          0   0.001292     0.00\n",
      "NOTE:      2   400 0.000134           0.9059     0.8674  1.535e+04       2346          0   0.001292     0.00\n",
      "NOTE:      3   400 0.000134           0.9337     0.8763  1.464e+04       2067          0   0.001292     0.00\n",
      "NOTE:      4   400 0.000134             0.97      0.845  1.356e+04       2487          0   0.001292     0.00\n",
      "NOTE:      5   400 0.000134           0.9898      0.889  1.607e+04       2005          0   0.001292     0.00\n",
      "NOTE:      6   400 0.000134           0.8107     0.8671  1.381e+04       2116          0   0.001292     0.00\n",
      "NOTE:      7   400 0.000134           0.9507     0.8325  1.364e+04       2744          0   0.001292     0.00\n",
      "NOTE:      8   400 0.000134           0.7868     0.9024  1.349e+04       1460          0   0.001293     0.00\n",
      "NOTE:      9   400 0.000134           0.8455     0.8943  1.363e+04       1611          0   0.001293     0.00\n",
      "NOTE:     10   400 0.000134           0.7726     0.8814  1.234e+04       1661          0   0.001293     0.00\n",
      "NOTE:     11   400 0.000134           0.8748     0.8828  1.438e+04       1908          0   0.001293     0.00\n",
      "NOTE:     12   400 0.000134           0.7823     0.8565  1.255e+04       2102          0   0.001293     0.00\n",
      "NOTE:     13   400 0.000134           0.9914     0.8103  1.305e+04       3055          0   0.001293     0.00\n",
      "NOTE:     14   400 0.000134           0.9156     0.8718  1.419e+04       2087          0   0.001293     0.00\n",
      "NOTE:     15   400 0.000134           0.8831     0.8645  1.439e+04       2255          0   0.001293     0.00\n",
      "NOTE:     16   400 0.000134           0.9352     0.8515  1.435e+04       2503          0   0.001293     0.00\n",
      "NOTE:     17   400 0.000134           0.8787     0.8803  1.518e+04       2064          0   0.001293     0.00\n",
      "NOTE:     18   400 0.000134             1.16     0.8704  1.788e+04       2662          0   0.001293     0.00\n",
      "NOTE:     19   400 0.000134            1.023     0.8508  1.634e+04       2866          0   0.001293     0.00\n",
      "NOTE:     20   400 0.000134            1.057     0.8366  1.576e+04       3078          0   0.001293     0.00\n",
      "NOTE:     21   400 0.000134           0.6565     0.8569  1.138e+04       1901          0   0.001293     0.00\n",
      "NOTE:     22   400 0.000134           0.8218      0.857  1.321e+04       2205          0   0.001293     0.00\n",
      "NOTE:     23   400 0.000134            1.058     0.8594  1.694e+04       2770          0   0.001293     0.00\n",
      "NOTE:     24   400 0.000134           0.8476      0.858  1.317e+04       2179          0   0.001293     0.00\n",
      "NOTE:     25   400 0.000134           0.8266      0.862  1.263e+04       2021          0   0.001293     0.00\n",
      "NOTE:     26   400 0.000134           0.8626     0.8402  1.294e+04       2461          0   0.001293     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  106      0.0001          0.9008     0.8626   3.84e+05  6.117e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000134           0.8861     0.8705   1.35e+04       2008          0   0.001293     0.00\n",
      "NOTE:      1   400 0.000134           0.7596     0.8116  1.159e+04       2691          0   0.001293     0.00\n",
      "NOTE:      2   400 0.000134           0.8951     0.8492  1.376e+04       2443          0   0.001293     0.00\n",
      "NOTE:      3   400 0.000134           0.7754     0.8815  1.336e+04       1795          0   0.001293     0.00\n",
      "NOTE:      4   400 0.000134           0.9866     0.8691  1.463e+04       2204          0   0.001293     0.00\n",
      "NOTE:      5   400 0.000134            1.026     0.8622  1.485e+04       2374          0   0.001293     0.00\n",
      "NOTE:      6   400 0.000134           0.8941     0.8421  1.318e+04       2472          0   0.001293     0.00\n",
      "NOTE:      7   400 0.000134            0.906      0.842  1.392e+04       2613          0   0.001293     0.00\n",
      "NOTE:      8   400 0.000134            1.072     0.9052   1.77e+04       1854          0   0.001293     0.00\n",
      "NOTE:      9   400 0.000134           0.8213     0.8476  1.223e+04       2200          0   0.001293     0.00\n",
      "NOTE:     10   400 0.000134           0.8576     0.9143  1.531e+04       1436          0   0.001293     0.00\n",
      "NOTE:     11   400 0.000134           0.8659      0.817  1.151e+04       2577          0   0.001294     0.00\n",
      "NOTE:     12   400 0.000134           0.9368     0.8464  1.486e+04       2697          0   0.001294     0.00\n",
      "NOTE:     13   400 0.000134           0.7878     0.8126  1.088e+04       2510          0   0.001294     0.00\n",
      "NOTE:     14   400 0.000134           0.9902     0.8355   1.48e+04       2913          0   0.001294     0.00\n",
      "NOTE:     15   400 0.000134           0.8863     0.8776  1.449e+04       2020          0   0.001294     0.00\n",
      "NOTE:     16   400 0.000134           0.9892     0.8375  1.508e+04       2927          0   0.001294     0.00\n",
      "NOTE:     17   400 0.000134           0.7716     0.8832  1.331e+04       1759          0   0.001294     0.00\n",
      "NOTE:     18   400 0.000134           0.8175     0.8153  1.249e+04       2829          0   0.001294     0.00\n",
      "NOTE:     19   400 0.000134            1.054     0.8481    1.5e+04       2686          0   0.001294     0.00\n",
      "NOTE:     20   400 0.000134           0.7957       0.89  1.197e+04       1480          0   0.001294     0.00\n",
      "NOTE:     21   400 0.000134           0.8814     0.8328  1.273e+04       2554          0   0.001294     0.00\n",
      "NOTE:     22   400 0.000134           0.8053     0.9001  1.312e+04       1456          0   0.001294     0.00\n",
      "NOTE:     23   400 0.000134           0.9993     0.8173  1.396e+04       3121          0   0.001294     0.00\n",
      "NOTE:     24   400 0.000134           0.8377      0.895  1.258e+04       1476          0   0.001294     0.00\n",
      "NOTE:     25   400 0.000134           0.8878     0.8729  1.425e+04       2075          0   0.001294     0.00\n",
      "NOTE:     26   400 0.000134           0.8176     0.8689  1.311e+04       1978          0   0.001294     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  107      0.0001           0.889     0.8576  3.682e+05  6.115e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000134           0.9898     0.8723  1.439e+04       2107          0   0.001294     0.00\n",
      "NOTE:      1   400 0.000134            1.079      0.867  1.695e+04       2600          0   0.001294     0.00\n",
      "NOTE:      2   400 0.000134            1.001     0.8238  1.501e+04       3211          0   0.001294     0.00\n",
      "NOTE:      3   400 0.000134           0.8245     0.8509   1.29e+04       2262          0   0.001294     0.00\n",
      "NOTE:      4   400 0.000134           0.8018     0.8731   1.32e+04       1918          0   0.001294     0.00\n",
      "NOTE:      5   400 0.000134           0.8849     0.8761   1.37e+04       1938          0   0.001294     0.00\n",
      "NOTE:      6   400 0.000134           0.8864     0.8333   1.24e+04       2481          0   0.001294     0.00\n",
      "NOTE:      7   400 0.000134           0.8208      0.901  1.377e+04       1512          0   0.001294     0.00\n",
      "NOTE:      8   400 0.000134            0.748     0.8583  1.157e+04       1910          0   0.001294     0.00\n",
      "NOTE:      9   400 0.000134           0.8098     0.8606  1.241e+04       2009          0   0.001294     0.00\n",
      "NOTE:     10   400 0.000134           0.8774     0.8264  1.209e+04       2539          0   0.001294     0.00\n",
      "NOTE:     11   400 0.000134           0.9004     0.8348  1.254e+04       2481          0   0.001294     0.00\n",
      "NOTE:     12   400 0.000134           0.9358     0.8482  1.444e+04       2584          0   0.001294     0.00\n",
      "NOTE:     13   400 0.000134           0.6921     0.8901  1.173e+04       1448          0   0.001294     0.00\n",
      "NOTE:     14   400 0.000134           0.9724     0.8543   1.46e+04       2490          0   0.001294     0.00\n",
      "NOTE:     15   400 0.000134           0.7914     0.8656  1.362e+04       2115          0   0.001294     0.00\n",
      "NOTE:     16   400 0.000134            1.056     0.8402   1.57e+04       2985          0   0.001294     0.00\n",
      "NOTE:     17   400 0.000134           0.7923     0.8674  1.197e+04       1830          0   0.001294     0.00\n",
      "NOTE:     18   400 0.000134           0.8723     0.8664  1.357e+04       2093          0   0.001294     0.00\n",
      "NOTE:     19   400 0.000134           0.7952     0.8346  1.179e+04       2337          0   0.001294     0.00\n",
      "NOTE:     20   400 0.000134            0.868     0.8798  1.465e+04       2001          0   0.001294     0.00\n",
      "NOTE:     21   400 0.000134           0.7212     0.9065  1.284e+04       1324          0   0.001294     0.00\n",
      "NOTE:     22   400 0.000134            0.954     0.8747  1.594e+04       2283          0   0.001294     0.00\n",
      "NOTE:     23   400 0.000134           0.8539     0.8804  1.359e+04       1847          0   0.001294     0.00\n",
      "NOTE:     24   400 0.000134           0.8219     0.8499  1.249e+04       2206          0   0.001294     0.00\n",
      "NOTE:     25   400 0.000134            1.015     0.8311   1.44e+04       2926          0   0.001294     0.00\n",
      "NOTE:     26   400 0.000134           0.8098     0.8533  1.324e+04       2275          0   0.001294     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  108      0.0001          0.8731     0.8596  3.655e+05  5.971e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000134           0.8864     0.8211  1.152e+04       2509          0   0.001294     0.00\n",
      "NOTE:      1   400 0.000134            0.995     0.8602  1.511e+04       2456          0   0.001294     0.00\n",
      "NOTE:      2   400 0.000134           0.9878     0.8511  1.455e+04       2546          0   0.001294     0.00\n",
      "NOTE:      3   400 0.000134           0.9795     0.8793  1.524e+04       2093          0   0.001294     0.00\n",
      "NOTE:      4   400 0.000134           0.9211     0.8166  1.347e+04       3026          0   0.001294     0.00\n",
      "NOTE:      5   400 0.000134           0.9083     0.8791  1.499e+04       2061          0   0.001294     0.00\n",
      "NOTE:      6   400 0.000134           0.8993     0.8587  1.474e+04       2425          0   0.001294     0.00\n",
      "NOTE:      7   400 0.000134           0.7699     0.8778  1.293e+04       1800          0   0.001294     0.00\n",
      "NOTE:      8   400 0.000134           0.8933     0.9123  1.643e+04       1579          0   0.001294     0.00\n",
      "NOTE:      9   400 0.000134            0.998     0.8501  1.465e+04       2584          0   0.001294     0.00\n",
      "NOTE:     10   400 0.000134           0.9792     0.8373  1.415e+04       2748          0   0.001294     0.00\n",
      "NOTE:     11   400 0.000134           0.8734     0.8737  1.251e+04       1809          0   0.001294     0.00\n",
      "NOTE:     12   400 0.000134            1.092     0.8336  1.563e+04       3120          0   0.001294     0.00\n",
      "NOTE:     13   400 0.000134           0.9998     0.8806   1.57e+04       2130          0   0.001294     0.00\n",
      "NOTE:     14   400 0.000134           0.8195     0.8601  1.237e+04       2012          0   0.001294     0.00\n",
      "NOTE:     15   400 0.000134           0.7736     0.8943  1.259e+04       1488          0   0.001294     0.00\n",
      "NOTE:     16   400 0.000134            0.926     0.8672  1.442e+04       2209          0   0.001294     0.00\n",
      "NOTE:     17   400 0.000134           0.8377     0.8375  1.249e+04       2424          0   0.001294     0.00\n",
      "NOTE:     18   400 0.000134           0.9499     0.8746  1.545e+04       2214          0   0.001294     0.00\n",
      "NOTE:     19   400 0.000134           0.9242     0.8084  1.162e+04       2753          0   0.001294     0.00\n",
      "NOTE:     20   400 0.000134           0.8865     0.8521  1.278e+04       2218          0   0.001294     0.00\n",
      "NOTE:     21   400 0.000134           0.8289     0.8654  1.392e+04       2165          0   0.001294     0.00\n",
      "NOTE:     22   400 0.000134           0.8665     0.8524  1.319e+04       2284          0   0.001294     0.00\n",
      "NOTE:     23   400 0.000134           0.8216     0.8561  1.319e+04       2217          0   0.001294     0.00\n",
      "NOTE:     24   400 0.000134           0.9824       0.89  1.659e+04       2050          0   0.001294     0.00\n",
      "NOTE:     25   400 0.000134           0.9805     0.8693  1.509e+04       2268          0   0.001294     0.00\n",
      "NOTE:     26   400 0.000134           0.8243      0.874  1.249e+04       1801          0   0.001295     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  109      0.0001          0.9113      0.861  3.778e+05  6.099e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000134           0.9724     0.8379  1.491e+04       2885          0   0.001295     0.00\n",
      "NOTE:      1   400 0.000134           0.9056      0.863  1.383e+04       2195          0   0.001295     0.00\n",
      "NOTE:      2   400 0.000134           0.8003     0.8786   1.33e+04       1837          0   0.001295     0.00\n",
      "NOTE:      3   400 0.000134           0.8314     0.8244  1.177e+04       2507          0   0.001295     0.00\n",
      "NOTE:      4   400 0.000134           0.8902     0.8434  1.376e+04       2555          0   0.001295     0.00\n",
      "NOTE:      5   400 0.000134           0.9981     0.8155   1.49e+04       3369          0   0.001295     0.00\n",
      "NOTE:      6   400 0.000134            0.841     0.8648  1.324e+04       2069          0   0.001295     0.00\n",
      "NOTE:      7   400 0.000134           0.8847     0.8371  1.333e+04       2593          0   0.001295     0.00\n",
      "NOTE:      8   400 0.000134           0.9285     0.8761  1.486e+04       2102          0   0.001295     0.00\n",
      "NOTE:      9   400 0.000134           0.9316      0.847  1.334e+04       2411          0   0.001295     0.00\n",
      "NOTE:     10   400 0.000134           0.7592     0.8801  1.256e+04       1711          0   0.001295     0.00\n",
      "NOTE:     11   400 0.000134           0.8693     0.8595  1.323e+04       2162          0   0.001295     0.00\n",
      "NOTE:     12   400 0.000134           0.8051     0.8536  1.363e+04       2339          0   0.001295     0.00\n",
      "NOTE:     13   400 0.000134           0.9465     0.9003  1.682e+04       1863          0   0.001295     0.00\n",
      "NOTE:     14   400 0.000134            0.782     0.9182  1.349e+04       1202          0   0.001295     0.00\n",
      "NOTE:     15   400 0.000134           0.8793     0.8477  1.439e+04       2584          0   0.001295     0.00\n",
      "NOTE:     16   400 0.000134           0.7104     0.8683  1.215e+04       1843          0   0.001295     0.00\n",
      "NOTE:     17   400 0.000134           0.7563     0.8491  1.145e+04       2035          0   0.001295     0.00\n",
      "NOTE:     18   400 0.000134           0.7962      0.852   1.18e+04       2050          0   0.001295     0.00\n",
      "NOTE:     19   400 0.000134            1.034     0.8196  1.477e+04       3251          0   0.001295     0.00\n",
      "NOTE:     20   400 0.000134           0.9744     0.8459  1.415e+04       2579          0   0.001295     0.00\n",
      "NOTE:     21   400 0.000134           0.9999     0.8405  1.553e+04       2947          0   0.001295     0.00\n",
      "NOTE:     22   400 0.000134           0.9203     0.8236  1.191e+04       2551          0   0.001295     0.00\n",
      "NOTE:     23   400 0.000134           0.9611     0.8605  1.523e+04       2468          0   0.001295     0.00\n",
      "NOTE:     24   400 0.000134           0.7647     0.8784  1.224e+04       1695          0   0.001295     0.00\n",
      "NOTE:     25   400 0.000134           0.9329     0.8687  1.505e+04       2274          0   0.001295     0.00\n",
      "NOTE:     26   400 0.000134           0.9025     0.8536  1.454e+04       2493          0   0.001295     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  110      0.0001          0.8807     0.8554  3.702e+05  6.257e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000134           0.9581     0.8795  1.649e+04       2259          0   0.001295     0.00\n",
      "NOTE:      1   400 0.000134           0.9796     0.8776  1.593e+04       2223          0   0.001295     0.00\n",
      "NOTE:      2   400 0.000134            0.945     0.8725  1.422e+04       2078          0   0.001295     0.00\n",
      "NOTE:      3   400 0.000134           0.9603     0.8586  1.478e+04       2434          0   0.001295     0.00\n",
      "NOTE:      4   400 0.000134           0.9248     0.8568  1.367e+04       2284          0   0.001295     0.00\n",
      "NOTE:      5   400 0.000134            0.703     0.8552  1.118e+04       1892          0   0.001295     0.00\n",
      "NOTE:      6   400 0.000134           0.8373     0.8568  1.343e+04       2245          0   0.001295     0.00\n",
      "NOTE:      7   400 0.000134           0.8355     0.8322  1.281e+04       2582          0   0.001295     0.00\n",
      "NOTE:      8   400 0.000134            1.014     0.8714  1.641e+04       2423          0   0.001295     0.00\n",
      "NOTE:      9   400 0.000134           0.8569     0.8413  1.249e+04       2356          0   0.001295     0.00\n",
      "NOTE:     10   400 0.000134           0.8058     0.8106  1.122e+04       2621          0   0.001295     0.00\n",
      "NOTE:     11   400 0.000134           0.8603     0.8434  1.288e+04       2391          0   0.001295     0.00\n",
      "NOTE:     12   400 0.000134           0.9573     0.8888   1.58e+04       1978          0   0.001295     0.00\n",
      "NOTE:     13   400 0.000134           0.8918     0.8852  1.452e+04       1883          0   0.001295     0.00\n",
      "NOTE:     14   400 0.000134           0.9202      0.894  1.545e+04       1832          0   0.001295     0.00\n",
      "NOTE:     15   400 0.000134            0.895     0.8456  1.431e+04       2613          0   0.001295     0.00\n",
      "NOTE:     16   400 0.000134            0.954      0.843  1.332e+04       2480          0   0.001295     0.00\n",
      "NOTE:     17   400 0.000134           0.8775     0.8487  1.257e+04       2240          0   0.001295     0.00\n",
      "NOTE:     18   400 0.000134            0.913     0.8558  1.444e+04       2432          0   0.001295     0.00\n",
      "NOTE:     19   400 0.000134           0.9654     0.8451  1.445e+04       2649          0   0.001295     0.00\n",
      "NOTE:     20   400 0.000134             0.93     0.8717  1.397e+04       2056          0   0.001295     0.00\n",
      "NOTE:     21   400 0.000134            1.001     0.8589  1.637e+04       2690          0   0.001295     0.00\n",
      "NOTE:     22   400 0.000134           0.9635      0.864   1.48e+04       2329          0   0.001295     0.00\n",
      "NOTE:     23   400 0.000134           0.8105     0.8502  1.244e+04       2192          0   0.001295     0.00\n",
      "NOTE:     24   400 0.000134           0.8826     0.8465  1.227e+04       2224          0   0.001295     0.00\n",
      "NOTE:     25   400 0.000134           0.9715     0.8478  1.431e+04       2569          0   0.001295     0.00\n",
      "NOTE:     26   400 0.000134           0.7888     0.8497  1.236e+04       2185          0   0.001295     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  111      0.0001          0.9038     0.8585  3.769e+05  6.214e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000134           0.8935     0.8667  1.285e+04       1976          0   0.001295     0.00\n",
      "NOTE:      1   400 0.000134           0.8376     0.8748  1.316e+04       1883          0   0.001295     0.00\n",
      "NOTE:      2   400 0.000134           0.8224     0.8859  1.382e+04       1780          0   0.001295     0.00\n",
      "NOTE:      3   400 0.000134           0.8604      0.868  1.253e+04       1905          0   0.001295     0.00\n",
      "NOTE:      4   400 0.000134           0.9583      0.862  1.503e+04       2406          0   0.001295     0.00\n",
      "NOTE:      5   400 0.000134           0.9362     0.8861  1.586e+04       2039          0   0.001295     0.00\n",
      "NOTE:      6   400 0.000134            0.802     0.8751  1.401e+04       1999          0   0.001295     0.00\n",
      "NOTE:      7   400 0.000134           0.9573     0.8678  1.416e+04       2157          0   0.001295     0.00\n",
      "NOTE:      8   400 0.000134             1.01     0.8361  1.464e+04       2870          0   0.001295     0.00\n",
      "NOTE:      9   400 0.000134           0.8152     0.8714  1.341e+04       1979          0   0.001295     0.00\n",
      "NOTE:     10   400 0.000134           0.9542     0.8897  1.564e+04       1939          0   0.001295     0.00\n",
      "NOTE:     11   400 0.000134           0.9116     0.8539  1.327e+04       2269          0   0.001295     0.00\n",
      "NOTE:     12   400 0.000134           0.8294     0.9131  1.525e+04       1451          0   0.001295     0.00\n",
      "NOTE:     13   400 0.000134             0.91     0.8833  1.506e+04       1989          0   0.001295     0.00\n",
      "NOTE:     14   400 0.000134           0.8076     0.8565  1.247e+04       2089          0   0.001295     0.00\n",
      "NOTE:     15   400 0.000134            0.962     0.8649  1.527e+04       2386          0   0.001296     0.00\n",
      "NOTE:     16   400 0.000134            1.002     0.8853   1.69e+04       2190          0   0.001296     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     17   400 0.000134           0.8709     0.8617  1.329e+04       2134          0   0.001296     0.00\n",
      "NOTE:     18   400 0.000134           0.8486     0.9055  1.539e+04       1605          0   0.001296     0.00\n",
      "NOTE:     19   400 0.000134           0.8217     0.8932   1.44e+04       1722          0   0.001296     0.00\n",
      "NOTE:     20   400 0.000134           0.8586     0.8277  1.277e+04       2659          0   0.001296     0.00\n",
      "NOTE:     21   400 0.000134           0.8263     0.9002  1.457e+04       1616          0   0.001296     0.00\n",
      "NOTE:     22   400 0.000134           0.8873     0.8805  1.533e+04       2080          0   0.001296     0.00\n",
      "NOTE:     23   400 0.000134           0.8278     0.8289  1.313e+04       2710          0   0.001296     0.00\n",
      "NOTE:     24   400 0.000134           0.7652     0.8872  1.261e+04       1603          0   0.001296     0.00\n",
      "NOTE:     25   400 0.000134           0.7992     0.8548  1.239e+04       2104          0   0.001296     0.00\n",
      "NOTE:     26   400 0.000134           0.9593     0.8428  1.354e+04       2526          0   0.001296     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  112      0.0001           0.879     0.8716  3.807e+05  5.607e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000134            1.046     0.8522  1.516e+04       2628          0   0.001296     0.00\n",
      "NOTE:      1   400 0.000134           0.8072     0.8703  1.242e+04       1851          0   0.001296     0.00\n",
      "NOTE:      2   400 0.000134           0.8496     0.8402  1.262e+04       2400          0   0.001296     0.00\n",
      "NOTE:      3   400 0.000134           0.9619     0.8543  1.641e+04       2799          0   0.001296     0.00\n",
      "NOTE:      4   400 0.000134           0.9349     0.8449  1.302e+04       2389          0   0.001296     0.00\n",
      "NOTE:      5   400 0.000134           0.7098     0.8559  1.115e+04       1877          0   0.001296     0.00\n",
      "NOTE:      6   400 0.000134           0.8586     0.8916  1.445e+04       1757          0   0.001296     0.00\n",
      "NOTE:      7   400 0.000134           0.9147     0.8598  1.409e+04       2297          0   0.001296     0.00\n",
      "NOTE:      8   400 0.000134           0.8092     0.8729  1.219e+04       1775          0   0.001296     0.00\n",
      "NOTE:      9   400 0.000134            1.012     0.8894  1.799e+04       2238          0   0.001296     0.00\n",
      "NOTE:     10   400 0.000134           0.9828     0.8893   1.68e+04       2092          0   0.001296     0.00\n",
      "NOTE:     11   400 0.000134           0.7755     0.8695  1.269e+04       1904          0   0.001296     0.00\n",
      "NOTE:     12   400 0.000134           0.7604     0.8571  1.175e+04       1960          0   0.001296     0.00\n",
      "NOTE:     13   400 0.000134            1.021     0.8423  1.595e+04       2985          0   0.001296     0.00\n",
      "NOTE:     14   400 0.000134           0.9365     0.8872   1.51e+04       1920          0   0.001297     0.00\n",
      "NOTE:     15   400 0.000134           0.8466     0.8436  1.232e+04       2284          0   0.001297     0.00\n",
      "NOTE:     16   400 0.000134           0.9188     0.8786  1.505e+04       2079          0   0.001297     0.00\n",
      "NOTE:     17   400 0.000134           0.8179     0.8413    1.2e+04       2263          0   0.001297     0.00\n",
      "NOTE:     18   400 0.000134           0.7626     0.8839  1.146e+04       1506          0   0.001297     0.00\n",
      "NOTE:     19   400 0.000134            0.852      0.832  1.206e+04       2434          0   0.001297     0.00\n",
      "NOTE:     20   400 0.000134           0.8559       0.84   1.32e+04       2514          0   0.001297     0.00\n",
      "NOTE:     21   400 0.000134           0.8753     0.8442  1.371e+04       2530          0   0.001297     0.00\n",
      "NOTE:     22   400 0.000134           0.7296     0.8514  1.205e+04       2104          0   0.001297     0.00\n",
      "NOTE:     23   400 0.000134            1.077     0.8518  1.608e+04       2797          0   0.001297     0.00\n",
      "NOTE:     24   400 0.000134           0.7834      0.874  1.418e+04       2045          0   0.001297     0.00\n",
      "NOTE:     25   400 0.000134           0.8355     0.8353  1.276e+04       2516          0   0.001297     0.00\n",
      "NOTE:     26   400 0.000134           0.8543     0.8533  1.325e+04       2279          0   0.001297     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  113      0.0001          0.8737       0.86  3.699e+05  6.022e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.9118     0.8414  1.363e+04       2569          0   0.001297     0.00\n",
      "NOTE:      1   400 0.000107           0.9034     0.8651  1.475e+04       2300          0   0.001297     0.00\n",
      "NOTE:      2   400 0.000107           0.9624     0.8557  1.522e+04       2566          0   0.001297     0.00\n",
      "NOTE:      3   400 0.000107           0.8596     0.8147  1.231e+04       2799          0   0.001297     0.00\n",
      "NOTE:      4   400 0.000107            1.054     0.8337  1.548e+04       3089          0   0.001297     0.00\n",
      "NOTE:      5   400 0.000107           0.8755     0.8133  1.294e+04       2971          0   0.001297     0.00\n",
      "NOTE:      6   400 0.000107           0.8553      0.893  1.424e+04       1706          0   0.001297     0.00\n",
      "NOTE:      7   400 0.000107           0.8606     0.8598   1.29e+04       2104          0   0.001297     0.00\n",
      "NOTE:      8   400 0.000107           0.9283     0.8265  1.324e+04       2779          0   0.001297     0.00\n",
      "NOTE:      9   400 0.000107           0.8439     0.8666  1.255e+04       1931          0   0.001297     0.00\n",
      "NOTE:     10   400 0.000107           0.7822      0.871  1.326e+04       1965          0   0.001297     0.00\n",
      "NOTE:     11   400 0.000107           0.8654     0.8718  1.335e+04       1963          0   0.001297     0.00\n",
      "NOTE:     12   400 0.000107           0.9046      0.841  1.274e+04       2409          0   0.001297     0.00\n",
      "NOTE:     13   400 0.000107           0.7358     0.8588  1.133e+04       1862          0   0.001297     0.00\n",
      "NOTE:     14   400 0.000107           0.8253     0.8832  1.328e+04       1756          0   0.001297     0.00\n",
      "NOTE:     15   400 0.000107           0.8233     0.8484  1.321e+04       2360          0   0.001297     0.00\n",
      "NOTE:     16   400 0.000107           0.9264     0.8635  1.434e+04       2266          0   0.001297     0.00\n",
      "NOTE:     17   400 0.000107           0.9712     0.8699  1.282e+04       1918          0   0.001297     0.00\n",
      "NOTE:     18   400 0.000107           0.9205     0.8286  1.278e+04       2644          0   0.001297     0.00\n",
      "NOTE:     19   400 0.000107           0.9116     0.8828   1.55e+04       2057          0   0.001297     0.00\n",
      "NOTE:     20   400 0.000107           0.8596     0.8613  1.403e+04       2260          0   0.001297     0.00\n",
      "NOTE:     21   400 0.000107           0.8719     0.8238  1.266e+04       2708          0   0.001297     0.00\n",
      "NOTE:     22   400 0.000107           0.9769     0.8052  1.416e+04       3426          0   0.001297     0.00\n",
      "NOTE:     23   400 0.000107           0.9476     0.8039  1.188e+04       2898          0   0.001297     0.00\n",
      "NOTE:     24   400 0.000107           0.8885     0.8698  1.536e+04       2300          0   0.001297     0.00\n",
      "NOTE:     25   400 0.000107           0.8922      0.876  1.351e+04       1911          0   0.001297     0.00\n",
      "NOTE:     26   400 0.000107           0.7224      0.904  1.229e+04       1305          0   0.001297     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  114      0.0001          0.8844     0.8527  3.638e+05  6.282e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.7588     0.8831   1.16e+04       1535          0   0.001297     0.00\n",
      "NOTE:      1   400 0.000107           0.8868     0.8427  1.285e+04       2399          0   0.001297     0.00\n",
      "NOTE:      2   400 0.000107           0.9479     0.8538  1.454e+04       2489          0   0.001297     0.00\n",
      "NOTE:      3   400 0.000107           0.8905     0.8808  1.445e+04       1956          0   0.001297     0.00\n",
      "NOTE:      4   400 0.000107           0.8712     0.8779  1.376e+04       1914          0   0.001298     0.00\n",
      "NOTE:      5   400 0.000107           0.9088     0.8779  1.619e+04       2253          0   0.001298     0.00\n",
      "NOTE:      6   400 0.000107           0.7804     0.8529  1.209e+04       2084          0   0.001298     0.00\n",
      "NOTE:      7   400 0.000107           0.9406     0.8383  1.398e+04       2696          0   0.001298     0.00\n",
      "NOTE:      8   400 0.000107           0.8005     0.8707  1.342e+04       1994          0   0.001298     0.00\n",
      "NOTE:      9   400 0.000107            0.823     0.8966  1.424e+04       1642          0   0.001298     0.00\n",
      "NOTE:     10   400 0.000107           0.9219     0.8795  1.585e+04       2171          0   0.001298     0.00\n",
      "NOTE:     11   400 0.000107           0.8201      0.888  1.312e+04       1654          0   0.001298     0.00\n",
      "NOTE:     12   400 0.000107           0.9156     0.8644  1.367e+04       2144          0   0.001298     0.00\n",
      "NOTE:     13   400 0.000107           0.8896     0.8977  1.425e+04       1624          0   0.001298     0.00\n",
      "NOTE:     14   400 0.000107           0.7894     0.8792  1.325e+04       1820          0   0.001298     0.00\n",
      "NOTE:     15   400 0.000107           0.7797     0.8837  1.378e+04       1814          0   0.001298     0.00\n",
      "NOTE:     16   400 0.000107           0.7204     0.8345  1.176e+04       2332          0   0.001298     0.00\n",
      "NOTE:     17   400 0.000107           0.9379     0.8549  1.513e+04       2568          0   0.001298     0.00\n",
      "NOTE:     18   400 0.000107           0.6934     0.8478  1.129e+04       2027          0   0.001298     0.00\n",
      "NOTE:     19   400 0.000107           0.8483     0.8757  1.455e+04       2065          0   0.001298     0.00\n",
      "NOTE:     20   400 0.000107            1.058     0.8669  1.739e+04       2671          0   0.001298     0.00\n",
      "NOTE:     21   400 0.000107            1.065     0.8491  1.617e+04       2872          0   0.001298     0.00\n",
      "NOTE:     22   400 0.000107           0.8642     0.8514  1.301e+04       2269          0   0.001298     0.00\n",
      "NOTE:     23   400 0.000107           0.8239     0.8436  1.247e+04       2311          0   0.001298     0.00\n",
      "NOTE:     24   400 0.000107           0.8532     0.8392  1.344e+04       2574          0   0.001298     0.00\n",
      "NOTE:     25   400 0.000107           0.8154     0.8017  1.299e+04       3212          0   0.001298     0.00\n",
      "NOTE:     26   400 0.000107           0.8309     0.8415  1.231e+04       2318          0   0.001298     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  115      0.0001          0.8605     0.8621  3.715e+05  5.941e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107            0.864     0.8702  1.435e+04       2141          0   0.001298     0.00\n",
      "NOTE:      1   400 0.000107           0.9141     0.8439  1.345e+04       2488          0   0.001298     0.00\n",
      "NOTE:      2   400 0.000107           0.9256      0.858  1.448e+04       2396          0   0.001298     0.00\n",
      "NOTE:      3   400 0.000107            1.038     0.8474  1.538e+04       2770          0   0.001298     0.00\n",
      "NOTE:      4   400 0.000107           0.9235     0.8466  1.345e+04       2437          0   0.001298     0.00\n",
      "NOTE:      5   400 0.000107           0.7987     0.8858  1.356e+04       1748          0   0.001298     0.00\n",
      "NOTE:      6   400 0.000107           0.9627     0.8845  1.491e+04       1947          0   0.001298     0.00\n",
      "NOTE:      7   400 0.000107           0.8792     0.8884  1.502e+04       1887          0   0.001298     0.00\n",
      "NOTE:      8   400 0.000107            1.033     0.8638  1.511e+04       2383          0   0.001298     0.00\n",
      "NOTE:      9   400 0.000107           0.8924      0.813  1.121e+04       2579          0   0.001298     0.00\n",
      "NOTE:     10   400 0.000107           0.9926     0.8578  1.564e+04       2592          0   0.001298     0.00\n",
      "NOTE:     11   400 0.000107            1.033     0.8358  1.599e+04       3142          0   0.001298     0.00\n",
      "NOTE:     12   400 0.000107           0.8854     0.8608  1.536e+04       2485          0   0.001298     0.00\n",
      "NOTE:     13   400 0.000107           0.8791     0.8532   1.31e+04       2254          0   0.001298     0.00\n",
      "NOTE:     14   400 0.000107           0.8458     0.8229   1.15e+04       2475          0   0.001298     0.00\n",
      "NOTE:     15   400 0.000107           0.8509     0.8719  1.338e+04       1965          0   0.001298     0.00\n",
      "NOTE:     16   400 0.000107           0.8342      0.883  1.456e+04       1929          0   0.001298     0.00\n",
      "NOTE:     17   400 0.000107           0.9295     0.8904  1.612e+04       1984          0   0.001298     0.00\n",
      "NOTE:     18   400 0.000107           0.8938     0.8934  1.522e+04       1816          0   0.001298     0.00\n",
      "NOTE:     19   400 0.000107           0.7968     0.8581  1.274e+04       2107          0   0.001298     0.00\n",
      "NOTE:     20   400 0.000107           0.9256     0.8688  1.453e+04       2194          0   0.001298     0.00\n",
      "NOTE:     21   400 0.000107           0.8267     0.8908  1.354e+04       1660          0   0.001298     0.00\n",
      "NOTE:     22   400 0.000107           0.7967      0.854  1.146e+04       1960          0   0.001298     0.00\n",
      "NOTE:     23   400 0.000107           0.7805     0.8875   1.31e+04       1660          0   0.001298     0.00\n",
      "NOTE:     24   400 0.000107           0.8903     0.8325  1.274e+04       2563          0   0.001299     0.00\n",
      "NOTE:     25   400 0.000107           0.7945     0.8675  1.377e+04       2104          0   0.001299     0.00\n",
      "NOTE:     26   400 0.000107           0.8884      0.871  1.606e+04       2379          0   0.001299     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  116      0.0001          0.8917     0.8635  3.798e+05  6.005e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.8536      0.848  1.381e+04       2475          0   0.001299     0.00\n",
      "NOTE:      1   400 0.000107           0.6977     0.8924  1.191e+04       1437          0   0.001299     0.00\n",
      "NOTE:      2   400 0.000107           0.9721     0.8555   1.42e+04       2398          0   0.001299     0.00\n",
      "NOTE:      3   400 0.000107           0.7597     0.8641  1.165e+04       1832          0   0.001299     0.00\n",
      "NOTE:      4   400 0.000107           0.7897     0.8987   1.53e+04       1724          0   0.001299     0.00\n",
      "NOTE:      5   400 0.000107           0.7837     0.8546  1.258e+04       2140          0   0.001299     0.00\n",
      "NOTE:      6   400 0.000107           0.8236     0.8739  1.329e+04       1917          0   0.001299     0.00\n",
      "NOTE:      7   400 0.000107           0.8762     0.8788  1.325e+04       1828          0   0.001299     0.00\n",
      "NOTE:      8   400 0.000107           0.7174     0.8585  1.133e+04       1868          0   0.001299     0.00\n",
      "NOTE:      9   400 0.000107           0.8903     0.8557  1.369e+04       2309          0   0.001299     0.00\n",
      "NOTE:     10   400 0.000107           0.9188     0.8827  1.608e+04       2136          0   0.001299     0.00\n",
      "NOTE:     11   400 0.000107           0.8339      0.895  1.399e+04       1641          0   0.001299     0.00\n",
      "NOTE:     12   400 0.000107           0.8037     0.8415  1.131e+04       2129          0   0.001299     0.00\n",
      "NOTE:     13   400 0.000107           0.8899     0.8537  1.335e+04       2288          0   0.001299     0.00\n",
      "NOTE:     14   400 0.000107            1.035     0.8642  1.687e+04       2651          0   0.001299     0.00\n",
      "NOTE:     15   400 0.000107            0.913     0.8474  1.343e+04       2418          0   0.001299     0.00\n",
      "NOTE:     16   400 0.000107           0.7789     0.8464  1.232e+04       2235          0   0.001299     0.00\n",
      "NOTE:     17   400 0.000107           0.8976     0.8286  1.375e+04       2843          0   0.001299     0.00\n",
      "NOTE:     18   400 0.000107           0.8598     0.8534  1.279e+04       2197          0   0.001299     0.00\n",
      "NOTE:     19   400 0.000107           0.8848     0.9072  1.566e+04       1603          0   0.001299     0.00\n",
      "NOTE:     20   400 0.000107            0.771     0.8181   1.17e+04       2601          0   0.001299     0.00\n",
      "NOTE:     21   400 0.000107           0.8786      0.872  1.423e+04       2088          0   0.001299     0.00\n",
      "NOTE:     22   400 0.000107           0.9279     0.8719  1.415e+04       2079          0   0.001299     0.00\n",
      "NOTE:     23   400 0.000107            1.003     0.8837  1.629e+04       2144          0   0.001299     0.00\n",
      "NOTE:     24   400 0.000107           0.8832     0.8573  1.346e+04       2240          0   0.001299     0.00\n",
      "NOTE:     25   400 0.000107           0.9718     0.8628  1.561e+04       2483          0   0.001299     0.00\n",
      "NOTE:     26   400 0.000107           0.7845     0.8736   1.33e+04       1924          0   0.001299     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  117      0.0001          0.8593      0.865  3.693e+05  5.763e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.8571     0.8937  1.421e+04       1690          0   0.001299     0.00\n",
      "NOTE:      1   400 0.000107           0.8013     0.8525  1.382e+04       2391          0   0.001299     0.00\n",
      "NOTE:      2   400 0.000107           0.8531     0.8605  1.372e+04       2224          0   0.001299     0.00\n",
      "NOTE:      3   400 0.000107           0.9459     0.8613  1.441e+04       2320          0   0.001299     0.00\n",
      "NOTE:      4   400 0.000107           0.9017     0.8336  1.399e+04       2791          0   0.001299     0.00\n",
      "NOTE:      5   400 0.000107           0.7739     0.8837  1.311e+04       1725          0   0.001299     0.00\n",
      "NOTE:      6   400 0.000107           0.9344     0.8471  1.407e+04       2539          0   0.001299     0.00\n",
      "NOTE:      7   400 0.000107            0.818     0.8411   1.25e+04       2362          0   0.001299     0.00\n",
      "NOTE:      8   400 0.000107           0.8472     0.8889  1.514e+04       1893          0   0.001299     0.00\n",
      "NOTE:      9   400 0.000107           0.8121     0.8536  1.293e+04       2219          0   0.001299     0.00\n",
      "NOTE:     10   400 0.000107           0.9635     0.8314  1.419e+04       2879          0   0.001299     0.00\n",
      "NOTE:     11   400 0.000107           0.9243     0.8388  1.388e+04       2666          0   0.001299     0.00\n",
      "NOTE:     12   400 0.000107            0.875     0.8874  1.495e+04       1898          0   0.001299     0.00\n",
      "NOTE:     13   400 0.000107           0.8887     0.8078  1.244e+04       2959          0   0.001299     0.00\n",
      "NOTE:     14   400 0.000107           0.9291     0.8587  1.475e+04       2427          0   0.001299     0.00\n",
      "NOTE:     15   400 0.000107           0.7464     0.9045  1.353e+04       1428          0   0.001299     0.00\n",
      "NOTE:     16   400 0.000107           0.7855     0.8639  1.273e+04       2005          0   0.001299     0.00\n",
      "NOTE:     17   400 0.000107           0.8404     0.8328  1.257e+04       2522          0   0.001299     0.00\n",
      "NOTE:     18   400 0.000107            0.869     0.8258  1.257e+04       2651          0   0.001299     0.00\n",
      "NOTE:     19   400 0.000107           0.8605     0.8852  1.462e+04       1896          0   0.001299     0.00\n",
      "NOTE:     20   400 0.000107           0.8426     0.8177  1.254e+04       2796          0   0.001299     0.00\n",
      "NOTE:     21   400 0.000107            1.013      0.831  1.548e+04       3150          0   0.001299     0.00\n",
      "NOTE:     22   400 0.000107           0.9869     0.8924   1.62e+04       1954          0   0.001299     0.00\n",
      "NOTE:     23   400 0.000107           0.8545     0.8448  1.409e+04       2589          0   0.001299     0.00\n",
      "NOTE:     24   400 0.000107           0.8895      0.857  1.368e+04       2283          0   0.001299     0.00\n",
      "NOTE:     25   400 0.000107           0.7499     0.9011  1.292e+04       1418          0   0.001299     0.00\n",
      "NOTE:     26   400 0.000107           0.8282     0.8814  1.408e+04       1894          0   0.001299     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  118      0.0001          0.8663     0.8584  3.731e+05  6.157e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.8263     0.8756  1.311e+04       1862          0     0.0013     0.00\n",
      "NOTE:      1   400 0.000107            0.796     0.8534  1.302e+04       2238          0     0.0013     0.00\n",
      "NOTE:      2   400 0.000107           0.7773     0.8439  1.208e+04       2234          0     0.0013     0.00\n",
      "NOTE:      3   400 0.000107           0.8665     0.8673  1.368e+04       2093          0     0.0013     0.00\n",
      "NOTE:      4   400 0.000107           0.8877     0.8917  1.436e+04       1744          0     0.0013     0.00\n",
      "NOTE:      5   400 0.000107           0.8308     0.8355  1.365e+04       2688          0     0.0013     0.00\n",
      "NOTE:      6   400 0.000107           0.8954     0.8746  1.573e+04       2255          0     0.0013     0.00\n",
      "NOTE:      7   400 0.000107            0.867     0.8827  1.604e+04       2132          0     0.0013     0.00\n",
      "NOTE:      8   400 0.000107           0.9532     0.8883  1.549e+04       1948          0     0.0013     0.00\n",
      "NOTE:      9   400 0.000107           0.8792     0.8494  1.311e+04       2323          0     0.0013     0.00\n",
      "NOTE:     10   400 0.000107           0.9987     0.8303  1.415e+04       2893          0     0.0013     0.00\n",
      "NOTE:     11   400 0.000107           0.9869     0.8584  1.466e+04       2419          0     0.0013     0.00\n",
      "NOTE:     12   400 0.000107           0.9351     0.8471  1.379e+04       2490          0     0.0013     0.00\n",
      "NOTE:     13   400 0.000107           0.8593     0.8833  1.318e+04       1742          0     0.0013     0.00\n",
      "NOTE:     14   400 0.000107           0.6865     0.8617  1.115e+04       1789          0     0.0013     0.00\n",
      "NOTE:     15   400 0.000107           0.8049     0.8799  1.286e+04       1755          0     0.0013     0.00\n",
      "NOTE:     16   400 0.000107           0.8357     0.8819   1.25e+04       1673          0     0.0013     0.00\n",
      "NOTE:     17   400 0.000107           0.9088     0.8086  1.209e+04       2862          0     0.0013     0.00\n",
      "NOTE:     18   400 0.000107           0.8237     0.8869  1.365e+04       1740          0     0.0013     0.00\n",
      "NOTE:     19   400 0.000107           0.8589     0.8315  1.217e+04       2466          0     0.0013     0.00\n",
      "NOTE:     20   400 0.000107           0.7977      0.869  1.278e+04       1926          0     0.0013     0.00\n",
      "NOTE:     21   400 0.000107           0.9038     0.8203  1.123e+04       2460          0     0.0013     0.00\n",
      "NOTE:     22   400 0.000107           0.7836     0.9014  1.348e+04       1475          0     0.0013     0.00\n",
      "NOTE:     23   400 0.000107           0.9507     0.8858  1.426e+04       1838          0     0.0013     0.00\n",
      "NOTE:     24   400 0.000107           0.9103     0.8976  1.529e+04       1744          0     0.0013     0.00\n",
      "NOTE:     25   400 0.000107           0.9354     0.8087  1.267e+04       2997          0     0.0013     0.00\n",
      "NOTE:     26   400 0.000107            0.787     0.8811  1.281e+04       1729          0     0.0013     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  119      0.0001          0.8647     0.8632   3.63e+05  5.752e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.7574      0.888  1.225e+04       1545          0     0.0013     0.00\n",
      "NOTE:      1   400 0.000107           0.8899     0.8795  1.408e+04       1929          0     0.0013     0.00\n",
      "NOTE:      2   400 0.000107           0.7991     0.8617  1.244e+04       1995          0     0.0013     0.00\n",
      "NOTE:      3   400 0.000107           0.7628     0.8639   1.11e+04       1748          0     0.0013     0.00\n",
      "NOTE:      4   400 0.000107           0.9502     0.8701  1.502e+04       2242          0     0.0013     0.00\n",
      "NOTE:      5   400 0.000107           0.8643     0.8352  1.237e+04       2441          0     0.0013     0.00\n",
      "NOTE:      6   400 0.000107            1.016     0.8644  1.582e+04       2482          0     0.0013     0.00\n",
      "NOTE:      7   400 0.000107            0.811     0.8715  1.267e+04       1868          0     0.0013     0.00\n",
      "NOTE:      8   400 0.000107           0.9298       0.87    1.6e+04       2390          0     0.0013     0.00\n",
      "NOTE:      9   400 0.000107           0.8799     0.8505  1.412e+04       2482          0     0.0013     0.00\n",
      "NOTE:     10   400 0.000107           0.8276       0.81  1.202e+04       2820          0     0.0013     0.00\n",
      "NOTE:     11   400 0.000107           0.8271     0.8653   1.32e+04       2055          0     0.0013     0.00\n",
      "NOTE:     12   400 0.000107            1.023     0.8692  1.774e+04       2670          0     0.0013     0.00\n",
      "NOTE:     13   400 0.000107           0.9077     0.8901  1.581e+04       1952          0     0.0013     0.00\n",
      "NOTE:     14   400 0.000107           0.8901     0.8559  1.561e+04       2628          0     0.0013     0.00\n",
      "NOTE:     15   400 0.000107           0.8904     0.8522  1.374e+04       2382          0     0.0013     0.00\n",
      "NOTE:     16   400 0.000107           0.9275     0.8496  1.441e+04       2550          0     0.0013     0.00\n",
      "NOTE:     17   400 0.000107            1.016     0.8674  1.496e+04       2287          0     0.0013     0.00\n",
      "NOTE:     18   400 0.000107           0.8037     0.8418  1.272e+04       2390          0     0.0013     0.00\n",
      "NOTE:     19   400 0.000107           0.9323     0.8664  1.403e+04       2163          0     0.0013     0.00\n",
      "NOTE:     20   400 0.000107           0.7035     0.8771  1.089e+04       1526          0     0.0013     0.00\n",
      "NOTE:     21   400 0.000107           0.8388     0.8243  1.095e+04       2334          0     0.0013     0.00\n",
      "NOTE:     22   400 0.000107           0.9688     0.8483  1.375e+04       2458          0     0.0013     0.00\n",
      "NOTE:     23   400 0.000107           0.9585     0.8621  1.479e+04       2366          0     0.0013     0.00\n",
      "NOTE:     24   400 0.000107            1.005     0.8685  1.566e+04       2370          0     0.0013     0.00\n",
      "NOTE:     25   400 0.000107           0.8593      0.834  1.239e+04       2466          0     0.0013     0.00\n",
      "NOTE:     26   400 0.000107           0.8033     0.8859  1.319e+04       1699          0     0.0013     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  120      0.0001          0.8831     0.8605  3.717e+05  6.024e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.6877     0.8868  1.198e+04       1529          0     0.0013     0.00\n",
      "NOTE:      1   400 0.000107           0.7657     0.8704  1.159e+04       1726          0     0.0013     0.00\n",
      "NOTE:      2   400 0.000107           0.9461     0.8625  1.457e+04       2322          0   0.001301     0.00\n",
      "NOTE:      3   400 0.000107           0.9306     0.8811  1.401e+04       1891          0   0.001301     0.00\n",
      "NOTE:      4   400 0.000107            0.957     0.8418  1.424e+04       2676          0   0.001301     0.00\n",
      "NOTE:      5   400 0.000107            1.027       0.87   1.61e+04       2405          0   0.001301     0.00\n",
      "NOTE:      6   400 0.000107           0.7624     0.8422   1.16e+04       2172          0   0.001301     0.00\n",
      "NOTE:      7   400 0.000107           0.7017     0.8797  1.092e+04       1493          0   0.001301     0.00\n",
      "NOTE:      8   400 0.000107           0.7322     0.8451  1.179e+04       2161          0   0.001301     0.00\n",
      "NOTE:      9   400 0.000107            0.858      0.881  1.382e+04       1866          0   0.001301     0.00\n",
      "NOTE:     10   400 0.000107           0.8709     0.8728   1.41e+04       2056          0   0.001301     0.00\n",
      "NOTE:     11   400 0.000107           0.9724     0.8985  1.628e+04       1839          0   0.001301     0.00\n",
      "NOTE:     12   400 0.000107           0.7778     0.8883  1.301e+04       1635          0   0.001301     0.00\n",
      "NOTE:     13   400 0.000107           0.8142     0.8373  1.289e+04       2506          0   0.001301     0.00\n",
      "NOTE:     14   400 0.000107           0.7621     0.8537  1.184e+04       2029          0   0.001301     0.00\n",
      "NOTE:     15   400 0.000107           0.8246     0.8495   1.29e+04       2285          0   0.001301     0.00\n",
      "NOTE:     16   400 0.000107           0.8624     0.8814  1.278e+04       1721          0   0.001301     0.00\n",
      "NOTE:     17   400 0.000107           0.8611     0.8545  1.298e+04       2211          0   0.001301     0.00\n",
      "NOTE:     18   400 0.000107           0.8135     0.8327  1.236e+04       2483          0   0.001301     0.00\n",
      "NOTE:     19   400 0.000107           0.6963     0.8902  1.251e+04       1543          0   0.001301     0.00\n",
      "NOTE:     20   400 0.000107           0.8411      0.834  1.109e+04       2207          0   0.001301     0.00\n",
      "NOTE:     21   400 0.000107           0.6979     0.8533       9966       1714          0   0.001301     0.00\n",
      "NOTE:     22   400 0.000107            1.016     0.8199  1.466e+04       3220          0   0.001301     0.00\n",
      "NOTE:     23   400 0.000107           0.8139     0.8773  1.255e+04       1756          0   0.001301     0.00\n",
      "NOTE:     24   400 0.000107           0.9882     0.8338  1.376e+04       2742          0   0.001301     0.00\n",
      "NOTE:     25   400 0.000107           0.8187     0.9032  1.343e+04       1439          0   0.001301     0.00\n",
      "NOTE:     26   400 0.000107           0.9757     0.8775  1.586e+04       2213          0   0.001301     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  121      0.0001          0.8435     0.8636  3.536e+05  5.584e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.8216     0.8495  1.245e+04       2206          0   0.001301     0.00\n",
      "NOTE:      1   400 0.000107           0.9085      0.889  1.423e+04       1777          0   0.001301     0.00\n",
      "NOTE:      2   400 0.000107           0.9522     0.8628  1.508e+04       2398          0   0.001301     0.00\n",
      "NOTE:      3   400 0.000107           0.7078     0.8345  1.179e+04       2338          0   0.001301     0.00\n",
      "NOTE:      4   400 0.000107           0.7197     0.8478       9678       1737          0   0.001301     0.00\n",
      "NOTE:      5   400 0.000107           0.8943     0.8808  1.361e+04       1842          0   0.001301     0.00\n",
      "NOTE:      6   400 0.000107           0.9257     0.8843  1.591e+04       2081          0   0.001301     0.00\n",
      "NOTE:      7   400 0.000107           0.7393     0.8854  1.202e+04       1556          0   0.001301     0.00\n",
      "NOTE:      8   400 0.000107           0.8413     0.8503  1.361e+04       2397          0   0.001301     0.00\n",
      "NOTE:      9   400 0.000107           0.6704     0.8788  1.195e+04       1648          0   0.001301     0.00\n",
      "NOTE:     10   400 0.000107           0.8716     0.8396  1.307e+04       2497          0   0.001301     0.00\n",
      "NOTE:     11   400 0.000107           0.8957     0.8499  1.368e+04       2417          0   0.001301     0.00\n",
      "NOTE:     12   400 0.000107           0.8622     0.8255  1.279e+04       2702          0   0.001301     0.00\n",
      "NOTE:     13   400 0.000107           0.8526     0.8842  1.415e+04       1853          0   0.001301     0.00\n",
      "NOTE:     14   400 0.000107           0.9305     0.8502  1.356e+04       2389          0   0.001301     0.00\n",
      "NOTE:     15   400 0.000107           0.8775     0.8585  1.427e+04       2352          0   0.001301     0.00\n",
      "NOTE:     16   400 0.000107           0.7724     0.8786  1.111e+04       1535          0   0.001301     0.00\n",
      "NOTE:     17   400 0.000107           0.9876     0.8482  1.484e+04       2655          0   0.001301     0.00\n",
      "NOTE:     18   400 0.000107           0.8425     0.8315  1.188e+04       2407          0   0.001301     0.00\n",
      "NOTE:     19   400 0.000107           0.9782     0.8084  1.452e+04       3441          0   0.001301     0.00\n",
      "NOTE:     20   400 0.000107           0.8236     0.8655   1.17e+04       1818          0   0.001301     0.00\n",
      "NOTE:     21   400 0.000107           0.7695     0.8036  1.082e+04       2644          0   0.001301     0.00\n",
      "NOTE:     22   400 0.000107           0.9215     0.8576  1.509e+04       2506          0   0.001301     0.00\n",
      "NOTE:     23   400 0.000107           0.7474     0.8907  1.216e+04       1491          0   0.001301     0.00\n",
      "NOTE:     24   400 0.000107           0.8844     0.8692  1.333e+04       2005          0   0.001301     0.00\n",
      "NOTE:     25   400 0.000107           0.7666     0.8497  1.156e+04       2044          0   0.001301     0.00\n",
      "NOTE:     26   400 0.000107            0.876     0.8809  1.478e+04       1998          0   0.001301     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  122      0.0001           0.846     0.8576  3.536e+05  5.873e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.8789     0.8754  1.414e+04       2011          0   0.001301     0.00\n",
      "NOTE:      1   400 0.000107           0.9838     0.8922  1.594e+04       1926          0   0.001301     0.00\n",
      "NOTE:      2   400 0.000107           0.8706     0.8694  1.332e+04       2000          0   0.001301     0.00\n",
      "NOTE:      3   400 0.000107           0.8846     0.8884  1.438e+04       1806          0   0.001301     0.00\n",
      "NOTE:      4   400 0.000107            1.077     0.8448   1.53e+04       2809          0   0.001301     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:      5   400 0.000107           0.8357     0.9143  1.544e+04       1447          0   0.001301     0.00\n",
      "NOTE:      6   400 0.000107           0.9436     0.8373  1.378e+04       2677          0   0.001301     0.00\n",
      "NOTE:      7   400 0.000107            1.029     0.8572  1.605e+04       2675          0   0.001301     0.00\n",
      "NOTE:      8   400 0.000107             1.09     0.8575  1.637e+04       2719          0   0.001301     0.00\n",
      "NOTE:      9   400 0.000107           0.8087     0.8615  1.372e+04       2206          0   0.001301     0.00\n",
      "NOTE:     10   400 0.000107           0.7837     0.8567  1.194e+04       1997          0   0.001301     0.00\n",
      "NOTE:     11   400 0.000107           0.9232     0.8592  1.444e+04       2366          0   0.001301     0.00\n",
      "NOTE:     12   400 0.000107           0.8686     0.8585  1.343e+04       2214          0   0.001301     0.00\n",
      "NOTE:     13   400 0.000107           0.9545     0.8951   1.63e+04       1911          0   0.001301     0.00\n",
      "NOTE:     14   400 0.000107             1.13     0.8723  1.644e+04       2406          0   0.001301     0.00\n",
      "NOTE:     15   400 0.000107            0.838     0.8366  1.252e+04       2447          0   0.001302     0.00\n",
      "NOTE:     16   400 0.000107           0.8942     0.8517  1.495e+04       2603          0   0.001302     0.00\n",
      "NOTE:     17   400 0.000107            1.012     0.8193  1.519e+04       3352          0   0.001302     0.00\n",
      "NOTE:     18   400 0.000107            0.774     0.8781  1.317e+04       1829          0   0.001302     0.00\n",
      "NOTE:     19   400 0.000107           0.9364     0.8741  1.462e+04       2106          0   0.001302     0.00\n",
      "NOTE:     20   400 0.000107           0.8156     0.8391  1.266e+04       2429          0   0.001302     0.00\n",
      "NOTE:     21   400 0.000107           0.8534     0.8537  1.233e+04       2112          0   0.001302     0.00\n",
      "NOTE:     22   400 0.000107           0.9596     0.8676  1.692e+04       2581          0   0.001302     0.00\n",
      "NOTE:     23   400 0.000107           0.8606     0.8935  1.538e+04       1833          0   0.001302     0.00\n",
      "NOTE:     24   400 0.000107           0.8493     0.8569  1.368e+04       2284          0   0.001302     0.00\n",
      "NOTE:     25   400 0.000107           0.9025     0.7935  1.243e+04       3236          0   0.001302     0.00\n",
      "NOTE:     26   400 0.000107            0.888     0.8138  1.277e+04       2920          0   0.001302     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  123      0.0001          0.9128     0.8604  3.876e+05   6.29e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.9317     0.8891  1.531e+04       1909          0   0.001302     0.00\n",
      "NOTE:      1   400 0.000107           0.9248     0.8915  1.492e+04       1816          0   0.001302     0.00\n",
      "NOTE:      2   400 0.000107           0.8589     0.8912  1.376e+04       1679          0   0.001302     0.00\n",
      "NOTE:      3   400 0.000107           0.9756     0.8696  1.551e+04       2326          0   0.001302     0.00\n",
      "NOTE:      4   400 0.000107           0.8358     0.8179  1.237e+04       2753          0   0.001302     0.00\n",
      "NOTE:      5   400 0.000107           0.9089     0.8525  1.433e+04       2479          0   0.001302     0.00\n",
      "NOTE:      6   400 0.000107           0.9145     0.8106  1.334e+04       3116          0   0.001302     0.00\n",
      "NOTE:      7   400 0.000107            0.876     0.8486  1.336e+04       2384          0   0.001302     0.00\n",
      "NOTE:      8   400 0.000107           0.8644     0.8518  1.352e+04       2353          0   0.001302     0.00\n",
      "NOTE:      9   400 0.000107           0.8841      0.896   1.48e+04       1717          0   0.001302     0.00\n",
      "NOTE:     10   400 0.000107           0.9494      0.818  1.243e+04       2764          0   0.001302     0.00\n",
      "NOTE:     11   400 0.000107           0.8936     0.8514  1.345e+04       2347          0   0.001302     0.00\n",
      "NOTE:     12   400 0.000107           0.9023     0.8832  1.477e+04       1953          0   0.001302     0.00\n",
      "NOTE:     13   400 0.000107           0.9348     0.8277  1.419e+04       2953          0   0.001302     0.00\n",
      "NOTE:     14   400 0.000107           0.8759     0.8305  1.364e+04       2784          0   0.001302     0.00\n",
      "NOTE:     15   400 0.000107           0.9622       0.87  1.565e+04       2338          0   0.001302     0.00\n",
      "NOTE:     16   400 0.000107           0.8948     0.8835  1.426e+04       1881          0   0.001302     0.00\n",
      "NOTE:     17   400 0.000107           0.8754     0.8443  1.368e+04       2523          0   0.001302     0.00\n",
      "NOTE:     18   400 0.000107           0.7246     0.8934  1.111e+04       1326          0   0.001302     0.00\n",
      "NOTE:     19   400 0.000107           0.8226     0.8445  1.299e+04       2392          0   0.001302     0.00\n",
      "NOTE:     20   400 0.000107            1.021     0.8384  1.503e+04       2897          0   0.001302     0.00\n",
      "NOTE:     21   400 0.000107            1.056     0.8158  1.415e+04       3195          0   0.001302     0.00\n",
      "NOTE:     22   400 0.000107           0.7683      0.874  1.134e+04       1635          0   0.001302     0.00\n",
      "NOTE:     23   400 0.000107           0.9666     0.9065  1.662e+04       1714          0   0.001302     0.00\n",
      "NOTE:     24   400 0.000107           0.9347     0.8413  1.396e+04       2633          0   0.001302     0.00\n",
      "NOTE:     25   400 0.000107            1.112      0.911   2.03e+04       1984          0   0.001302     0.00\n",
      "NOTE:     26   400 0.000107            0.862     0.8106  1.106e+04       2583          0   0.001302     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  124      0.0001          0.9086     0.8588  3.798e+05  6.243e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.7535     0.8281  1.159e+04       2405          0   0.001302     0.00\n",
      "NOTE:      1   400 0.000107           0.9351     0.8073  1.317e+04       3144          0   0.001302     0.00\n",
      "NOTE:      2   400 0.000107           0.7975     0.8872  1.393e+04       1770          0   0.001302     0.00\n",
      "NOTE:      3   400 0.000107           0.7312     0.8745  1.281e+04       1839          0   0.001302     0.00\n",
      "NOTE:      4   400 0.000107           0.8691     0.8568  1.338e+04       2236          0   0.001302     0.00\n",
      "NOTE:      5   400 0.000107           0.9367     0.8857  1.452e+04       1874          0   0.001302     0.00\n",
      "NOTE:      6   400 0.000107           0.9837     0.8777   1.56e+04       2175          0   0.001302     0.00\n",
      "NOTE:      7   400 0.000107           0.9206     0.8686  1.436e+04       2173          0   0.001302     0.00\n",
      "NOTE:      8   400 0.000107           0.9157     0.8305  1.274e+04       2599          0   0.001302     0.00\n",
      "NOTE:      9   400 0.000107           0.9693     0.8492  1.421e+04       2524          0   0.001302     0.00\n",
      "NOTE:     10   400 0.000107           0.7717     0.8711  1.232e+04       1823          0   0.001302     0.00\n",
      "NOTE:     11   400 0.000107           0.7736     0.8698  1.059e+04       1585          0   0.001302     0.00\n",
      "NOTE:     12   400 0.000107           0.8958     0.8503  1.372e+04       2416          0   0.001302     0.00\n",
      "NOTE:     13   400 0.000107           0.9017     0.8791   1.48e+04       2035          0   0.001302     0.00\n",
      "NOTE:     14   400 0.000107           0.9359     0.8561  1.384e+04       2326          0   0.001303     0.00\n",
      "NOTE:     15   400 0.000107           0.7553     0.8606  1.222e+04       1979          0   0.001303     0.00\n",
      "NOTE:     16   400 0.000107           0.9449     0.8394  1.393e+04       2666          0   0.001303     0.00\n",
      "NOTE:     17   400 0.000107           0.8906     0.8268  1.424e+04       2984          0   0.001303     0.00\n",
      "NOTE:     18   400 0.000107           0.9575     0.8391  1.407e+04       2699          0   0.001303     0.00\n",
      "NOTE:     19   400 0.000107           0.8746     0.8432  1.303e+04       2423          0   0.001303     0.00\n",
      "NOTE:     20   400 0.000107           0.7928     0.9011  1.268e+04       1391          0   0.001303     0.00\n",
      "NOTE:     21   400 0.000107           0.8019     0.8669  1.271e+04       1951          0   0.001303     0.00\n",
      "NOTE:     22   400 0.000107           0.8063     0.8784  1.341e+04       1857          0   0.001303     0.00\n",
      "NOTE:     23   400 0.000107           0.8169     0.9074  1.383e+04       1411          0   0.001303     0.00\n",
      "NOTE:     24   400 0.000107           0.8921      0.869  1.572e+04       2371          0   0.001303     0.00\n",
      "NOTE:     25   400 0.000107           0.7807     0.8409  1.326e+04       2508          0   0.001303     0.00\n",
      "NOTE:     26   400 0.000107           0.7436     0.8347  1.089e+04       2157          0   0.001303     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  125      0.0001          0.8573     0.8591  3.616e+05  5.932e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107            0.828     0.8465   1.28e+04       2320          0   0.001303     0.00\n",
      "NOTE:      1   400 0.000107           0.7687     0.8416  1.085e+04       2042          0   0.001303     0.00\n",
      "NOTE:      2   400 0.000107           0.9449     0.8587  1.413e+04       2325          0   0.001303     0.00\n",
      "NOTE:      3   400 0.000107           0.7886      0.869    1.3e+04       1959          0   0.001303     0.00\n",
      "NOTE:      4   400 0.000107           0.9849     0.8816  1.576e+04       2116          0   0.001303     0.00\n",
      "NOTE:      5   400 0.000107           0.9312     0.8549  1.389e+04       2358          0   0.001303     0.00\n",
      "NOTE:      6   400 0.000107           0.9188     0.8612  1.355e+04       2184          0   0.001303     0.00\n",
      "NOTE:      7   400 0.000107           0.6957     0.8666  1.132e+04       1742          0   0.001303     0.00\n",
      "NOTE:      8   400 0.000107           0.7815     0.8981  1.318e+04       1496          0   0.001303     0.00\n",
      "NOTE:      9   400 0.000107           0.7485     0.8592  1.137e+04       1864          0   0.001303     0.00\n",
      "NOTE:     10   400 0.000107           0.7738     0.8447  1.288e+04       2368          0   0.001303     0.00\n",
      "NOTE:     11   400 0.000107           0.7923     0.8599  1.164e+04       1897          0   0.001303     0.00\n",
      "NOTE:     12   400 0.000107           0.6985     0.8895  1.151e+04       1429          0   0.001303     0.00\n",
      "NOTE:     13   400 0.000107            1.067     0.8644  1.679e+04       2634          0   0.001303     0.00\n",
      "NOTE:     14   400 0.000107           0.8352     0.8709  1.337e+04       1982          0   0.001303     0.00\n",
      "NOTE:     15   400 0.000107           0.9653     0.8563  1.505e+04       2526          0   0.001303     0.00\n",
      "NOTE:     16   400 0.000107             0.93     0.8707  1.402e+04       2082          0   0.001303     0.00\n",
      "NOTE:     17   400 0.000107           0.7546     0.8628  1.119e+04       1779          0   0.001303     0.00\n",
      "NOTE:     18   400 0.000107           0.9342     0.8602  1.524e+04       2476          0   0.001303     0.00\n",
      "NOTE:     19   400 0.000107           0.6925     0.8718  1.117e+04       1642          0   0.001303     0.00\n",
      "NOTE:     20   400 0.000107            1.072     0.8073  1.477e+04       3526          0   0.001303     0.00\n",
      "NOTE:     21   400 0.000107           0.9416     0.8745  1.487e+04       2133          0   0.001303     0.00\n",
      "NOTE:     22   400 0.000107           0.8614      0.865  1.323e+04       2065          0   0.001303     0.00\n",
      "NOTE:     23   400 0.000107           0.9639     0.8163  1.325e+04       2981          0   0.001303     0.00\n",
      "NOTE:     24   400 0.000107           0.9435     0.8284  1.318e+04       2730          0   0.001303     0.00\n",
      "NOTE:     25   400 0.000107           0.8648     0.8311  1.298e+04       2638          0   0.001303     0.00\n",
      "NOTE:     26   400 0.000107           0.7443     0.8825  1.124e+04       1496          0   0.001303     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  126      0.0001          0.8602     0.8583  3.562e+05  5.879e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000107           0.9774     0.8192  1.233e+04       2720          0   0.001303     0.00\n",
      "NOTE:      1   400 0.000107           0.9083     0.8847  1.449e+04       1888          0   0.001303     0.00\n",
      "NOTE:      2   400 0.000107           0.8332     0.8777    1.4e+04       1951          0   0.001303     0.00\n",
      "NOTE:      3   400 0.000107            0.804     0.8528  1.181e+04       2038          0   0.001303     0.00\n",
      "NOTE:      4   400 0.000107           0.9206     0.8455  1.452e+04       2653          0   0.001303     0.00\n",
      "NOTE:      5   400 0.000107           0.8867     0.8597  1.492e+04       2435          0   0.001303     0.00\n",
      "NOTE:      6   400 0.000107           0.7746     0.8528  1.134e+04       1956          0   0.001303     0.00\n",
      "NOTE:      7   400 0.000107            0.843     0.8723  1.344e+04       1967          0   0.001303     0.00\n",
      "NOTE:      8   400 0.000107           0.8405     0.8291  1.242e+04       2560          0   0.001303     0.00\n",
      "NOTE:      9   400 0.000107           0.8648     0.8141  1.316e+04       3003          0   0.001303     0.00\n",
      "NOTE:     10   400 0.000107           0.8723      0.895  1.523e+04       1787          0   0.001303     0.00\n",
      "NOTE:     11   400 0.000107           0.9137     0.8737  1.488e+04       2150          0   0.001303     0.00\n",
      "NOTE:     12   400 0.000107           0.9422     0.8638   1.42e+04       2239          0   0.001303     0.00\n",
      "NOTE:     13   400 0.000107           0.7083     0.8498  1.061e+04       1875          0   0.001303     0.00\n",
      "NOTE:     14   400 0.000107            0.802     0.8627  1.256e+04       1998          0   0.001303     0.00\n",
      "NOTE:     15   400 0.000107           0.8623     0.8574  1.372e+04       2282          0   0.001303     0.00\n",
      "NOTE:     16   400 0.000107           0.8417     0.8935  1.352e+04       1612          0   0.001303     0.00\n",
      "NOTE:     17   400 0.000107           0.8574     0.8912  1.418e+04       1731          0   0.001303     0.00\n",
      "NOTE:     18   400 0.000107           0.9279     0.8844  1.514e+04       1979          0   0.001303     0.00\n",
      "NOTE:     19   400 0.000107           0.7755     0.8733  1.194e+04       1732          0   0.001303     0.00\n",
      "NOTE:     20   400 0.000107           0.9409     0.8376  1.313e+04       2546          0   0.001303     0.00\n",
      "NOTE:     21   400 0.000107           0.8562     0.8506  1.325e+04       2328          0   0.001304     0.00\n",
      "NOTE:     22   400 0.000107           0.8386     0.7821  1.127e+04       3141          0   0.001304     0.00\n",
      "NOTE:     23   400 0.000107           0.9085     0.8682  1.432e+04       2174          0   0.001304     0.00\n",
      "NOTE:     24   400 0.000107           0.8301     0.8748  1.333e+04       1907          0   0.001304     0.00\n",
      "NOTE:     25   400 0.000107           0.7955     0.8538  1.199e+04       2054          0   0.001304     0.00\n",
      "NOTE:     26   400 0.000107           0.9253     0.8304  1.391e+04       2842          0   0.001304     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  127      0.0001          0.8612     0.8579  3.596e+05  5.955e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000086           0.8234     0.8777  1.349e+04       1880          0   0.001304     0.00\n",
      "NOTE:      1   400 0.000086           0.8272     0.8736  1.427e+04       2064          0   0.001304     0.00\n",
      "NOTE:      2   400 0.000086           0.9742     0.8348  1.422e+04       2813          0   0.001304     0.00\n",
      "NOTE:      3   400 0.000086           0.9478     0.8711  1.598e+04       2366          0   0.001304     0.00\n",
      "NOTE:      4   400 0.000086           0.9272     0.8379  1.435e+04       2776          0   0.001304     0.00\n",
      "NOTE:      5   400 0.000086            1.082     0.8608  1.723e+04       2786          0   0.001304     0.00\n",
      "NOTE:      6   400 0.000086           0.7933     0.8819  1.381e+04       1849          0   0.001304     0.00\n",
      "NOTE:      7   400 0.000086           0.7397     0.8741  1.126e+04       1622          0   0.001304     0.00\n",
      "NOTE:      8   400 0.000086            0.944     0.9057  1.629e+04       1696          0   0.001304     0.00\n",
      "NOTE:      9   400 0.000086            1.039     0.8238  1.408e+04       3011          0   0.001304     0.00\n",
      "NOTE:     10   400 0.000086           0.9477     0.8725  1.598e+04       2334          0   0.001304     0.00\n",
      "NOTE:     11   400 0.000086           0.8312     0.8416  1.293e+04       2433          0   0.001304     0.00\n",
      "NOTE:     12   400 0.000086             1.02     0.8621  1.508e+04       2412          0   0.001304     0.00\n",
      "NOTE:     13   400 0.000086             0.76     0.9086  1.274e+04       1281          0   0.001304     0.00\n",
      "NOTE:     14   400 0.000086           0.9877     0.8281  1.439e+04       2987          0   0.001304     0.00\n",
      "NOTE:     15   400 0.000086            1.043     0.8341  1.506e+04       2994          0   0.001304     0.00\n",
      "NOTE:     16   400 0.000086           0.8613     0.8834  1.495e+04       1973          0   0.001304     0.00\n",
      "NOTE:     17   400 0.000086           0.8882     0.8652   1.46e+04       2275          0   0.001304     0.00\n",
      "NOTE:     18   400 0.000086           0.7205     0.8552  1.103e+04       1867          0   0.001304     0.00\n",
      "NOTE:     19   400 0.000086           0.8826     0.8475  1.233e+04       2219          0   0.001304     0.00\n",
      "NOTE:     20   400 0.000086           0.8914     0.8676  1.436e+04       2192          0   0.001304     0.00\n",
      "NOTE:     21   400 0.000086            1.053     0.8372   1.59e+04       3092          0   0.001304     0.00\n",
      "NOTE:     22   400 0.000086           0.8698     0.8651  1.394e+04       2174          0   0.001304     0.00\n",
      "NOTE:     23   400 0.000086            0.873     0.8481   1.35e+04       2418          0   0.001304     0.00\n",
      "NOTE:     24   400 0.000086            1.109     0.8265   1.65e+04       3463          0   0.001304     0.00\n",
      "NOTE:     25   400 0.000086            0.786      0.844  1.245e+04       2302          0   0.001304     0.00\n",
      "NOTE:     26   400 0.000086           0.9951     0.8662   1.57e+04       2424          0   0.001304     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  128      0.0001          0.9118     0.8585  3.864e+05   6.37e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000086           0.9594     0.8324  1.417e+04       2854          0   0.001304     0.00\n",
      "NOTE:      1   400 0.000086           0.7855     0.8433  1.193e+04       2218          0   0.001304     0.00\n",
      "NOTE:      2   400 0.000086           0.9895     0.8968  1.606e+04       1847          1   0.001304     0.00\n",
      "NOTE:      3   400 0.000086           0.9309     0.8532  1.482e+04       2549          0   0.001304     0.00\n",
      "NOTE:      4   400 0.000086           0.7665     0.8867  1.344e+04       1716          0   0.001304     0.00\n",
      "NOTE:      5   400 0.000086           0.8988      0.891   1.41e+04       1726          0   0.001304     0.00\n",
      "NOTE:      6   400 0.000086           0.8062     0.8757  1.087e+04       1543          0   0.001304     0.00\n",
      "NOTE:      7   400 0.000086           0.9458     0.9034  1.774e+04       1896          0   0.001304     0.00\n",
      "NOTE:      8   400 0.000086           0.9798     0.8747  1.498e+04       2146          0   0.001304     0.00\n",
      "NOTE:      9   400 0.000086           0.8644     0.8017  1.197e+04       2961          0   0.001304     0.00\n",
      "NOTE:     10   400 0.000086           0.7338     0.8427  1.049e+04       1957          0   0.001304     0.00\n",
      "NOTE:     11   400 0.000086            0.898     0.8576  1.319e+04       2190          0   0.001304     0.00\n",
      "NOTE:     12   400 0.000086           0.9805     0.8759  1.593e+04       2256          0   0.001304     0.00\n",
      "NOTE:     13   400 0.000086           0.9171     0.8807  1.451e+04       1965          0   0.001304     0.00\n",
      "NOTE:     14   400 0.000086            0.707     0.8595  1.067e+04       1744          0   0.001304     0.00\n",
      "NOTE:     15   400 0.000086           0.9262     0.8718  1.409e+04       2071          0   0.001304     0.00\n",
      "NOTE:     16   400 0.000086           0.9661      0.834   1.42e+04       2827          0   0.001304     0.00\n",
      "NOTE:     17   400 0.000086           0.7631     0.8668  1.225e+04       1882          0   0.001304     0.00\n",
      "NOTE:     18   400 0.000086           0.8053     0.8996  1.445e+04       1613          0   0.001304     0.00\n",
      "NOTE:     19   400 0.000086           0.6448     0.8832  1.084e+04       1433          0   0.001304     0.00\n",
      "NOTE:     20   400 0.000086           0.7404     0.9032  1.336e+04       1432          0   0.001304     0.00\n",
      "NOTE:     21   400 0.000086           0.7606     0.8478  1.177e+04       2113          0   0.001304     0.00\n",
      "NOTE:     22   400 0.000086           0.8463     0.8717    1.4e+04       2061          0   0.001304     0.00\n",
      "NOTE:     23   400 0.000086           0.8089      0.827  1.228e+04       2569          0   0.001304     0.00\n",
      "NOTE:     24   400 0.000086           0.8867     0.8537  1.363e+04       2337          0   0.001304     0.00\n",
      "NOTE:     25   400 0.000086            0.736     0.9114  1.321e+04       1285          0   0.001304     0.00\n",
      "NOTE:     26   400 0.000086           0.7742     0.8694  1.206e+04       1812          0   0.001304     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  129      0.0001          0.8453     0.8678   3.61e+05    5.5e+04          1     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000086           0.8578     0.8744   1.39e+04       1996          0   0.001304     0.00\n",
      "NOTE:      1   400 0.000086           0.8068     0.8601  1.207e+04       1962          0   0.001304     0.00\n",
      "NOTE:      2   400 0.000086           0.8857     0.8625  1.286e+04       2051          0   0.001304     0.00\n",
      "NOTE:      3   400 0.000086           0.8811     0.8267  1.353e+04       2836          0   0.001304     0.00\n",
      "NOTE:      4   400 0.000086           0.9204     0.8555  1.425e+04       2406          0   0.001304     0.00\n",
      "NOTE:      5   400 0.000086           0.8582       0.86  1.287e+04       2094          0   0.001304     0.00\n",
      "NOTE:      6   400 0.000086            0.803     0.8911  1.426e+04       1742          0   0.001304     0.00\n",
      "NOTE:      7   400 0.000086           0.9235      0.832  1.412e+04       2851          0   0.001304     0.00\n",
      "NOTE:      8   400 0.000086           0.8419     0.8323  1.184e+04       2385          0   0.001304     0.00\n",
      "NOTE:      9   400 0.000086             1.03     0.8271  1.656e+04       3462          0   0.001304     0.00\n",
      "NOTE:     10   400 0.000086           0.9983     0.8426  1.539e+04       2875          0   0.001304     0.00\n",
      "NOTE:     11   400 0.000086           0.9204     0.8682  1.461e+04       2219          0   0.001304     0.00\n",
      "NOTE:     12   400 0.000086           0.8039     0.8628  1.325e+04       2107          0   0.001304     0.00\n",
      "NOTE:     13   400 0.000086           0.9266     0.8551  1.479e+04       2507          0   0.001304     0.00\n",
      "NOTE:     14   400 0.000086           0.8092     0.8863  1.377e+04       1766          0   0.001304     0.00\n",
      "NOTE:     15   400 0.000086           0.9751     0.8566   1.56e+04       2611          0   0.001304     0.00\n",
      "NOTE:     16   400 0.000086           0.8737     0.8622  1.487e+04       2377          0   0.001304     0.00\n",
      "NOTE:     17   400 0.000086           0.8935     0.8816  1.519e+04       2039          0   0.001304     0.00\n",
      "NOTE:     18   400 0.000086           0.9657     0.8714  1.536e+04       2267          0   0.001304     0.00\n",
      "NOTE:     19   400 0.000086           0.8953     0.8699  1.356e+04       2027          0   0.001304     0.00\n",
      "NOTE:     20   400 0.000086           0.9307     0.8575  1.517e+04       2521          0   0.001304     0.00\n",
      "NOTE:     21   400 0.000086           0.7214     0.8513  1.137e+04       1986          0   0.001304     0.00\n",
      "NOTE:     22   400 0.000086           0.9702     0.8739  1.423e+04       2054          0   0.001305     0.00\n",
      "NOTE:     23   400 0.000086           0.9052     0.8349  1.241e+04       2454          0   0.001305     0.00\n",
      "NOTE:     24   400 0.000086           0.9713     0.8371  1.454e+04       2828          0   0.001305     0.00\n",
      "NOTE:     25   400 0.000086           0.8679     0.8897  1.461e+04       1812          0   0.001305     0.00\n",
      "NOTE:     26   400 0.000086           0.8405     0.8622  1.284e+04       2053          0   0.001305     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  130      0.0001          0.8918     0.8585  3.778e+05  6.229e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000086             1.06     0.8312  1.517e+04       3081          0   0.001305     0.00\n",
      "NOTE:      1   400 0.000086           0.8593     0.8529  1.318e+04       2273          0   0.001305     0.00\n",
      "NOTE:      2   400 0.000086           0.9645     0.8851  1.531e+04       1988          0   0.001305     0.00\n",
      "NOTE:      3   400 0.000086           0.8675       0.83  1.202e+04       2463          0   0.001305     0.00\n",
      "NOTE:      4   400 0.000086           0.9257     0.8933  1.569e+04       1874          0   0.001305     0.00\n",
      "NOTE:      5   400 0.000086           0.9083     0.8346  1.433e+04       2839          0   0.001305     0.00\n",
      "NOTE:      6   400 0.000086           0.9808     0.8627  1.349e+04       2146          0   0.001305     0.00\n",
      "NOTE:      7   400 0.000086            0.967     0.8298  1.305e+04       2678          0   0.001305     0.00\n",
      "NOTE:      8   400 0.000086           0.8924     0.8723  1.393e+04       2040          0   0.001305     0.00\n",
      "NOTE:      9   400 0.000086           0.8892      0.829  1.211e+04       2499          0   0.001305     0.00\n",
      "NOTE:     10   400 0.000086           0.8345     0.8618   1.43e+04       2293          0   0.001305     0.00\n",
      "NOTE:     11   400 0.000086           0.8755     0.8588  1.287e+04       2116          0   0.001305     0.00\n",
      "NOTE:     12   400 0.000086           0.8513     0.8308  1.227e+04       2498          0   0.001305     0.00\n",
      "NOTE:     13   400 0.000086           0.7599     0.8556  1.169e+04       1972          0   0.001305     0.00\n",
      "NOTE:     14   400 0.000086           0.8226     0.8476  1.288e+04       2315          0   0.001305     0.00\n",
      "NOTE:     15   400 0.000086            1.036     0.7958  1.341e+04       3442          0   0.001305     0.00\n",
      "NOTE:     16   400 0.000086           0.8656      0.884  1.489e+04       1953          0   0.001305     0.00\n",
      "NOTE:     17   400 0.000086           0.8201     0.8624  1.292e+04       2061          0   0.001305     0.00\n",
      "NOTE:     18   400 0.000086           0.8658     0.8886  1.387e+04       1738          0   0.001305     0.00\n",
      "NOTE:     19   400 0.000086           0.8711     0.8466   1.27e+04       2300          0   0.001305     0.00\n",
      "NOTE:     20   400 0.000086           0.9018     0.8804  1.447e+04       1966          0   0.001305     0.00\n",
      "NOTE:     21   400 0.000086           0.9655     0.8725  1.555e+04       2272          0   0.001305     0.00\n",
      "NOTE:     22   400 0.000086           0.7523     0.8807  1.169e+04       1584          0   0.001305     0.00\n",
      "NOTE:     23   400 0.000086           0.8691     0.8722  1.461e+04       2140          0   0.001305     0.00\n",
      "NOTE:     24   400 0.000086           0.8796     0.8506  1.458e+04       2561          0   0.001305     0.00\n",
      "NOTE:     25   400 0.000086            0.935     0.8333  1.356e+04       2712          0   0.001305     0.00\n",
      "NOTE:     26   400 0.000086           0.7831     0.8479  1.254e+04       2249          0   0.001305     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  131      0.0001           0.889     0.8554  3.671e+05  6.205e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000086           0.9573      0.903  1.574e+04       1690          0   0.001305     0.00\n",
      "NOTE:      1   400 0.000086           0.9068     0.8543  1.451e+04       2475          0   0.001305     0.00\n",
      "NOTE:      2   400 0.000086           0.8606     0.8945   1.52e+04       1792          0   0.001305     0.00\n",
      "NOTE:      3   400 0.000086           0.8245     0.8716  1.342e+04       1976          0   0.001305     0.00\n",
      "NOTE:      4   400 0.000086           0.9067     0.8921  1.475e+04       1784          0   0.001305     0.00\n",
      "NOTE:      5   400 0.000086           0.8927     0.8625  1.437e+04       2291          0   0.001305     0.00\n",
      "NOTE:      6   400 0.000086           0.9545     0.8891  1.565e+04       1952          0   0.001305     0.00\n",
      "NOTE:      7   400 0.000086           0.8485     0.8193  1.238e+04       2729          0   0.001305     0.00\n",
      "NOTE:      8   400 0.000086           0.7349     0.8851  1.323e+04       1718          0   0.001305     0.00\n",
      "NOTE:      9   400 0.000086           0.8298     0.9067  1.505e+04       1548          0   0.001305     0.00\n",
      "NOTE:     10   400 0.000086           0.8612     0.8452  1.272e+04       2329          0   0.001305     0.00\n",
      "NOTE:     11   400 0.000086            1.075     0.8536  1.672e+04       2868          0   0.001305     0.00\n",
      "NOTE:     12   400 0.000086           0.8702     0.8471  1.304e+04       2354          0   0.001305     0.00\n",
      "NOTE:     13   400 0.000086           0.8631     0.8778  1.365e+04       1900          0   0.001305     0.00\n",
      "NOTE:     14   400 0.000086           0.9311     0.8623  1.613e+04       2576          0   0.001305     0.00\n",
      "NOTE:     15   400 0.000086           0.8461     0.8833  1.377e+04       1819          0   0.001305     0.00\n",
      "NOTE:     16   400 0.000086           0.8973     0.8315  1.274e+04       2583          0   0.001305     0.00\n",
      "NOTE:     17   400 0.000086           0.8929       0.84  1.351e+04       2572          0   0.001305     0.00\n",
      "NOTE:     18   400 0.000086           0.8715     0.8661  1.486e+04       2297          0   0.001305     0.00\n",
      "NOTE:     19   400 0.000086            0.843     0.8676  1.297e+04       1979          0   0.001305     0.00\n",
      "NOTE:     20   400 0.000086           0.8801     0.8548  1.391e+04       2362          0   0.001305     0.00\n",
      "NOTE:     21   400 0.000086            0.735     0.8944  1.359e+04       1604          0   0.001305     0.00\n",
      "NOTE:     22   400 0.000086           0.9775     0.8119  1.346e+04       3118          0   0.001305     0.00\n",
      "NOTE:     23   400 0.000086           0.8774     0.8428    1.2e+04       2238          0   0.001305     0.00\n",
      "NOTE:     24   400 0.000086           0.9642     0.8579  1.442e+04       2389          0   0.001305     0.00\n",
      "NOTE:     25   400 0.000086           0.9686     0.8523  1.543e+04       2673          0   0.001305     0.00\n",
      "NOTE:     26   400 0.000086           0.8516     0.8755  1.429e+04       2031          0   0.001305     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  132      0.0001           0.886     0.8648  3.815e+05  5.965e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000086           0.7904     0.8759  1.286e+04       1822          0   0.001305     0.00\n",
      "NOTE:      1   400 0.000086           0.8697     0.8792  1.408e+04       1935          0   0.001305     0.00\n",
      "NOTE:      2   400 0.000086            0.864     0.8712  1.376e+04       2034          0   0.001305     0.00\n",
      "NOTE:      3   400 0.000086           0.8757     0.8416  1.322e+04       2489          0   0.001306     0.00\n",
      "NOTE:      4   400 0.000086           0.9368     0.8494  1.432e+04       2539          0   0.001306     0.00\n",
      "NOTE:      5   400 0.000086           0.7351     0.8463  1.111e+04       2017          0   0.001306     0.00\n",
      "NOTE:      6   400 0.000086           0.9828     0.8108  1.281e+04       2989          0   0.001306     0.00\n",
      "NOTE:      7   400 0.000086            1.069     0.8788  1.673e+04       2308          0   0.001306     0.00\n",
      "NOTE:      8   400 0.000086           0.7648     0.8938  1.267e+04       1506          0   0.001306     0.00\n",
      "NOTE:      9   400 0.000086            1.027     0.8335  1.551e+04       3099          0   0.001306     0.00\n",
      "NOTE:     10   400 0.000086           0.8155     0.8907  1.393e+04       1709          0   0.001306     0.00\n",
      "NOTE:     11   400 0.000086           0.9058     0.8612  1.375e+04       2215          0   0.001306     0.00\n",
      "NOTE:     12   400 0.000086           0.7514     0.8733  1.236e+04       1794          0   0.001306     0.00\n",
      "NOTE:     13   400 0.000086           0.8821     0.8545  1.301e+04       2215          0   0.001306     0.00\n",
      "NOTE:     14   400 0.000086           0.7647     0.9029  1.362e+04       1465          0   0.001306     0.00\n",
      "NOTE:     15   400 0.000086            1.034     0.8653  1.611e+04       2509          0   0.001306     0.00\n",
      "NOTE:     16   400 0.000086           0.8965     0.8825  1.511e+04       2012          0   0.001306     0.00\n",
      "NOTE:     17   400 0.000086           0.9893     0.8541  1.525e+04       2605          0   0.001306     0.00\n",
      "NOTE:     18   400 0.000086           0.9165     0.8561  1.467e+04       2465          0   0.001306     0.00\n",
      "NOTE:     19   400 0.000086           0.8942     0.8353  1.297e+04       2557          0   0.001306     0.00\n",
      "NOTE:     20   400 0.000086           0.6706     0.8777  1.034e+04       1440          0   0.001306     0.00\n",
      "NOTE:     21   400 0.000086           0.8688     0.8562  1.337e+04       2245          0   0.001306     0.00\n",
      "NOTE:     22   400 0.000086           0.9134     0.8407  1.384e+04       2622          0   0.001306     0.00\n",
      "NOTE:     23   400 0.000086            1.055     0.8058  1.492e+04       3596          0   0.001306     0.00\n",
      "NOTE:     24   400 0.000086           0.7759     0.8258  1.104e+04       2328          0   0.001306     0.00\n",
      "NOTE:     25   400 0.000086           0.8962      0.862  1.287e+04       2060          0   0.001306     0.00\n",
      "NOTE:     26   400 0.000086           0.8028     0.8522   1.29e+04       2237          0   0.001306     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  133      0.0001          0.8795     0.8579  3.671e+05  6.081e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000086           0.9095     0.8687  1.346e+04       2035          0   0.001306     0.00\n",
      "NOTE:      1   400 0.000086           0.9468     0.8629  1.362e+04       2164          0   0.001306     0.00\n",
      "NOTE:      2   400 0.000086           0.8179     0.8332  1.151e+04       2305          0   0.001306     0.00\n",
      "NOTE:      3   400 0.000086           0.7044     0.8618  1.017e+04       1631          0   0.001306     0.00\n",
      "NOTE:      4   400 0.000086           0.9791     0.8685  1.667e+04       2523          0   0.001306     0.00\n",
      "NOTE:      5   400 0.000086           0.8571     0.8424  1.326e+04       2480          0   0.001306     0.00\n",
      "NOTE:      6   400 0.000086           0.9307     0.8698  1.469e+04       2199          0   0.001306     0.00\n",
      "NOTE:      7   400 0.000086           0.8991     0.8816  1.421e+04       1909          0   0.001306     0.00\n",
      "NOTE:      8   400 0.000086           0.9084     0.8532  1.384e+04       2381          0   0.001306     0.00\n",
      "NOTE:      9   400 0.000086           0.8628     0.8951  1.481e+04       1736          0   0.001306     0.00\n",
      "NOTE:     10   400 0.000086           0.8054      0.878   1.22e+04       1695          0   0.001306     0.00\n",
      "NOTE:     11   400 0.000086           0.6834     0.8409  1.028e+04       1944          0   0.001306     0.00\n",
      "NOTE:     12   400 0.000086           0.9017     0.8452  1.293e+04       2368          0   0.001306     0.00\n",
      "NOTE:     13   400 0.000086           0.8773     0.8559  1.429e+04       2406          0   0.001306     0.00\n",
      "NOTE:     14   400 0.000086           0.9122     0.8573  1.361e+04       2266          0   0.001306     0.00\n",
      "NOTE:     15   400 0.000086           0.7344     0.8949  1.257e+04       1475          0   0.001306     0.00\n",
      "NOTE:     16   400 0.000086            0.796     0.8783  1.256e+04       1740          0   0.001306     0.00\n",
      "NOTE:     17   400 0.000086           0.9344     0.8749  1.426e+04       2039          0   0.001306     0.00\n",
      "NOTE:     18   400 0.000086           0.9296     0.8365  1.328e+04       2596          0   0.001306     0.00\n",
      "NOTE:     19   400 0.000086           0.8524     0.8388  1.265e+04       2431          0   0.001306     0.00\n",
      "NOTE:     20   400 0.000086           0.9571     0.8287   1.36e+04       2811          0   0.001306     0.00\n",
      "NOTE:     21   400 0.000086           0.8511      0.854  1.211e+04       2071          0   0.001306     0.00\n",
      "NOTE:     22   400 0.000086             1.02     0.8376  1.592e+04       3086          0   0.001306     0.00\n",
      "NOTE:     23   400 0.000086           0.9355     0.8282  1.441e+04       2990          0   0.001306     0.00\n",
      "NOTE:     24   400 0.000086           0.9837     0.8636  1.545e+04       2441          0   0.001306     0.00\n",
      "NOTE:     25   400 0.000086            0.782     0.8623  1.251e+04       1998          0   0.001306     0.00\n",
      "NOTE:     26   400 0.000086           0.9817     0.8756  1.592e+04       2263          0   0.001306     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  134      0.0001          0.8798     0.8588  3.648e+05  5.998e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000086            1.061     0.8916  1.801e+04       2189          0   0.001306     0.00\n",
      "NOTE:      1   400 0.000086            0.857     0.8516  1.288e+04       2245          0   0.001306     0.00\n",
      "NOTE:      2   400 0.000086           0.7983     0.8442   1.13e+04       2086          0   0.001306     0.00\n",
      "NOTE:      3   400 0.000086           0.9881     0.8799  1.611e+04       2198          0   0.001306     0.00\n",
      "NOTE:      4   400 0.000086            1.012     0.8919  1.654e+04       2005          0   0.001306     0.00\n",
      "NOTE:      5   400 0.000086           0.8343     0.8155  1.118e+04       2530          0   0.001306     0.00\n",
      "NOTE:      6   400 0.000086           0.8433      0.877  1.261e+04       1768          0   0.001306     0.00\n",
      "NOTE:      7   400 0.000086           0.8989     0.8718  1.397e+04       2055          0   0.001306     0.00\n",
      "NOTE:      8   400 0.000086           0.8393      0.907  1.446e+04       1482          0   0.001306     0.00\n",
      "NOTE:      9   400 0.000086           0.8683     0.8605  1.379e+04       2236          0   0.001306     0.00\n",
      "NOTE:     10   400 0.000086           0.6845     0.8978  1.083e+04       1233          0   0.001306     0.00\n",
      "NOTE:     11   400 0.000086           0.8526     0.8414  1.318e+04       2484          0   0.001306     0.00\n",
      "NOTE:     12   400 0.000086           0.8877     0.8321  1.364e+04       2752          0   0.001306     0.00\n",
      "NOTE:     13   400 0.000086           0.9454      0.825  1.311e+04       2781          0   0.001306     0.00\n",
      "NOTE:     14   400 0.000086           0.9185      0.854  1.363e+04       2329          0   0.001306     0.00\n",
      "NOTE:     15   400 0.000086           0.8484     0.8455  1.316e+04       2404          0   0.001306     0.00\n",
      "NOTE:     16   400 0.000086           0.9696     0.8318  1.512e+04       3056          0   0.001306     0.00\n",
      "NOTE:     17   400 0.000086           0.7787     0.8962  1.318e+04       1527          0   0.001306     0.00\n",
      "NOTE:     18   400 0.000086            1.039      0.857  1.745e+04       2913          0   0.001306     0.00\n",
      "NOTE:     19   400 0.000086           0.9136      0.881  1.531e+04       2067          0   0.001306     0.00\n",
      "NOTE:     20   400 0.000086           0.9413     0.8469  1.473e+04       2663          0   0.001306     0.00\n",
      "NOTE:     21   400 0.000086           0.9098     0.8956  1.632e+04       1901          0   0.001306     0.00\n",
      "NOTE:     22   400 0.000086           0.8139     0.8411   1.17e+04       2210          0   0.001306     0.00\n",
      "NOTE:     23   400 0.000086           0.9481     0.8413  1.472e+04       2777          0   0.001306     0.00\n",
      "NOTE:     24   400 0.000086            1.066     0.8525  1.667e+04       2884          0   0.001306     0.00\n",
      "NOTE:     25   400 0.000086           0.7977     0.8735   1.35e+04       1956          0   0.001306     0.00\n",
      "NOTE:     26   400 0.000086           0.9912     0.8455  1.565e+04       2860          0   0.001306     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  135      0.0001          0.9002     0.8614  3.827e+05  6.159e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000086           0.7371     0.8664  1.222e+04       1885          0   0.001306     0.00\n",
      "NOTE:      1   400 0.000086           0.9431     0.8531  1.442e+04       2483          0   0.001306     0.00\n",
      "NOTE:      2   400 0.000086           0.8555     0.8675  1.355e+04       2069          0   0.001306     0.00\n",
      "NOTE:      3   400 0.000086           0.8512     0.8662  1.353e+04       2091          0   0.001306     0.00\n",
      "NOTE:      4   400 0.000086           0.9989     0.8819   1.59e+04       2128          0   0.001306     0.00\n",
      "NOTE:      5   400 0.000086           0.8604     0.8634  1.369e+04       2166          0   0.001306     0.00\n",
      "NOTE:      6   400 0.000086           0.9277     0.8593   1.43e+04       2341          0   0.001306     0.00\n",
      "NOTE:      7   400 0.000086           0.9451     0.8463  1.523e+04       2766          0   0.001306     0.00\n",
      "NOTE:      8   400 0.000086           0.8999     0.7768   1.21e+04       3477          0   0.001306     0.00\n",
      "NOTE:      9   400 0.000086            0.774     0.8593  1.139e+04       1864          0   0.001306     0.00\n",
      "NOTE:     10   400 0.000086            0.934     0.8475  1.442e+04       2594          0   0.001306     0.00\n",
      "NOTE:     11   400 0.000086           0.8415     0.8712  1.263e+04       1867          0   0.001306     0.00\n",
      "NOTE:     12   400 0.000086           0.8427     0.8675  1.401e+04       2140          0   0.001306     0.00\n",
      "NOTE:     13   400 0.000086           0.8709     0.8753  1.426e+04       2032          0   0.001306     0.00\n",
      "NOTE:     14   400 0.000086            0.958     0.8381   1.52e+04       2935          0   0.001306     0.00\n",
      "NOTE:     15   400 0.000086           0.8478      0.871  1.472e+04       2181          0   0.001306     0.00\n",
      "NOTE:     16   400 0.000086           0.9846      0.849  1.551e+04       2758          0   0.001306     0.00\n",
      "NOTE:     17   400 0.000086           0.7847     0.8179  1.152e+04       2564          0   0.001306     0.00\n",
      "NOTE:     18   400 0.000086           0.8799     0.8262   1.34e+04       2818          0   0.001306     0.00\n",
      "NOTE:     19   400 0.000086            0.868     0.7728  1.054e+04       3099          0   0.001306     0.00\n",
      "NOTE:     20   400 0.000086           0.9587     0.8549  1.453e+04       2467          0   0.001306     0.00\n",
      "NOTE:     21   400 0.000086           0.9991       0.86  1.552e+04       2526          0   0.001306     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     22   400 0.000086           0.8148     0.8698  1.293e+04       1935          0   0.001306     0.00\n",
      "NOTE:     23   400 0.000086           0.9926      0.869   1.55e+04       2337          0   0.001306     0.00\n",
      "NOTE:     24   400 0.000086           0.8854     0.8703  1.483e+04       2210          0   0.001306     0.00\n",
      "NOTE:     25   400 0.000086            0.908     0.8448  1.459e+04       2679          0   0.001306     0.00\n",
      "NOTE:     26   400 0.000086           0.7395     0.8592  1.133e+04       1857          0   0.001306     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  136      0.0001          0.8853     0.8526  3.717e+05  6.427e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000069            1.009     0.8575  1.501e+04       2493          0   0.001306     0.00\n",
      "NOTE:      1   400 0.000069           0.7809     0.8753  1.298e+04       1849          0   0.001306     0.00\n",
      "NOTE:      2   400 0.000069           0.9418     0.8639  1.441e+04       2271          0   0.001306     0.00\n",
      "NOTE:      3   400 0.000069           0.8603     0.8774  1.364e+04       1905          0   0.001306     0.00\n",
      "NOTE:      4   400 0.000069           0.8366     0.8657  1.329e+04       2062          0   0.001306     0.00\n",
      "NOTE:      5   400 0.000069           0.9024     0.8775  1.483e+04       2070          0   0.001306     0.00\n",
      "NOTE:      6   400 0.000069           0.9091     0.8687  1.354e+04       2047          0   0.001307     0.00\n",
      "NOTE:      7   400 0.000069           0.9866     0.8746  1.599e+04       2292          0   0.001307     0.00\n",
      "NOTE:      8   400 0.000069           0.8201     0.8827  1.332e+04       1770          0   0.001307     0.00\n",
      "NOTE:      9   400 0.000069           0.9193     0.8573  1.518e+04       2526          0   0.001307     0.00\n",
      "NOTE:     10   400 0.000069           0.8562     0.8527  1.295e+04       2237          0   0.001307     0.00\n",
      "NOTE:     11   400 0.000069           0.9537     0.8645  1.355e+04       2124          0   0.001307     0.00\n",
      "NOTE:     12   400 0.000069            0.907     0.8452  1.453e+04       2662          0   0.001307     0.00\n",
      "NOTE:     13   400 0.000069            1.002     0.8751   1.51e+04       2156          0   0.001307     0.00\n",
      "NOTE:     14   400 0.000069           0.9074     0.8922  1.435e+04       1734          0   0.001307     0.00\n",
      "NOTE:     15   400 0.000069           0.9166     0.8534  1.396e+04       2399          0   0.001307     0.00\n",
      "NOTE:     16   400 0.000069           0.8638     0.8543  1.365e+04       2329          0   0.001307     0.00\n",
      "NOTE:     17   400 0.000069           0.8953     0.8665  1.434e+04       2210          0   0.001307     0.00\n",
      "NOTE:     18   400 0.000069            0.824     0.8711  1.308e+04       1936          0   0.001307     0.00\n",
      "NOTE:     19   400 0.000069            0.916      0.896  1.605e+04       1864          0   0.001307     0.00\n",
      "NOTE:     20   400 0.000069           0.8392     0.8187  1.214e+04       2689          0   0.001307     0.00\n",
      "NOTE:     21   400 0.000069           0.9079     0.8695  1.415e+04       2125          0   0.001307     0.00\n",
      "NOTE:     22   400 0.000069            1.085     0.8005  1.408e+04       3509          0   0.001307     0.00\n",
      "NOTE:     23   400 0.000069           0.9933      0.903  1.682e+04       1806          0   0.001307     0.00\n",
      "NOTE:     24   400 0.000069            1.026     0.8751  1.647e+04       2350          0   0.001307     0.00\n",
      "NOTE:     25   400 0.000069           0.8569     0.8891  1.433e+04       1787          0   0.001307     0.00\n",
      "NOTE:     26   400 0.000069            0.759     0.8876  1.292e+04       1636          0   0.001307     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  137      0.0001          0.9065     0.8673  3.847e+05  5.884e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000069           0.9314     0.8427  1.393e+04       2601          0   0.001307     0.00\n",
      "NOTE:      1   400 0.000069           0.9737     0.8694  1.507e+04       2265          0   0.001307     0.00\n",
      "NOTE:      2   400 0.000069           0.8812     0.8622  1.388e+04       2217          0   0.001307     0.00\n",
      "NOTE:      3   400 0.000069           0.8311     0.8245  1.251e+04       2662          0   0.001307     0.00\n",
      "NOTE:      4   400 0.000069           0.9152      0.871  1.527e+04       2262          0   0.001307     0.00\n",
      "NOTE:      5   400 0.000069            1.018     0.8254  1.373e+04       2905          0   0.001307     0.00\n",
      "NOTE:      6   400 0.000069            1.041     0.8548  1.531e+04       2600          0   0.001307     0.00\n",
      "NOTE:      7   400 0.000069            0.985     0.8392  1.449e+04       2776          0   0.001307     0.00\n",
      "NOTE:      8   400 0.000069           0.8157     0.8754  1.314e+04       1871          0   0.001307     0.00\n",
      "NOTE:      9   400 0.000069           0.7754     0.7567       9428       3032          0   0.001307     0.00\n",
      "NOTE:     10   400 0.000069           0.8352     0.8895  1.428e+04       1773          0   0.001307     0.00\n",
      "NOTE:     11   400 0.000069           0.9458     0.8378  1.275e+04       2468          0   0.001307     0.00\n",
      "NOTE:     12   400 0.000069           0.8421     0.8455  1.292e+04       2361          0   0.001307     0.00\n",
      "NOTE:     13   400 0.000069           0.9427     0.8494  1.476e+04       2617          0   0.001307     0.00\n",
      "NOTE:     14   400 0.000069           0.8152     0.8598  1.239e+04       2020          0   0.001307     0.00\n",
      "NOTE:     15   400 0.000069           0.9495     0.8869  1.588e+04       2024          0   0.001307     0.00\n",
      "NOTE:     16   400 0.000069           0.8224     0.8649  1.273e+04       1988          0   0.001307     0.00\n",
      "NOTE:     17   400 0.000069           0.8308     0.8571   1.35e+04       2251          0   0.001307     0.00\n",
      "NOTE:     18   400 0.000069           0.8784     0.8808  1.419e+04       1921          0   0.001307     0.00\n",
      "NOTE:     19   400 0.000069           0.9389     0.7836   1.19e+04       3286          0   0.001307     0.00\n",
      "NOTE:     20   400 0.000069           0.9488     0.8797  1.508e+04       2062          0   0.001307     0.00\n",
      "NOTE:     21   400 0.000069           0.8842     0.8353  1.335e+04       2633          0   0.001307     0.00\n",
      "NOTE:     22   400 0.000069           0.6888     0.8886  1.306e+04       1638          0   0.001307     0.00\n",
      "NOTE:     23   400 0.000069            1.004     0.8627  1.671e+04       2659          0   0.001307     0.00\n",
      "NOTE:     24   400 0.000069             1.05     0.8297  1.593e+04       3268          0   0.001307     0.00\n",
      "NOTE:     25   400 0.000069           0.8669     0.8736  1.323e+04       1915          0   0.001307     0.00\n",
      "NOTE:     26   400 0.000069           0.9004     0.8513  1.409e+04       2462          0   0.001307     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  138      0.0001          0.9004     0.8527  3.735e+05  6.454e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000069             1.06     0.8324  1.616e+04       3254          0   0.001307     0.00\n",
      "NOTE:      1   400 0.000069           0.8636     0.8548  1.338e+04       2272          0   0.001307     0.00\n",
      "NOTE:      2   400 0.000069           0.8641     0.8594   1.29e+04       2110          0   0.001307     0.00\n",
      "NOTE:      3   400 0.000069            0.921      0.861  1.416e+04       2286          0   0.001307     0.00\n",
      "NOTE:      4   400 0.000069           0.8676     0.8828  1.365e+04       1813          0   0.001307     0.00\n",
      "NOTE:      5   400 0.000069           0.8388     0.8753  1.301e+04       1853          0   0.001307     0.00\n",
      "NOTE:      6   400 0.000069            0.891     0.8801  1.398e+04       1905          0   0.001307     0.00\n",
      "NOTE:      7   400 0.000069           0.9507     0.8539  1.417e+04       2423          0   0.001307     0.00\n",
      "NOTE:      8   400 0.000069             1.07     0.8619  1.724e+04       2761          0   0.001307     0.00\n",
      "NOTE:      9   400 0.000069           0.7959     0.8558  1.185e+04       1997          0   0.001307     0.00\n",
      "NOTE:     10   400 0.000069           0.8734     0.8293  1.341e+04       2761          0   0.001307     0.00\n",
      "NOTE:     11   400 0.000069           0.8453     0.8555  1.257e+04       2122          0   0.001307     0.00\n",
      "NOTE:     12   400 0.000069           0.9044     0.8369  1.391e+04       2710          0   0.001307     0.00\n",
      "NOTE:     13   400 0.000069           0.7665      0.882  1.193e+04       1596          0   0.001307     0.00\n",
      "NOTE:     14   400 0.000069            1.026     0.8751  1.582e+04       2257          0   0.001307     0.00\n",
      "NOTE:     15   400 0.000069           0.9136     0.8485  1.475e+04       2632          0   0.001307     0.00\n",
      "NOTE:     16   400 0.000069           0.8896     0.8722  1.425e+04       2087          0   0.001307     0.00\n",
      "NOTE:     17   400 0.000069            0.799     0.8538  1.145e+04       1962          0   0.001307     0.00\n",
      "NOTE:     18   400 0.000069           0.8648     0.8587  1.441e+04       2371          0   0.001307     0.00\n",
      "NOTE:     19   400 0.000069           0.9029     0.9175  1.543e+04       1388          0   0.001307     0.00\n",
      "NOTE:     20   400 0.000069            0.968     0.8207   1.36e+04       2970          0   0.001307     0.00\n",
      "NOTE:     21   400 0.000069           0.9343     0.8772  1.562e+04       2187          0   0.001307     0.00\n",
      "NOTE:     22   400 0.000069           0.9465     0.8373  1.356e+04       2634          0   0.001307     0.00\n",
      "NOTE:     23   400 0.000069           0.8593     0.8682  1.291e+04       1960          0   0.001307     0.00\n",
      "NOTE:     24   400 0.000069           0.8444     0.8197  1.239e+04       2726          0   0.001307     0.00\n",
      "NOTE:     25   400 0.000069            0.849     0.8195  1.276e+04       2810          0   0.001307     0.00\n",
      "NOTE:     26   400 0.000069           0.8157     0.8733  1.419e+04       2059          0   0.001307     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  139      0.0001          0.8935     0.8578  3.734e+05  6.191e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000069           0.8441     0.8689  1.288e+04       1943          0   0.001307     0.00\n",
      "NOTE:      1   400 0.000069            1.058     0.8335  1.433e+04       2863          0   0.001307     0.00\n",
      "NOTE:      2   400 0.000069           0.9356     0.8407   1.49e+04       2824          0   0.001307     0.00\n",
      "NOTE:      3   400 0.000069           0.7293     0.8745  1.152e+04       1653          0   0.001307     0.00\n",
      "NOTE:      4   400 0.000069            0.916      0.888  1.489e+04       1879          0   0.001307     0.00\n",
      "NOTE:      5   400 0.000069           0.9353     0.8267   1.46e+04       3059          0   0.001307     0.00\n",
      "NOTE:      6   400 0.000069           0.6898      0.869  1.149e+04       1732          0   0.001307     0.00\n",
      "NOTE:      7   400 0.000069           0.8759     0.8136  1.107e+04       2536          0   0.001307     0.00\n",
      "NOTE:      8   400 0.000069           0.8221     0.8819  1.357e+04       1816          0   0.001307     0.00\n",
      "NOTE:      9   400 0.000069           0.8146      0.862  1.291e+04       2066          0   0.001307     0.00\n",
      "NOTE:     10   400 0.000069           0.9207     0.8789  1.463e+04       2015          0   0.001307     0.00\n",
      "NOTE:     11   400 0.000069           0.8731     0.8833   1.44e+04       1902          0   0.001307     0.00\n",
      "NOTE:     12   400 0.000069            0.921     0.8161  1.439e+04       3243          0   0.001307     0.00\n",
      "NOTE:     13   400 0.000069            0.884     0.8686  1.547e+04       2340          0   0.001307     0.00\n",
      "NOTE:     14   400 0.000069            0.916     0.8991   1.48e+04       1661          0   0.001307     0.00\n",
      "NOTE:     15   400 0.000069           0.8797     0.8494  1.433e+04       2540          0   0.001307     0.00\n",
      "NOTE:     16   400 0.000069           0.8827     0.8397  1.267e+04       2418          0   0.001307     0.00\n",
      "NOTE:     17   400 0.000069           0.9133     0.8262  1.285e+04       2703          0   0.001307     0.00\n",
      "NOTE:     18   400 0.000069           0.8657     0.8225  1.306e+04       2819          0   0.001307     0.00\n",
      "NOTE:     19   400 0.000069           0.9345     0.8361  1.384e+04       2714          0   0.001307     0.00\n",
      "NOTE:     20   400 0.000069           0.9751     0.8319  1.355e+04       2737          0   0.001307     0.00\n",
      "NOTE:     21   400 0.000069           0.7754     0.8563  1.086e+04       1821          0   0.001307     0.00\n",
      "NOTE:     22   400 0.000069            0.973      0.892   1.67e+04       2021          0   0.001307     0.00\n",
      "NOTE:     23   400 0.000069           0.8616     0.8881  1.372e+04       1728          0   0.001307     0.00\n",
      "NOTE:     24   400 0.000069            0.933     0.8315  1.316e+04       2667          0   0.001307     0.00\n",
      "NOTE:     25   400 0.000069           0.8495     0.8617  1.382e+04       2217          0   0.001307     0.00\n",
      "NOTE:     26   400 0.000069           0.8938     0.8908   1.58e+04       1936          0   0.001307     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  140      0.0001          0.8842     0.8568  3.702e+05  6.185e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000069           0.8963     0.8493  1.373e+04       2436          0   0.001308     0.00\n",
      "NOTE:      1   400 0.000069           0.8872     0.8283  1.281e+04       2654          0   0.001308     0.00\n",
      "NOTE:      2   400 0.000069           0.8569     0.8767  1.381e+04       1943          0   0.001308     0.00\n",
      "NOTE:      3   400 0.000069           0.8314     0.8953  1.465e+04       1712          0   0.001308     0.00\n",
      "NOTE:      4   400 0.000069           0.8595     0.8397  1.297e+04       2476          0   0.001308     0.00\n",
      "NOTE:      5   400 0.000069            1.041     0.8718  1.575e+04       2316          0   0.001308     0.00\n",
      "NOTE:      6   400 0.000069           0.8991     0.8113  1.277e+04       2969          0   0.001308     0.00\n",
      "NOTE:      7   400 0.000069           0.6777     0.8759  1.076e+04       1524          0   0.001308     0.00\n",
      "NOTE:      8   400 0.000069           0.8182     0.8962  1.444e+04       1673          0   0.001308     0.00\n",
      "NOTE:      9   400 0.000069            0.986     0.8702  1.624e+04       2423          0   0.001308     0.00\n",
      "NOTE:     10   400 0.000069           0.9095     0.8204  1.262e+04       2763          0   0.001308     0.00\n",
      "NOTE:     11   400 0.000069           0.6848     0.8334       9754       1950          0   0.001308     0.00\n",
      "NOTE:     12   400 0.000069           0.9085     0.8871  1.526e+04       1942          0   0.001308     0.00\n",
      "NOTE:     13   400 0.000069           0.8862     0.8134  1.196e+04       2744          0   0.001308     0.00\n",
      "NOTE:     14   400 0.000069           0.7662     0.8934  1.314e+04       1567          0   0.001308     0.00\n",
      "NOTE:     15   400 0.000069            1.003     0.8742  1.584e+04       2281          0   0.001308     0.00\n",
      "NOTE:     16   400 0.000069           0.8443     0.8726   1.38e+04       2016          0   0.001308     0.00\n",
      "NOTE:     17   400 0.000069           0.8976     0.8685  1.475e+04       2233          0   0.001308     0.00\n",
      "NOTE:     18   400 0.000069           0.8477      0.867  1.345e+04       2064          0   0.001308     0.00\n",
      "NOTE:     19   400 0.000069            1.002     0.8847  1.657e+04       2158          0   0.001308     0.00\n",
      "NOTE:     20   400 0.000069           0.8762     0.8336  1.346e+04       2687          0   0.001308     0.00\n",
      "NOTE:     21   400 0.000069           0.9713     0.8757  1.483e+04       2104          0   0.001308     0.00\n",
      "NOTE:     22   400 0.000069           0.8796     0.8565   1.31e+04       2195          0   0.001308     0.00\n",
      "NOTE:     23   400 0.000069           0.9338     0.8747  1.429e+04       2046          0   0.001308     0.00\n",
      "NOTE:     24   400 0.000069           0.7951     0.8649  1.291e+04       2015          0   0.001308     0.00\n",
      "NOTE:     25   400 0.000069            0.879     0.8565    1.3e+04       2177          0   0.001308     0.00\n",
      "NOTE:     26   400 0.000069           0.9402     0.8407   1.38e+04       2614          0   0.001308     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  141      0.0001          0.8807     0.8612  3.704e+05  5.968e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000069            1.065       0.85  1.559e+04       2751          0   0.001308     0.00\n",
      "NOTE:      1   400 0.000069           0.8447     0.8453  1.169e+04       2139          0   0.001308     0.00\n",
      "NOTE:      2   400 0.000069           0.9719     0.8557  1.561e+04       2633          0   0.001308     0.00\n",
      "NOTE:      3   400 0.000069              0.8     0.8339  1.226e+04       2442          0   0.001308     0.00\n",
      "NOTE:      4   400 0.000069            1.103     0.8543  1.534e+04       2617          0   0.001308     0.00\n",
      "NOTE:      5   400 0.000069           0.8495     0.8412   1.23e+04       2322          0   0.001308     0.00\n",
      "NOTE:      6   400 0.000069           0.9711     0.8555  1.466e+04       2476          0   0.001308     0.00\n",
      "NOTE:      7   400 0.000069            0.929     0.8521  1.343e+04       2332          0   0.001308     0.00\n",
      "NOTE:      8   400 0.000069            0.762     0.8774  1.248e+04       1743          0   0.001308     0.00\n",
      "NOTE:      9   400 0.000069            1.027      0.836  1.561e+04       3063          0   0.001308     0.00\n",
      "NOTE:     10   400 0.000069           0.9785     0.8556  1.516e+04       2557          0   0.001308     0.00\n",
      "NOTE:     11   400 0.000069           0.9193     0.8629  1.421e+04       2258          0   0.001308     0.00\n",
      "NOTE:     12   400 0.000069            0.969     0.8632  1.613e+04       2556          0   0.001308     0.00\n",
      "NOTE:     13   400 0.000069           0.7438     0.8681  1.218e+04       1850          0   0.001308     0.00\n",
      "NOTE:     14   400 0.000069           0.8576     0.8297  1.263e+04       2592          0   0.001308     0.00\n",
      "NOTE:     15   400 0.000069           0.8655     0.8598  1.458e+04       2378          0   0.001308     0.00\n",
      "NOTE:     16   400 0.000069            0.764      0.862  1.213e+04       1943          0   0.001308     0.00\n",
      "NOTE:     17   400 0.000069           0.9803     0.8447  1.472e+04       2705          0   0.001308     0.00\n",
      "NOTE:     18   400 0.000069           0.9507     0.8759  1.493e+04       2114          0   0.001308     0.00\n",
      "NOTE:     19   400 0.000069           0.8792     0.8777  1.398e+04       1948          0   0.001308     0.00\n",
      "NOTE:     20   400 0.000069           0.8014     0.8278  1.143e+04       2377          0   0.001308     0.00\n",
      "NOTE:     21   400 0.000069           0.7734     0.8505  1.174e+04       2063          0   0.001308     0.00\n",
      "NOTE:     22   400 0.000069            1.016     0.8606  1.567e+04       2539          0   0.001308     0.00\n",
      "NOTE:     23   400 0.000069            0.885     0.8639  1.445e+04       2276          0   0.001308     0.00\n",
      "NOTE:     24   400 0.000069           0.9738     0.8535  1.394e+04       2392          0   0.001308     0.00\n",
      "NOTE:     25   400 0.000069           0.9885     0.8518  1.487e+04       2586          0   0.001308     0.00\n",
      "NOTE:     26   400 0.000069           0.7961     0.8313   1.11e+04       2252          0   0.001308     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  142      0.0001          0.9061     0.8537  3.728e+05   6.39e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000069           0.8847     0.8553  1.348e+04       2279          0   0.001308     0.00\n",
      "NOTE:      1   400 0.000069           0.9705     0.8332  1.361e+04       2725          0   0.001308     0.00\n",
      "NOTE:      2   400 0.000069           0.9157     0.8627  1.471e+04       2340          0   0.001308     0.00\n",
      "NOTE:      3   400 0.000069           0.9428     0.8402  1.384e+04       2633          0   0.001308     0.00\n",
      "NOTE:      4   400 0.000069           0.8751     0.8454  1.349e+04       2468          0   0.001308     0.00\n",
      "NOTE:      5   400 0.000069           0.8156     0.8969  1.459e+04       1678          0   0.001308     0.00\n",
      "NOTE:      6   400 0.000069           0.7791     0.7925  1.105e+04       2892          0   0.001308     0.00\n",
      "NOTE:      7   400 0.000069           0.7701     0.8779  1.199e+04       1667          0   0.001308     0.00\n",
      "NOTE:      8   400 0.000069           0.8977     0.8677  1.353e+04       2062          0   0.001308     0.00\n",
      "NOTE:      9   400 0.000069           0.8635     0.8813  1.408e+04       1897          0   0.001308     0.00\n",
      "NOTE:     10   400 0.000069           0.8782     0.8661  1.363e+04       2108          0   0.001308     0.00\n",
      "NOTE:     11   400 0.000069           0.7397     0.8502  1.084e+04       1910          0   0.001308     0.00\n",
      "NOTE:     12   400 0.000069           0.8612     0.8647  1.244e+04       1946          0   0.001308     0.00\n",
      "NOTE:     13   400 0.000069           0.8865     0.8396  1.319e+04       2520          0   0.001308     0.00\n",
      "NOTE:     14   400 0.000069           0.9189     0.8744  1.462e+04       2101          0   0.001308     0.00\n",
      "NOTE:     15   400 0.000069           0.8125     0.8787  1.436e+04       1983          0   0.001308     0.00\n",
      "NOTE:     16   400 0.000069           0.8828       0.87  1.401e+04       2093          0   0.001308     0.00\n",
      "NOTE:     17   400 0.000069           0.8322     0.8703  1.334e+04       1988          0   0.001308     0.00\n",
      "NOTE:     18   400 0.000069           0.8335     0.8841  1.384e+04       1815          0   0.001308     0.00\n",
      "NOTE:     19   400 0.000069           0.9787     0.8981  1.636e+04       1857          0   0.001308     0.00\n",
      "NOTE:     20   400 0.000069           0.8045     0.8703  1.206e+04       1797          0   0.001308     0.00\n",
      "NOTE:     21   400 0.000069            0.851     0.8639  1.305e+04       2056          0   0.001308     0.00\n",
      "NOTE:     22   400 0.000069           0.8619     0.8605   1.23e+04       1993          0   0.001308     0.00\n",
      "NOTE:     23   400 0.000069           0.8111     0.8642  1.266e+04       1989          0   0.001308     0.00\n",
      "NOTE:     24   400 0.000069           0.7782       0.87  1.321e+04       1974          0   0.001308     0.00\n",
      "NOTE:     25   400 0.000069           0.8725     0.8644  1.387e+04       2175          0   0.001308     0.00\n",
      "NOTE:     26   400 0.000069           0.9677     0.8897  1.602e+04       1986          0   0.001308     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  143      0.0001          0.8624     0.8648  3.641e+05  5.693e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000069           0.8874      0.828  1.335e+04       2772          0   0.001308     0.00\n",
      "NOTE:      1   400 0.000069           0.9522     0.8688  1.507e+04       2276          0   0.001308     0.00\n",
      "NOTE:      2   400 0.000069           0.8553     0.8211  1.294e+04       2819          0   0.001308     0.00\n",
      "NOTE:      3   400 0.000069           0.9488     0.8862  1.521e+04       1954          0   0.001308     0.00\n",
      "NOTE:      4   400 0.000069           0.8551     0.8776  1.314e+04       1833          0   0.001308     0.00\n",
      "NOTE:      5   400 0.000069           0.9442     0.8716  1.571e+04       2314          0   0.001308     0.00\n",
      "NOTE:      6   400 0.000069           0.8112     0.8748  1.321e+04       1891          0   0.001308     0.00\n",
      "NOTE:      7   400 0.000069           0.8768     0.8533  1.459e+04       2508          0   0.001308     0.00\n",
      "NOTE:      8   400 0.000069           0.9524     0.8852  1.545e+04       2003          0   0.001308     0.00\n",
      "NOTE:      9   400 0.000069           0.8116     0.8827  1.374e+04       1825          0   0.001308     0.00\n",
      "NOTE:     10   400 0.000069           0.8389     0.8845  1.288e+04       1682          0   0.001308     0.00\n",
      "NOTE:     11   400 0.000069           0.9343     0.8893  1.515e+04       1886          0   0.001308     0.00\n",
      "NOTE:     12   400 0.000069           0.9094     0.9086   1.53e+04       1539          0   0.001308     0.00\n",
      "NOTE:     13   400 0.000069           0.8793     0.8495  1.332e+04       2360          0   0.001308     0.00\n",
      "NOTE:     14   400 0.000069           0.7067     0.8529  1.061e+04       1830          0   0.001308     0.00\n",
      "NOTE:     15   400 0.000069           0.8379     0.8571  1.492e+04       2489          0   0.001308     0.00\n",
      "NOTE:     16   400 0.000069            1.005     0.8819  1.599e+04       2141          0   0.001309     0.00\n",
      "NOTE:     17   400 0.000069           0.9558     0.8529  1.487e+04       2565          0   0.001309     0.00\n",
      "NOTE:     18   400 0.000069           0.8309       0.86  1.277e+04       2079          0   0.001309     0.00\n",
      "NOTE:     19   400 0.000069           0.8757     0.8975  1.485e+04       1696          0   0.001309     0.00\n",
      "NOTE:     20   400 0.000069            1.041      0.842  1.527e+04       2867          0   0.001309     0.00\n",
      "NOTE:     21   400 0.000069           0.9556     0.8452  1.517e+04       2777          0   0.001309     0.00\n",
      "NOTE:     22   400 0.000069             0.75     0.8277  1.124e+04       2340          0   0.001309     0.00\n",
      "NOTE:     23   400 0.000069           0.8429     0.8279  1.258e+04       2616          0   0.001309     0.00\n",
      "NOTE:     24   400 0.000069           0.8395     0.8484  1.338e+04       2392          0   0.001309     0.00\n",
      "NOTE:     25   400 0.000069           0.7755      0.887  1.301e+04       1657          0   0.001309     0.00\n",
      "NOTE:     26   400 0.000069           0.8843     0.9198  1.668e+04       1454          0   0.001309     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  144      0.0001          0.8799     0.8666  3.804e+05  5.857e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000069           0.9711     0.8188  1.363e+04       3018          0   0.001309     0.00\n",
      "NOTE:      1   400 0.000069           0.9905     0.8256  1.367e+04       2888          0   0.001309     0.00\n",
      "NOTE:      2   400 0.000069           0.7907     0.8729  1.413e+04       2058          0   0.001309     0.00\n",
      "NOTE:      3   400 0.000069           0.9074     0.8515  1.383e+04       2413          0   0.001309     0.00\n",
      "NOTE:      4   400 0.000069            0.796     0.8571  1.202e+04       2003          0   0.001309     0.00\n",
      "NOTE:      5   400 0.000069           0.7811     0.8541  1.278e+04       2182          0   0.001309     0.00\n",
      "NOTE:      6   400 0.000069           0.9513     0.8501  1.457e+04       2569          0   0.001309     0.00\n",
      "NOTE:      7   400 0.000069           0.9499     0.8587  1.484e+04       2442          0   0.001309     0.00\n",
      "NOTE:      8   400 0.000069           0.9359     0.8537  1.436e+04       2461          0   0.001309     0.00\n",
      "NOTE:      9   400 0.000069           0.9047     0.8403    1.4e+04       2660          0   0.001309     0.00\n",
      "NOTE:     10   400 0.000069           0.7853      0.868  1.146e+04       1743          0   0.001309     0.00\n",
      "NOTE:     11   400 0.000069            1.031     0.8816  1.613e+04       2166          0   0.001309     0.00\n",
      "NOTE:     12   400 0.000069           0.8873     0.8629  1.459e+04       2318          0   0.001309     0.00\n",
      "NOTE:     13   400 0.000069            0.972     0.8907  1.677e+04       2059          0   0.001309     0.00\n",
      "NOTE:     14   400 0.000069           0.8711      0.879  1.531e+04       2107          0   0.001309     0.00\n",
      "NOTE:     15   400 0.000069           0.8222     0.8474  1.191e+04       2146          0   0.001309     0.00\n",
      "NOTE:     16   400 0.000069           0.7047     0.8693  1.065e+04       1601          0   0.001309     0.00\n",
      "NOTE:     17   400 0.000069           0.9379     0.8557  1.605e+04       2706          0   0.001309     0.00\n",
      "NOTE:     18   400 0.000069           0.8818     0.8599  1.398e+04       2277          0   0.001309     0.00\n",
      "NOTE:     19   400 0.000069           0.8703     0.8965  1.453e+04       1677          0   0.001309     0.00\n",
      "NOTE:     20   400 0.000069            0.831     0.8579  1.334e+04       2210          0   0.001309     0.00\n",
      "NOTE:     21   400 0.000069           0.9193     0.8584  1.429e+04       2357          0   0.001309     0.00\n",
      "NOTE:     22   400 0.000069           0.8525     0.8535  1.329e+04       2282          0   0.001309     0.00\n",
      "NOTE:     23   400 0.000069           0.8691     0.8775  1.333e+04       1861          0   0.001309     0.00\n",
      "NOTE:     24   400 0.000069           0.7902     0.8426  1.201e+04       2244          0   0.001309     0.00\n",
      "NOTE:     25   400 0.000069           0.8207     0.8602  1.242e+04       2019          0   0.001309     0.00\n",
      "NOTE:     26   400 0.000069           0.8975     0.8327  1.375e+04       2764          0   0.001309     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  145      0.0001          0.8786     0.8585  3.716e+05  6.123e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000055            0.978     0.8279  1.331e+04       2768          0   0.001309     0.00\n",
      "NOTE:      1   400 0.000055           0.7152      0.867   1.12e+04       1718          0   0.001309     0.00\n",
      "NOTE:      2   400 0.000055           0.7685     0.8933  1.211e+04       1447          0   0.001309     0.00\n",
      "NOTE:      3   400 0.000055            0.925     0.8471  1.493e+04       2695          0   0.001309     0.00\n",
      "NOTE:      4   400 0.000055           0.7557     0.8414   1.09e+04       2053          0   0.001309     0.00\n",
      "NOTE:      5   400 0.000055           0.8814     0.8572  1.368e+04       2280          0   0.001309     0.00\n",
      "NOTE:      6   400 0.000055           0.8584     0.9015  1.413e+04       1544          0   0.001309     0.00\n",
      "NOTE:      7   400 0.000055           0.7929     0.8532  1.323e+04       2276          0   0.001309     0.00\n",
      "NOTE:      8   400 0.000055           0.7327     0.8611  1.188e+04       1916          0   0.001309     0.00\n",
      "NOTE:      9   400 0.000055             0.82     0.8757  1.326e+04       1882          0   0.001309     0.00\n",
      "NOTE:     10   400 0.000055           0.8672      0.834  1.313e+04       2613          0   0.001309     0.00\n",
      "NOTE:     11   400 0.000055            1.001     0.8507  1.484e+04       2605          0   0.001309     0.00\n",
      "NOTE:     12   400 0.000055            0.867      0.887  1.519e+04       1935          0   0.001309     0.00\n",
      "NOTE:     13   400 0.000055              0.8     0.8324  1.209e+04       2433          0   0.001309     0.00\n",
      "NOTE:     14   400 0.000055           0.8469     0.8809  1.437e+04       1943          0    0.00131     0.00\n",
      "NOTE:     15   400 0.000055            0.847     0.8435  1.161e+04       2155          0    0.00131     0.00\n",
      "NOTE:     16   400 0.000055           0.9899      0.844  1.529e+04       2826          0    0.00131     0.00\n",
      "NOTE:     17   400 0.000055           0.7474     0.8528  1.325e+04       2287          0    0.00131     0.00\n",
      "NOTE:     18   400 0.000055            0.901     0.8617  1.391e+04       2233          0    0.00131     0.00\n",
      "NOTE:     19   400 0.000055           0.8943     0.8653  1.432e+04       2229          0    0.00131     0.00\n",
      "NOTE:     20   400 0.000055           0.8804     0.8207  1.249e+04       2728          0    0.00131     0.00\n",
      "NOTE:     21   400 0.000055           0.7363      0.863  1.262e+04       2003          0    0.00131     0.00\n",
      "NOTE:     22   400 0.000055           0.8388     0.8574  1.222e+04       2032          0    0.00131     0.00\n",
      "NOTE:     23   400 0.000055           0.8964     0.8444  1.315e+04       2422          0    0.00131     0.00\n",
      "NOTE:     24   400 0.000055           0.9059     0.8729  1.506e+04       2193          0    0.00131     0.00\n",
      "NOTE:     25   400 0.000055            1.069     0.8277  1.619e+04       3370          0    0.00131     0.00\n",
      "NOTE:     26   400 0.000055           0.9015     0.8617  1.298e+04       2083          0    0.00131     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  146       55E-6          0.8599     0.8562  3.613e+05  6.067e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000055            0.829     0.8689  1.249e+04       1885          0    0.00131     0.00\n",
      "NOTE:      1   400 0.000055           0.7477     0.8108   1.08e+04       2520          0    0.00131     0.00\n",
      "NOTE:      2   400 0.000055           0.7367     0.8931  1.181e+04       1413          0    0.00131     0.00\n",
      "NOTE:      3   400 0.000055           0.7807     0.9095  1.497e+04       1489          0    0.00131     0.00\n",
      "NOTE:      4   400 0.000055           0.8716     0.8693  1.331e+04       2000          0    0.00131     0.00\n",
      "NOTE:      5   400 0.000055           0.8838     0.8486  1.395e+04       2489          0    0.00131     0.00\n",
      "NOTE:      6   400 0.000055           0.8795     0.8427  1.392e+04       2598          0    0.00131     0.00\n",
      "NOTE:      7   400 0.000055            0.865     0.8821  1.432e+04       1914          0    0.00131     0.00\n",
      "NOTE:      8   400 0.000055           0.9299     0.8543  1.456e+04       2484          0    0.00131     0.00\n",
      "NOTE:      9   400 0.000055           0.8516     0.9088  1.482e+04       1487          0    0.00131     0.00\n",
      "NOTE:     10   400 0.000055           0.6985     0.8435       9920       1841          0    0.00131     0.00\n",
      "NOTE:     11   400 0.000055            0.953     0.8633  1.513e+04       2396          0    0.00131     0.00\n",
      "NOTE:     12   400 0.000055           0.9276     0.8345  1.326e+04       2630          0    0.00131     0.00\n",
      "NOTE:     13   400 0.000055            0.937     0.8637  1.395e+04       2202          0    0.00131     0.00\n",
      "NOTE:     14   400 0.000055           0.8003     0.8845  1.308e+04       1708          0    0.00131     0.00\n",
      "NOTE:     15   400 0.000055           0.7713     0.8372  1.177e+04       2287          0    0.00131     0.00\n",
      "NOTE:     16   400 0.000055           0.7078     0.8798  1.216e+04       1660          0    0.00131     0.00\n",
      "NOTE:     17   400 0.000055           0.7414     0.8562  1.051e+04       1765          0    0.00131     0.00\n",
      "NOTE:     18   400 0.000055           0.9153     0.8573  1.319e+04       2195          0    0.00131     0.00\n",
      "NOTE:     19   400 0.000055           0.8744     0.8643  1.374e+04       2157          0    0.00131     0.00\n",
      "NOTE:     20   400 0.000055            0.923     0.8444  1.486e+04       2737          0    0.00131     0.00\n",
      "NOTE:     21   400 0.000055           0.8522     0.8792  1.401e+04       1925          0    0.00131     0.00\n",
      "NOTE:     22   400 0.000055           0.7643     0.8986  1.285e+04       1450          0    0.00131     0.00\n",
      "NOTE:     23   400 0.000055           0.9174     0.8318  1.297e+04       2624          0    0.00131     0.00\n",
      "NOTE:     24   400 0.000055            0.843     0.8609   1.32e+04       2134          0    0.00131     0.00\n",
      "NOTE:     25   400 0.000055           0.8315     0.8583  1.244e+04       2054          0    0.00131     0.00\n",
      "NOTE:     26   400 0.000055           0.8306     0.8419  1.226e+04       2302          0    0.00131     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  147       55E-6          0.8394     0.8628  3.542e+05  5.635e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000055            0.869     0.8711  1.424e+04       2106          0    0.00131     0.00\n",
      "NOTE:      1   400 0.000055            0.831     0.8638  1.243e+04       1960          0    0.00131     0.00\n",
      "NOTE:      2   400 0.000055           0.8436     0.8287  1.262e+04       2609          0    0.00131     0.00\n",
      "NOTE:      3   400 0.000055           0.8998     0.8761  1.453e+04       2055          0    0.00131     0.00\n",
      "NOTE:      4   400 0.000055           0.8931     0.8483  1.428e+04       2553          0    0.00131     0.00\n",
      "NOTE:      5   400 0.000055            0.729     0.8072  1.018e+04       2431          0    0.00131     0.00\n",
      "NOTE:      6   400 0.000055           0.9121     0.8506  1.475e+04       2591          0    0.00131     0.00\n",
      "NOTE:      7   400 0.000055           0.9653     0.8405  1.433e+04       2719          0    0.00131     0.00\n",
      "NOTE:      8   400 0.000055           0.9958     0.8763  1.547e+04       2185          0    0.00131     0.00\n",
      "NOTE:      9   400 0.000055           0.9527      0.831  1.399e+04       2845          0    0.00131     0.00\n",
      "NOTE:     10   400 0.000055           0.8688     0.8869  1.469e+04       1874          0    0.00131     0.00\n",
      "NOTE:     11   400 0.000055           0.9879     0.8616  1.549e+04       2488          0    0.00131     0.00\n",
      "NOTE:     12   400 0.000055           0.7728       0.88  1.273e+04       1735          0    0.00131     0.00\n",
      "NOTE:     13   400 0.000055           0.9339     0.8485  1.356e+04       2420          0    0.00131     0.00\n",
      "NOTE:     14   400 0.000055           0.7865      0.857  1.271e+04       2121          0    0.00131     0.00\n",
      "NOTE:     15   400 0.000055           0.9453     0.9155   1.55e+04       1430          0    0.00131     0.00\n",
      "NOTE:     16   400 0.000055           0.7631     0.8646   1.09e+04       1707          0    0.00131     0.00\n",
      "NOTE:     17   400 0.000055           0.9605     0.8812  1.418e+04       1912          0    0.00131     0.00\n",
      "NOTE:     18   400 0.000055           0.8796     0.8722  1.363e+04       1998          0    0.00131     0.00\n",
      "NOTE:     19   400 0.000055           0.8731     0.8744  1.373e+04       1971          0    0.00131     0.00\n",
      "NOTE:     20   400 0.000055           0.9042     0.8977  1.529e+04       1742          0    0.00131     0.00\n",
      "NOTE:     21   400 0.000055            0.654     0.8647       9356       1464          0    0.00131     0.00\n",
      "NOTE:     22   400 0.000055           0.9703     0.8315  1.453e+04       2943          0    0.00131     0.00\n",
      "NOTE:     23   400 0.000055           0.8346     0.8816  1.432e+04       1924          0    0.00131     0.00\n",
      "NOTE:     24   400 0.000055           0.8788     0.8318   1.36e+04       2750          0    0.00131     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     25   400 0.000055           0.7934     0.8605  1.315e+04       2131          0    0.00131     0.00\n",
      "NOTE:     26   400 0.000055           0.7665      0.891   1.38e+04       1689          0    0.00131     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  148       55E-6          0.8691     0.8631   3.68e+05  5.835e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000055           0.8498     0.8445  1.193e+04       2197          0    0.00131     0.00\n",
      "NOTE:      1   400 0.000055            0.938     0.8438  1.467e+04       2714          0    0.00131     0.00\n",
      "NOTE:      2   400 0.000055           0.7245     0.8721  1.262e+04       1852          0    0.00131     0.00\n",
      "NOTE:      3   400 0.000055           0.8431     0.8543  1.257e+04       2143          0    0.00131     0.00\n",
      "NOTE:      4   400 0.000055           0.8226     0.8826  1.276e+04       1697          0    0.00131     0.00\n",
      "NOTE:      5   400 0.000055           0.9465     0.8983  1.594e+04       1805          0    0.00131     0.00\n",
      "NOTE:      6   400 0.000055           0.8413     0.8706  1.356e+04       2016          0    0.00131     0.00\n",
      "NOTE:      7   400 0.000055           0.9521     0.8664  1.525e+04       2352          0    0.00131     0.00\n",
      "NOTE:      8   400 0.000055            0.862     0.8578  1.314e+04       2179          0    0.00131     0.00\n",
      "NOTE:      9   400 0.000055           0.9427     0.8326  1.394e+04       2803          0    0.00131     0.00\n",
      "NOTE:     10   400 0.000055           0.8581      0.816  1.338e+04       3017          0    0.00131     0.00\n",
      "NOTE:     11   400 0.000055           0.9642     0.8722  1.552e+04       2274          0    0.00131     0.00\n",
      "NOTE:     12   400 0.000055           0.9403     0.8982  1.505e+04       1706          0    0.00131     0.00\n",
      "NOTE:     13   400 0.000055           0.7888     0.8692  1.307e+04       1966          0    0.00131     0.00\n",
      "NOTE:     14   400 0.000055           0.9061     0.8366   1.29e+04       2519          0    0.00131     0.00\n",
      "NOTE:     15   400 0.000055           0.8241     0.8659  1.338e+04       2071          0    0.00131     0.00\n",
      "NOTE:     16   400 0.000055           0.8068     0.8302  1.117e+04       2285          0    0.00131     0.00\n",
      "NOTE:     17   400 0.000055           0.8178     0.8642  1.224e+04       1924          0    0.00131     0.00\n",
      "NOTE:     18   400 0.000055           0.8503     0.8301  1.264e+04       2588          0    0.00131     0.00\n",
      "NOTE:     19   400 0.000055            0.731     0.8787  1.208e+04       1668          0    0.00131     0.00\n",
      "NOTE:     20   400 0.000055           0.7463     0.8432  1.215e+04       2259          0    0.00131     0.00\n",
      "NOTE:     21   400 0.000055           0.9004     0.8794   1.48e+04       2030          0    0.00131     0.00\n",
      "NOTE:     22   400 0.000055           0.9638     0.8719  1.476e+04       2168          0    0.00131     0.00\n",
      "NOTE:     23   400 0.000055           0.8466     0.8598    1.3e+04       2120          0    0.00131     0.00\n",
      "NOTE:     24   400 0.000055           0.8783     0.8465  1.328e+04       2408          0    0.00131     0.00\n",
      "NOTE:     25   400 0.000055           0.8829     0.8313  1.119e+04       2272          0    0.00131     0.00\n",
      "NOTE:     26   400 0.000055           0.8188      0.877   1.31e+04       1836          0    0.00131     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  149       55E-6           0.861     0.8595  3.601e+05  5.887e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000055           0.8105     0.8139  1.106e+04       2529          0    0.00131     0.00\n",
      "NOTE:      1   400 0.000055           0.9403     0.8364  1.297e+04       2538          0    0.00131     0.00\n",
      "NOTE:      2   400 0.000055           0.8154     0.8543  1.244e+04       2122          0    0.00131     0.00\n",
      "NOTE:      3   400 0.000055           0.7823     0.8761  1.343e+04       1899          0    0.00131     0.00\n",
      "NOTE:      4   400 0.000055            0.904     0.8691   1.45e+04       2184          0    0.00131     0.00\n",
      "NOTE:      5   400 0.000055           0.8481     0.8863  1.419e+04       1819          0    0.00131     0.00\n",
      "NOTE:      6   400 0.000055           0.8685     0.8521  1.388e+04       2410          0    0.00131     0.00\n",
      "NOTE:      7   400 0.000055           0.9265     0.8768  1.488e+04       2091          0    0.00131     0.00\n",
      "NOTE:      8   400 0.000055           0.9165     0.8433  1.412e+04       2622          0    0.00131     0.00\n",
      "NOTE:      9   400 0.000055           0.9106     0.8766  1.455e+04       2048          0    0.00131     0.00\n",
      "NOTE:     10   400 0.000055           0.8507     0.8747  1.317e+04       1886          0    0.00131     0.00\n",
      "NOTE:     11   400 0.000055           0.9431     0.8695  1.562e+04       2345          0    0.00131     0.00\n",
      "NOTE:     12   400 0.000055           0.8414     0.8586  1.377e+04       2268          0    0.00131     0.00\n",
      "NOTE:     13   400 0.000055           0.9366      0.861  1.502e+04       2425          0    0.00131     0.00\n",
      "NOTE:     14   400 0.000055            0.712     0.8265  1.097e+04       2303          0   0.001311     0.00\n",
      "NOTE:     15   400 0.000055           0.9348     0.8416  1.283e+04       2415          0   0.001311     0.00\n",
      "NOTE:     16   400 0.000055           0.7937     0.8634  1.191e+04       1884          0   0.001311     0.00\n",
      "NOTE:     17   400 0.000055           0.7667     0.8782  1.196e+04       1659          0   0.001311     0.00\n",
      "NOTE:     18   400 0.000055           0.8799     0.8422   1.29e+04       2417          0   0.001311     0.00\n",
      "NOTE:     19   400 0.000055            1.034     0.8677  1.519e+04       2316          0   0.001311     0.00\n",
      "NOTE:     20   400 0.000055            0.757     0.9054  1.319e+04       1378          0   0.001311     0.00\n",
      "NOTE:     21   400 0.000055           0.9084     0.8197  1.283e+04       2821          0   0.001311     0.00\n",
      "NOTE:     22   400 0.000055           0.7954     0.8755  1.294e+04       1839          0   0.001311     0.00\n",
      "NOTE:     23   400 0.000055           0.7849     0.7899  1.095e+04       2912          0   0.001311     0.00\n",
      "NOTE:     24   400 0.000055           0.8964     0.8548  1.381e+04       2345          0   0.001311     0.00\n",
      "NOTE:     25   400 0.000055           0.8174     0.8289  1.268e+04       2617          0   0.001311     0.00\n",
      "NOTE:     26   400 0.000055           0.7544     0.8907  1.382e+04       1696          0   0.001311     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  150       55E-6          0.8567     0.8574  3.596e+05  5.979e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000055           0.8967     0.8491   1.35e+04       2399          0   0.001311     0.00\n",
      "NOTE:      1   400 0.000055            1.023     0.8691  1.678e+04       2528          0   0.001311     0.00\n",
      "NOTE:      2   400 0.000055           0.8617     0.8525  1.279e+04       2213          0   0.001311     0.00\n",
      "NOTE:      3   400 0.000055            1.047     0.8381  1.514e+04       2923          0   0.001311     0.00\n",
      "NOTE:      4   400 0.000055           0.8508     0.8249  1.191e+04       2528          0   0.001311     0.00\n",
      "NOTE:      5   400 0.000055           0.9341     0.8656  1.365e+04       2119          0   0.001311     0.00\n",
      "NOTE:      6   400 0.000055           0.9289     0.8774  1.433e+04       2001          0   0.001311     0.00\n",
      "NOTE:      7   400 0.000055           0.8121     0.8555  1.212e+04       2046          0   0.001311     0.00\n",
      "NOTE:      8   400 0.000055           0.9148     0.8703  1.503e+04       2241          0   0.001311     0.00\n",
      "NOTE:      9   400 0.000055           0.8982     0.8635  1.456e+04       2301          0   0.001311     0.00\n",
      "NOTE:     10   400 0.000055           0.8535     0.8578  1.437e+04       2382          0   0.001311     0.00\n",
      "NOTE:     11   400 0.000055           0.8447     0.8704  1.411e+04       2101          0   0.001311     0.00\n",
      "NOTE:     12   400 0.000055           0.8205     0.8647  1.306e+04       2043          0   0.001311     0.00\n",
      "NOTE:     13   400 0.000055           0.9876     0.8955  1.682e+04       1964          0   0.001311     0.00\n",
      "NOTE:     14   400 0.000055           0.8889     0.8732  1.515e+04       2199          0   0.001311     0.00\n",
      "NOTE:     15   400 0.000055           0.8483     0.7682  1.056e+04       3186          0   0.001311     0.00\n",
      "NOTE:     16   400 0.000055            1.024      0.859  1.565e+04       2568          0   0.001311     0.00\n",
      "NOTE:     17   400 0.000055           0.9043     0.8815  1.464e+04       1969          0   0.001311     0.00\n",
      "NOTE:     18   400 0.000055           0.7525      0.839   1.08e+04       2073          0   0.001311     0.00\n",
      "NOTE:     19   400 0.000055           0.8545     0.8738  1.335e+04       1928          0   0.001311     0.00\n",
      "NOTE:     20   400 0.000055           0.9568     0.8665  1.507e+04       2322          0   0.001311     0.00\n",
      "NOTE:     21   400 0.000055           0.9056     0.8574  1.455e+04       2420          0   0.001311     0.00\n",
      "NOTE:     22   400 0.000055             1.07     0.8545  1.616e+04       2752          0   0.001311     0.00\n",
      "NOTE:     23   400 0.000055            0.745     0.8441  1.124e+04       2075          0   0.001311     0.00\n",
      "NOTE:     24   400 0.000055           0.8808     0.9133   1.64e+04       1557          0   0.001311     0.00\n",
      "NOTE:     25   400 0.000055           0.8767     0.8694  1.412e+04       2121          0   0.001311     0.00\n",
      "NOTE:     26   400 0.000055            1.056     0.8708  1.723e+04       2557          0   0.001311     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  151       55E-6          0.9051     0.8616  3.831e+05  6.152e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000055           0.9381     0.8394  1.352e+04       2586          0   0.001311     0.00\n",
      "NOTE:      1   400 0.000055           0.9035     0.8357  1.308e+04       2570          0   0.001311     0.00\n",
      "NOTE:      2   400 0.000055           0.9947       0.86  1.501e+04       2443          0   0.001311     0.00\n",
      "NOTE:      3   400 0.000055            1.192      0.803   1.61e+04       3950          0   0.001311     0.00\n",
      "NOTE:      4   400 0.000055           0.9042     0.8731  1.453e+04       2111          0   0.001311     0.00\n",
      "NOTE:      5   400 0.000055           0.7775     0.8892   1.52e+04       1894          0   0.001311     0.00\n",
      "NOTE:      6   400 0.000055           0.8955     0.8874  1.506e+04       1912          0   0.001311     0.00\n",
      "NOTE:      7   400 0.000055           0.8781     0.8533  1.379e+04       2370          0   0.001311     0.00\n",
      "NOTE:      8   400 0.000055           0.6859     0.8641  1.118e+04       1758          0   0.001311     0.00\n",
      "NOTE:      9   400 0.000055           0.7758     0.8555  1.177e+04       1989          0   0.001311     0.00\n",
      "NOTE:     10   400 0.000055           0.7559     0.8464  1.116e+04       2026          0   0.001311     0.00\n",
      "NOTE:     11   400 0.000055           0.9772     0.8522  1.547e+04       2684          0   0.001311     0.00\n",
      "NOTE:     12   400 0.000055           0.9594     0.8753  1.629e+04       2321          0   0.001311     0.00\n",
      "NOTE:     13   400 0.000055            0.913     0.8857  1.444e+04       1863          0   0.001311     0.00\n",
      "NOTE:     14   400 0.000055           0.8288     0.8387  1.288e+04       2476          0   0.001311     0.00\n",
      "NOTE:     15   400 0.000055            0.711      0.874  1.086e+04       1566          0   0.001311     0.00\n",
      "NOTE:     16   400 0.000055           0.9949     0.8486  1.481e+04       2642          0   0.001311     0.00\n",
      "NOTE:     17   400 0.000055           0.8117     0.8608  1.253e+04       2026          0   0.001311     0.00\n",
      "NOTE:     18   400 0.000055           0.7976     0.8581  1.187e+04       1962          0   0.001311     0.00\n",
      "NOTE:     19   400 0.000055           0.9184     0.9056  1.583e+04       1651          0   0.001311     0.00\n",
      "NOTE:     20   400 0.000055           0.8971     0.8656  1.417e+04       2200          0   0.001311     0.00\n",
      "NOTE:     21   400 0.000055             0.93     0.8312  1.372e+04       2786          0   0.001311     0.00\n",
      "NOTE:     22   400 0.000055           0.7675     0.8595  1.112e+04       1818          0   0.001311     0.00\n",
      "NOTE:     23   400 0.000055           0.8098     0.8784  1.465e+04       2029          0   0.001311     0.00\n",
      "NOTE:     24   400 0.000055           0.8251     0.8939  1.422e+04       1687          0   0.001311     0.00\n",
      "NOTE:     25   400 0.000055           0.8149     0.8926  1.395e+04       1678          0   0.001311     0.00\n",
      "NOTE:     26   400 0.000055           0.8953     0.8508  1.496e+04       2622          0   0.001311     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  152       55E-6          0.8723     0.8619  3.722e+05  5.962e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000055           0.8346     0.8422  1.293e+04       2423          0   0.001311     0.00\n",
      "NOTE:      1   400 0.000055           0.7957     0.8499  1.352e+04       2388          0   0.001311     0.00\n",
      "NOTE:      2   400 0.000055           0.7622      0.838  1.119e+04       2163          0   0.001311     0.00\n",
      "NOTE:      3   400 0.000055           0.9453     0.8696  1.569e+04       2353          0   0.001311     0.00\n",
      "NOTE:      4   400 0.000055           0.8949      0.835  1.242e+04       2453          0   0.001311     0.00\n",
      "NOTE:      5   400 0.000055           0.8021      0.867  1.194e+04       1831          0   0.001311     0.00\n",
      "NOTE:      6   400 0.000055           0.8325     0.8311  1.237e+04       2512          0   0.001311     0.00\n",
      "NOTE:      7   400 0.000055            1.033     0.8775  1.662e+04       2319          0   0.001311     0.00\n",
      "NOTE:      8   400 0.000055           0.8985      0.862  1.334e+04       2135          0   0.001311     0.00\n",
      "NOTE:      9   400 0.000055            1.041     0.8034  1.394e+04       3411          0   0.001311     0.00\n",
      "NOTE:     10   400 0.000055           0.8188     0.7877       9950       2681          0   0.001311     0.00\n",
      "NOTE:     11   400 0.000055           0.8909     0.8569  1.386e+04       2315          0   0.001311     0.00\n",
      "NOTE:     12   400 0.000055           0.9133     0.7967  1.272e+04       3247          0   0.001311     0.00\n",
      "NOTE:     13   400 0.000055           0.7602     0.8593  1.264e+04       2071          0   0.001311     0.00\n",
      "NOTE:     14   400 0.000055            1.006     0.8773  1.627e+04       2275          0   0.001311     0.00\n",
      "NOTE:     15   400 0.000055            0.752     0.8811  1.159e+04       1564          0   0.001311     0.00\n",
      "NOTE:     16   400 0.000055           0.7996     0.8924  1.369e+04       1651          0   0.001311     0.00\n",
      "NOTE:     17   400 0.000055           0.8587     0.8863  1.443e+04       1852          0   0.001311     0.00\n",
      "NOTE:     18   400 0.000055           0.7962     0.8807  1.385e+04       1876          0   0.001311     0.00\n",
      "NOTE:     19   400 0.000055           0.9709     0.8141   1.29e+04       2946          0   0.001311     0.00\n",
      "NOTE:     20   400 0.000055           0.8614      0.865  1.338e+04       2088          0   0.001311     0.00\n",
      "NOTE:     21   400 0.000055           0.7195     0.8491  1.093e+04       1942          0   0.001311     0.00\n",
      "NOTE:     22   400 0.000055           0.9336     0.8585  1.495e+04       2463          0   0.001311     0.00\n",
      "NOTE:     23   400 0.000055            1.056     0.8074  1.419e+04       3385          0   0.001311     0.00\n",
      "NOTE:     24   400 0.000055           0.7576      0.891   1.31e+04       1603          0   0.001311     0.00\n",
      "NOTE:     25   400 0.000055           0.8956     0.8555  1.291e+04       2181          0   0.001311     0.00\n",
      "NOTE:     26   400 0.000055           0.8744     0.8918  1.426e+04       1730          0   0.001311     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  153       55E-6          0.8706     0.8532  3.595e+05  6.186e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000055           0.8508     0.8529  1.226e+04       2114          0   0.001311     0.00\n",
      "NOTE:      1   400 0.000055           0.8375     0.8484  1.298e+04       2319          0   0.001311     0.00\n",
      "NOTE:      2   400 0.000055           0.8909     0.8621   1.36e+04       2176          0   0.001311     0.00\n",
      "NOTE:      3   400 0.000055           0.8284     0.8405  1.328e+04       2521          0   0.001311     0.00\n",
      "NOTE:      4   400 0.000055           0.7923     0.8816  1.281e+04       1720          0   0.001311     0.00\n",
      "NOTE:      5   400 0.000055           0.8476     0.8767  1.322e+04       1859          0   0.001311     0.00\n",
      "NOTE:      6   400 0.000055           0.9989     0.7944   1.35e+04       3494          0   0.001311     0.00\n",
      "NOTE:      7   400 0.000055            0.864     0.8697  1.395e+04       2090          0   0.001311     0.00\n",
      "NOTE:      8   400 0.000055           0.8552     0.8668   1.31e+04       2013          0   0.001311     0.00\n",
      "NOTE:      9   400 0.000055           0.8437      0.843  1.386e+04       2582          0   0.001311     0.00\n",
      "NOTE:     10   400 0.000055           0.9557     0.8612  1.511e+04       2436          0   0.001311     0.00\n",
      "NOTE:     11   400 0.000055             1.07     0.8511  1.643e+04       2875          0   0.001311     0.00\n",
      "NOTE:     12   400 0.000055           0.9464     0.8275  1.337e+04       2787          0   0.001311     0.00\n",
      "NOTE:     13   400 0.000055           0.8697     0.8859   1.52e+04       1957          0   0.001311     0.00\n",
      "NOTE:     14   400 0.000055           0.9169     0.8471  1.387e+04       2504          0   0.001311     0.00\n",
      "NOTE:     15   400 0.000055            1.011     0.8664  1.622e+04       2500          0   0.001311     0.00\n",
      "NOTE:     16   400 0.000055           0.8222     0.8439  1.284e+04       2374          0   0.001311     0.00\n",
      "NOTE:     17   400 0.000055           0.8639     0.8737  1.432e+04       2069          0   0.001311     0.00\n",
      "NOTE:     18   400 0.000055           0.8592     0.8912  1.385e+04       1690          0   0.001311     0.00\n",
      "NOTE:     19   400 0.000055           0.8264     0.8917  1.382e+04       1678          0   0.001311     0.00\n",
      "NOTE:     20   400 0.000055           0.7688     0.8774   1.28e+04       1788          0   0.001311     0.00\n",
      "NOTE:     21   400 0.000055           0.8353     0.8408  1.299e+04       2461          0   0.001311     0.00\n",
      "NOTE:     22   400 0.000055             0.72     0.8248  1.053e+04       2237          0   0.001311     0.00\n",
      "NOTE:     23   400 0.000055           0.8123     0.7908  1.202e+04       3178          0   0.001311     0.00\n",
      "NOTE:     24   400 0.000055           0.9919     0.8618  1.537e+04       2464          0   0.001312     0.00\n",
      "NOTE:     25   400 0.000055           0.9648     0.8349  1.246e+04       2464          0   0.001312     0.00\n",
      "NOTE:     26   400 0.000055            1.009     0.8828  1.626e+04       2158          0   0.001312     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  154       55E-6          0.8835     0.8555    3.7e+05  6.251e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000044           0.8281     0.8383   1.23e+04       2374          0   0.001312     0.00\n",
      "NOTE:      1   400 0.000044           0.7479     0.8667  1.206e+04       1855          0   0.001312     0.00\n",
      "NOTE:      2   400 0.000044           0.9051     0.8526   1.39e+04       2403          0   0.001312     0.00\n",
      "NOTE:      3   400 0.000044           0.9418     0.8432  1.471e+04       2736          0   0.001312     0.00\n",
      "NOTE:      4   400 0.000044            1.007     0.8005  1.351e+04       3368          0   0.001312     0.00\n",
      "NOTE:      5   400 0.000044           0.8367     0.8686  1.367e+04       2068          0   0.001312     0.00\n",
      "NOTE:      6   400 0.000044           0.9343     0.8785  1.539e+04       2129          0   0.001312     0.00\n",
      "NOTE:      7   400 0.000044           0.9264     0.8512  1.332e+04       2328          0   0.001312     0.00\n",
      "NOTE:      8   400 0.000044           0.7602     0.8694  1.247e+04       1873          0   0.001312     0.00\n",
      "NOTE:      9   400 0.000044           0.8993     0.8294  1.423e+04       2927          0   0.001312     0.00\n",
      "NOTE:     10   400 0.000044           0.8311     0.8811  1.445e+04       1950          0   0.001312     0.00\n",
      "NOTE:     11   400 0.000044           0.8514     0.8463  1.243e+04       2257          0   0.001312     0.00\n",
      "NOTE:     12   400 0.000044           0.7922      0.875  1.209e+04       1727          0   0.001312     0.00\n",
      "NOTE:     13   400 0.000044            1.055     0.8327  1.556e+04       3126          0   0.001312     0.00\n",
      "NOTE:     14   400 0.000044            0.779     0.8724  1.253e+04       1834          0   0.001312     0.00\n",
      "NOTE:     15   400 0.000044           0.9106     0.8586  1.546e+04       2546          0   0.001312     0.00\n",
      "NOTE:     16   400 0.000044           0.9247     0.8833  1.471e+04       1944          0   0.001312     0.00\n",
      "NOTE:     17   400 0.000044           0.8062     0.8203  1.165e+04       2553          0   0.001312     0.00\n",
      "NOTE:     18   400 0.000044           0.8177     0.8671  1.399e+04       2145          0   0.001312     0.00\n",
      "NOTE:     19   400 0.000044           0.8235       0.86  1.244e+04       2025          0   0.001312     0.00\n",
      "NOTE:     20   400 0.000044           0.8605      0.881  1.399e+04       1890          0   0.001312     0.00\n",
      "NOTE:     21   400 0.000044           0.8758     0.8758  1.427e+04       2023          0   0.001312     0.00\n",
      "NOTE:     22   400 0.000044           0.8137      0.864  1.331e+04       2096          0   0.001312     0.00\n",
      "NOTE:     23   400 0.000044           0.7773     0.8652   1.29e+04       2010          0   0.001312     0.00\n",
      "NOTE:     24   400 0.000044           0.7128     0.8623  1.183e+04       1888          0   0.001312     0.00\n",
      "NOTE:     25   400 0.000044           0.7783      0.833  1.195e+04       2395          0   0.001312     0.00\n",
      "NOTE:     26   400 0.000044           0.8416     0.8783  1.478e+04       2047          0   0.001312     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  155       44E-6          0.8533     0.8574  3.639e+05  6.052e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000044           0.9063     0.8638  1.508e+04       2378          0   0.001312     0.00\n",
      "NOTE:      1   400 0.000044           0.8822     0.8992  1.549e+04       1736          0   0.001312     0.00\n",
      "NOTE:      2   400 0.000044           0.9715     0.8379  1.337e+04       2585          0   0.001312     0.00\n",
      "NOTE:      3   400 0.000044           0.8778     0.8569  1.344e+04       2245          0   0.001312     0.00\n",
      "NOTE:      4   400 0.000044           0.8542     0.8487  1.356e+04       2418          0   0.001312     0.00\n",
      "NOTE:      5   400 0.000044           0.7717     0.8742  1.146e+04       1649          0   0.001312     0.00\n",
      "NOTE:      6   400 0.000044           0.8911     0.8928  1.549e+04       1860          0   0.001312     0.00\n",
      "NOTE:      7   400 0.000044           0.8288     0.9092  1.456e+04       1454          0   0.001312     0.00\n",
      "NOTE:      8   400 0.000044           0.8933     0.8644  1.366e+04       2142          0   0.001312     0.00\n",
      "NOTE:      9   400 0.000044           0.7263      0.871  1.141e+04       1691          0   0.001312     0.00\n",
      "NOTE:     10   400 0.000044           0.8856     0.8469  1.287e+04       2325          0   0.001312     0.00\n",
      "NOTE:     11   400 0.000044           0.9322     0.8684  1.514e+04       2294          0   0.001312     0.00\n",
      "NOTE:     12   400 0.000044           0.9231     0.8608  1.403e+04       2268          0   0.001312     0.00\n",
      "NOTE:     13   400 0.000044           0.8845     0.8993  1.607e+04       1800          0   0.001312     0.00\n",
      "NOTE:     14   400 0.000044            0.875     0.8751    1.4e+04       1998          0   0.001312     0.00\n",
      "NOTE:     15   400 0.000044           0.8972     0.8806  1.567e+04       2125          0   0.001312     0.00\n",
      "NOTE:     16   400 0.000044           0.9949     0.8666  1.553e+04       2390          0   0.001312     0.00\n",
      "NOTE:     17   400 0.000044           0.7973     0.8958  1.326e+04       1542          0   0.001312     0.00\n",
      "NOTE:     18   400 0.000044           0.9936      0.846  1.518e+04       2764          0   0.001312     0.00\n",
      "NOTE:     19   400 0.000044           0.8413     0.8463  1.213e+04       2203          0   0.001312     0.00\n",
      "NOTE:     20   400 0.000044           0.9269     0.8743  1.424e+04       2047          0   0.001312     0.00\n",
      "NOTE:     21   400 0.000044           0.9729     0.8479  1.379e+04       2472          0   0.001312     0.00\n",
      "NOTE:     22   400 0.000044           0.9968     0.8332  1.405e+04       2811          0   0.001312     0.00\n",
      "NOTE:     23   400 0.000044           0.9154     0.8975  1.511e+04       1725          0   0.001312     0.00\n",
      "NOTE:     24   400 0.000044           0.8534     0.8232  1.275e+04       2739          0   0.001312     0.00\n",
      "NOTE:     25   400 0.000044            1.045     0.8685  1.571e+04       2380          0   0.001312     0.00\n",
      "NOTE:     26   400 0.000044           0.6465     0.8985  1.082e+04       1222          0   0.001312     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  156       44E-6          0.8883     0.8684  3.779e+05  5.726e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000044           0.8777     0.8973  1.529e+04       1750          0   0.001312     0.00\n",
      "NOTE:      1   400 0.000044           0.8795     0.8754  1.408e+04       2004          0   0.001312     0.00\n",
      "NOTE:      2   400 0.000044           0.8483      0.854  1.229e+04       2100          0   0.001312     0.00\n",
      "NOTE:      3   400 0.000044           0.8434      0.863  1.216e+04       1931          0   0.001312     0.00\n",
      "NOTE:      4   400 0.000044            1.026      0.823  1.585e+04       3408          0   0.001312     0.00\n",
      "NOTE:      5   400 0.000044           0.8662     0.8799  1.406e+04       1920          0   0.001312     0.00\n",
      "NOTE:      6   400 0.000044           0.8563     0.9043  1.415e+04       1497          0   0.001312     0.00\n",
      "NOTE:      7   400 0.000044           0.5802     0.8645       9421       1477          0   0.001312     0.00\n",
      "NOTE:      8   400 0.000044            1.074      0.901  1.909e+04       2098          0   0.001312     0.00\n",
      "NOTE:      9   400 0.000044           0.9209     0.8425  1.352e+04       2526          0   0.001312     0.00\n",
      "NOTE:     10   400 0.000044           0.8868     0.8589  1.433e+04       2354          0   0.001312     0.00\n",
      "NOTE:     11   400 0.000044           0.9052     0.8257  1.234e+04       2604          0   0.001312     0.00\n",
      "NOTE:     12   400 0.000044           0.8661     0.8907  1.549e+04       1900          0   0.001312     0.00\n",
      "NOTE:     13   400 0.000044           0.8309     0.8749  1.285e+04       1838          0   0.001312     0.00\n",
      "NOTE:     14   400 0.000044           0.7751     0.8449  1.179e+04       2165          0   0.001312     0.00\n",
      "NOTE:     15   400 0.000044            0.922     0.8716  1.506e+04       2218          0   0.001312     0.00\n",
      "NOTE:     16   400 0.000044           0.8295     0.8328  1.256e+04       2522          0   0.001312     0.00\n",
      "NOTE:     17   400 0.000044           0.8667     0.9053  1.442e+04       1508          0   0.001312     0.00\n",
      "NOTE:     18   400 0.000044           0.8096     0.8572  1.273e+04       2121          0   0.001312     0.00\n",
      "NOTE:     19   400 0.000044             0.95      0.829  1.325e+04       2733          0   0.001312     0.00\n",
      "NOTE:     20   400 0.000044           0.7904     0.8902  1.368e+04       1687          0   0.001312     0.00\n",
      "NOTE:     21   400 0.000044           0.9363     0.8121  1.276e+04       2953          0   0.001312     0.00\n",
      "NOTE:     22   400 0.000044           0.7577     0.8688  1.299e+04       1962          0   0.001312     0.00\n",
      "NOTE:     23   400 0.000044            0.706     0.8474  1.095e+04       1971          0   0.001312     0.00\n",
      "NOTE:     24   400 0.000044           0.8019      0.849  1.086e+04       1931          0   0.001312     0.00\n",
      "NOTE:     25   400 0.000044           0.8744     0.8505  1.342e+04       2358          0   0.001312     0.00\n",
      "NOTE:     26   400 0.000044           0.6999     0.8481  1.111e+04       1991          0   0.001312     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  157       44E-6          0.8511     0.8624  3.605e+05  5.753e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000044           0.7232     0.8644  1.221e+04       1915          0   0.001312     0.00\n",
      "NOTE:      1   400 0.000044           0.9137     0.8754  1.485e+04       2114          0   0.001312     0.00\n",
      "NOTE:      2   400 0.000044            1.075     0.7995  1.476e+04       3700          0   0.001312     0.00\n",
      "NOTE:      3   400 0.000044            1.085      0.847  1.712e+04       3092          0   0.001312     0.00\n",
      "NOTE:      4   400 0.000044            1.016     0.8912  1.642e+04       2004          0   0.001312     0.00\n",
      "NOTE:      5   400 0.000044           0.8423     0.8461  1.346e+04       2448          0   0.001312     0.00\n",
      "NOTE:      6   400 0.000044           0.9365     0.8239  1.383e+04       2955          0   0.001312     0.00\n",
      "NOTE:      7   400 0.000044            1.013     0.8542  1.483e+04       2532          0   0.001312     0.00\n",
      "NOTE:      8   400 0.000044           0.9654      0.837  1.465e+04       2853          0   0.001312     0.00\n",
      "NOTE:      9   400 0.000044           0.8362     0.8849  1.343e+04       1746          0   0.001312     0.00\n",
      "NOTE:     10   400 0.000044           0.9046     0.8609   1.33e+04       2148          0   0.001312     0.00\n",
      "NOTE:     11   400 0.000044           0.9119     0.8586  1.336e+04       2201          0   0.001312     0.00\n",
      "NOTE:     12   400 0.000044            1.054     0.8634  1.697e+04       2686          0   0.001312     0.00\n",
      "NOTE:     13   400 0.000044           0.8458     0.8839  1.344e+04       1764          0   0.001312     0.00\n",
      "NOTE:     14   400 0.000044           0.9165     0.8528  1.428e+04       2465          0   0.001312     0.00\n",
      "NOTE:     15   400 0.000044           0.9614     0.8527  1.535e+04       2651          0   0.001312     0.00\n",
      "NOTE:     16   400 0.000044           0.9851     0.8359  1.542e+04       3027          0   0.001312     0.00\n",
      "NOTE:     17   400 0.000044           0.9673      0.855  1.625e+04       2757          0   0.001312     0.00\n",
      "NOTE:     18   400 0.000044           0.8781     0.8851  1.451e+04       1883          0   0.001312     0.00\n",
      "NOTE:     19   400 0.000044           0.7527     0.8614  1.185e+04       1907          0   0.001312     0.00\n",
      "NOTE:     20   400 0.000044           0.8773     0.8989  1.529e+04       1719          0   0.001312     0.00\n",
      "NOTE:     21   400 0.000044           0.9172     0.8836  1.423e+04       1874          0   0.001312     0.00\n",
      "NOTE:     22   400 0.000044           0.8042     0.8567  1.228e+04       2055          0   0.001312     0.00\n",
      "NOTE:     23   400 0.000044           0.8994     0.8651  1.538e+04       2399          0   0.001312     0.00\n",
      "NOTE:     24   400 0.000044           0.8968     0.8538  1.415e+04       2423          0   0.001312     0.00\n",
      "NOTE:     25   400 0.000044           0.9177     0.8652  1.437e+04       2240          0   0.001312     0.00\n",
      "NOTE:     26   400 0.000044           0.9068     0.8464  1.345e+04       2441          0   0.001312     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  158       44E-6          0.9187     0.8589  3.894e+05    6.4e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000044           0.6904     0.8433  1.066e+04       1981          0   0.001312     0.00\n",
      "NOTE:      1   400 0.000044           0.9011     0.8541  1.399e+04       2390          0   0.001312     0.00\n",
      "NOTE:      2   400 0.000044           0.7028     0.8597  1.091e+04       1780          0   0.001312     0.00\n",
      "NOTE:      3   400 0.000044           0.8744      0.877  1.463e+04       2052          0   0.001312     0.00\n",
      "NOTE:      4   400 0.000044            0.945     0.8582   1.53e+04       2529          0   0.001312     0.00\n",
      "NOTE:      5   400 0.000044            1.025     0.8739  1.455e+04       2100          0   0.001312     0.00\n",
      "NOTE:      6   400 0.000044           0.8353     0.8707  1.342e+04       1993          0   0.001312     0.00\n",
      "NOTE:      7   400 0.000044           0.8837     0.8825  1.509e+04       2008          0   0.001312     0.00\n",
      "NOTE:      8   400 0.000044           0.8965      0.854  1.435e+04       2453          0   0.001312     0.00\n",
      "NOTE:      9   400 0.000044           0.9483     0.8621  1.457e+04       2332          0   0.001312     0.00\n",
      "NOTE:     10   400 0.000044           0.8836     0.8256  1.277e+04       2696          0   0.001312     0.00\n",
      "NOTE:     11   400 0.000044           0.8472     0.8487  1.261e+04       2248          0   0.001312     0.00\n",
      "NOTE:     12   400 0.000044           0.9144     0.8529  1.363e+04       2349          0   0.001312     0.00\n",
      "NOTE:     13   400 0.000044           0.8797     0.8833  1.537e+04       2031          0   0.001312     0.00\n",
      "NOTE:     14   400 0.000044           0.8692     0.8161  1.159e+04       2610          0   0.001312     0.00\n",
      "NOTE:     15   400 0.000044            0.882      0.871  1.385e+04       2051          0   0.001312     0.00\n",
      "NOTE:     16   400 0.000044           0.8618     0.8389  1.245e+04       2392          0   0.001312     0.00\n",
      "NOTE:     17   400 0.000044           0.7587      0.889  1.251e+04       1563          0   0.001312     0.00\n",
      "NOTE:     18   400 0.000044           0.8218     0.8611  1.241e+04       2002          0   0.001312     0.00\n",
      "NOTE:     19   400 0.000044           0.8556     0.8391  1.319e+04       2530          0   0.001312     0.00\n",
      "NOTE:     20   400 0.000044           0.8389     0.8429  1.252e+04       2334          0   0.001312     0.00\n",
      "NOTE:     21   400 0.000044            1.009     0.8718  1.485e+04       2184          0   0.001312     0.00\n",
      "NOTE:     22   400 0.000044            0.879     0.8258  1.423e+04       3001          0   0.001312     0.00\n",
      "NOTE:     23   400 0.000044           0.8525     0.8631  1.323e+04       2098          0   0.001312     0.00\n",
      "NOTE:     24   400 0.000044           0.9169     0.8245  1.303e+04       2774          0   0.001312     0.00\n",
      "NOTE:     25   400 0.000044           0.8859       0.85  1.339e+04       2362          0   0.001312     0.00\n",
      "NOTE:     26   400 0.000044           0.8706     0.8772  1.378e+04       1929          0   0.001312     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  159       44E-6          0.8714     0.8565  3.629e+05  6.077e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000044           0.8323     0.8774  1.342e+04       1875          0   0.001312     0.00\n",
      "NOTE:      1   400 0.000044           0.9731     0.8549  1.619e+04       2749          0   0.001312     0.00\n",
      "NOTE:      2   400 0.000044           0.8268     0.8513  1.316e+04       2299          0   0.001312     0.00\n",
      "NOTE:      3   400 0.000044           0.6956     0.8322       9570       1930          0   0.001312     0.00\n",
      "NOTE:      4   400 0.000044           0.9623     0.8593  1.452e+04       2377          0   0.001312     0.00\n",
      "NOTE:      5   400 0.000044           0.8969      0.874  1.407e+04       2028          0   0.001312     0.00\n",
      "NOTE:      6   400 0.000044           0.9132     0.8558  1.451e+04       2445          0   0.001312     0.00\n",
      "NOTE:      7   400 0.000044            0.999     0.9067  1.713e+04       1762          0   0.001312     0.00\n",
      "NOTE:      8   400 0.000044           0.9778     0.8391  1.562e+04       2996          0   0.001312     0.00\n",
      "NOTE:      9   400 0.000044           0.8268     0.8956  1.505e+04       1755          0   0.001312     0.00\n",
      "NOTE:     10   400 0.000044           0.9552      0.839  1.434e+04       2751          0   0.001312     0.00\n",
      "NOTE:     11   400 0.000044            1.001     0.8629  1.573e+04       2499          0   0.001312     0.00\n",
      "NOTE:     12   400 0.000044           0.9287     0.8689  1.501e+04       2265          0   0.001312     0.00\n",
      "NOTE:     13   400 0.000044           0.8542     0.8946  1.392e+04       1640          0   0.001312     0.00\n",
      "NOTE:     14   400 0.000044           0.9495      0.829  1.444e+04       2977          0   0.001312     0.00\n",
      "NOTE:     15   400 0.000044           0.8464     0.8735  1.396e+04       2022          0   0.001312     0.00\n",
      "NOTE:     16   400 0.000044           0.8996     0.8917  1.505e+04       1828          0   0.001312     0.00\n",
      "NOTE:     17   400 0.000044            0.839      0.857  1.254e+04       2092          0   0.001312     0.00\n",
      "NOTE:     18   400 0.000044           0.8895      0.855  1.346e+04       2282          0   0.001312     0.00\n",
      "NOTE:     19   400 0.000044           0.9124     0.8693  1.411e+04       2121          0   0.001312     0.00\n",
      "NOTE:     20   400 0.000044           0.9483      0.821  1.392e+04       3035          0   0.001312     0.00\n",
      "NOTE:     21   400 0.000044           0.7895      0.875  1.265e+04       1807          0   0.001312     0.00\n",
      "NOTE:     22   400 0.000044           0.7393     0.8771  1.244e+04       1742          0   0.001312     0.00\n",
      "NOTE:     23   400 0.000044            0.971     0.8884  1.457e+04       1831          0   0.001312     0.00\n",
      "NOTE:     24   400 0.000044           0.9539     0.8336  1.325e+04       2644          0   0.001312     0.00\n",
      "NOTE:     25   400 0.000044           0.9274     0.8579  1.455e+04       2411          0   0.001312     0.00\n",
      "NOTE:     26   400 0.000044           0.9651     0.8541  1.535e+04       2623          0   0.001312     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  160       44E-6           0.899     0.8629  3.825e+05  6.079e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000044           0.7743     0.8807  1.246e+04       1687          0   0.001312     0.00\n",
      "NOTE:      1   400 0.000044            1.063     0.8427  1.495e+04       2790          0   0.001312     0.00\n",
      "NOTE:      2   400 0.000044           0.9408     0.8324   1.43e+04       2879          0   0.001312     0.00\n",
      "NOTE:      3   400 0.000044           0.6909     0.8317  1.058e+04       2140          0   0.001312     0.00\n",
      "NOTE:      4   400 0.000044            1.023     0.8398  1.579e+04       3013          0   0.001312     0.00\n",
      "NOTE:      5   400 0.000044           0.7989     0.8693  1.335e+04       2007          0   0.001312     0.00\n",
      "NOTE:      6   400 0.000044           0.8393     0.8409  1.242e+04       2350          0   0.001312     0.00\n",
      "NOTE:      7   400 0.000044           0.7698     0.8996  1.248e+04       1393          0   0.001313     0.00\n",
      "NOTE:      8   400 0.000044           0.8696     0.8737  1.414e+04       2044          0   0.001313     0.00\n",
      "NOTE:      9   400 0.000044           0.9154      0.889  1.437e+04       1794          0   0.001313     0.00\n",
      "NOTE:     10   400 0.000044           0.6704     0.8727  1.073e+04       1566          0   0.001313     0.00\n",
      "NOTE:     11   400 0.000044           0.7713     0.8278   1.14e+04       2372          0   0.001313     0.00\n",
      "NOTE:     12   400 0.000044           0.8274     0.8772  1.338e+04       1873          0   0.001313     0.00\n",
      "NOTE:     13   400 0.000044           0.8878     0.8453  1.395e+04       2553          0   0.001313     0.00\n",
      "NOTE:     14   400 0.000044           0.9849     0.8461  1.511e+04       2748          0   0.001313     0.00\n",
      "NOTE:     15   400 0.000044           0.7938     0.8028   1.17e+04       2873          0   0.001313     0.00\n",
      "NOTE:     16   400 0.000044             0.76     0.9207  1.427e+04       1229          0   0.001313     0.00\n",
      "NOTE:     17   400 0.000044           0.7943     0.8465  1.203e+04       2182          0   0.001313     0.00\n",
      "NOTE:     18   400 0.000044           0.8087     0.8792  1.327e+04       1823          0   0.001313     0.00\n",
      "NOTE:     19   400 0.000044           0.9446     0.8436  1.537e+04       2849          0   0.001313     0.00\n",
      "NOTE:     20   400 0.000044           0.8891     0.9086  1.526e+04       1534          0   0.001313     0.00\n",
      "NOTE:     21   400 0.000044            1.045     0.8561  1.498e+04       2519          0   0.001313     0.00\n",
      "NOTE:     22   400 0.000044           0.8439      0.878  1.377e+04       1913          0   0.001313     0.00\n",
      "NOTE:     23   400 0.000044            0.795     0.8314  1.212e+04       2457          0   0.001313     0.00\n",
      "NOTE:     24   400 0.000044           0.9319     0.8397  1.404e+04       2680          0   0.001313     0.00\n",
      "NOTE:     25   400 0.000044            0.951     0.8564  1.574e+04       2638          0   0.001313     0.00\n",
      "NOTE:     26   400 0.000044           0.8152     0.8549  1.309e+04       2221          0   0.001313     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  161       44E-6          0.8592     0.8586   3.65e+05  6.013e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000044           0.8451     0.8568  1.292e+04       2159          0   0.001313     0.00\n",
      "NOTE:      1   400 0.000044             0.89     0.8824  1.502e+04       2003          0   0.001313     0.00\n",
      "NOTE:      2   400 0.000044           0.8399     0.8097  1.114e+04       2619          0   0.001313     0.00\n",
      "NOTE:      3   400 0.000044           0.8488     0.8643  1.266e+04       1988          0   0.001313     0.00\n",
      "NOTE:      4   400 0.000044           0.9782     0.8772  1.629e+04       2280          0   0.001313     0.00\n",
      "NOTE:      5   400 0.000044           0.9424     0.8385  1.517e+04       2922          0   0.001313     0.00\n",
      "NOTE:      6   400 0.000044            0.952      0.854  1.463e+04       2501          0   0.001313     0.00\n",
      "NOTE:      7   400 0.000044             1.04     0.8699  1.608e+04       2404          0   0.001313     0.00\n",
      "NOTE:      8   400 0.000044           0.8666     0.8973  1.457e+04       1667          0   0.001313     0.00\n",
      "NOTE:      9   400 0.000044           0.9236      0.876  1.538e+04       2177          0   0.001313     0.00\n",
      "NOTE:     10   400 0.000044            1.064     0.8215  1.554e+04       3376          0   0.001313     0.00\n",
      "NOTE:     11   400 0.000044           0.8534     0.8657   1.34e+04       2080          0   0.001313     0.00\n",
      "NOTE:     12   400 0.000044            1.006     0.8246  1.449e+04       3082          0   0.001313     0.00\n",
      "NOTE:     13   400 0.000044           0.8453      0.832  1.315e+04       2655          0   0.001313     0.00\n",
      "NOTE:     14   400 0.000044           0.9929     0.8299  1.369e+04       2806          0   0.001313     0.00\n",
      "NOTE:     15   400 0.000044           0.8235     0.8435  1.343e+04       2491          0   0.001313     0.00\n",
      "NOTE:     16   400 0.000044           0.7809     0.8967  1.269e+04       1461          0   0.001313     0.00\n",
      "NOTE:     17   400 0.000044           0.8228     0.8591  1.209e+04       1982          0   0.001313     0.00\n",
      "NOTE:     18   400 0.000044            1.034     0.8594  1.591e+04       2603          0   0.001313     0.00\n",
      "NOTE:     19   400 0.000044           0.8881     0.8742  1.383e+04       1990          0   0.001313     0.00\n",
      "NOTE:     20   400 0.000044           0.9755     0.8615  1.543e+04       2479          0   0.001313     0.00\n",
      "NOTE:     21   400 0.000044           0.7998       0.85  1.354e+04       2390          0   0.001313     0.00\n",
      "NOTE:     22   400 0.000044           0.8175     0.8921  1.288e+04       1559          0   0.001313     0.00\n",
      "NOTE:     23   400 0.000044           0.7747     0.8739  1.196e+04       1725          0   0.001313     0.00\n",
      "NOTE:     24   400 0.000044           0.9038     0.8925  1.479e+04       1782          0   0.001313     0.00\n",
      "NOTE:     25   400 0.000044           0.8664     0.8978  1.495e+04       1702          0   0.001313     0.00\n",
      "NOTE:     26   400 0.000044           0.9838     0.8555  1.448e+04       2446          0   0.001313     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  162       44E-6          0.9022     0.8611  3.801e+05  6.133e+04          0     0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000044           0.9958     0.8563  1.484e+04       2490          0   0.001313     0.00\n",
      "NOTE:      1   400 0.000044           0.9374     0.8763  1.533e+04       2164          0   0.001313     0.00\n",
      "NOTE:      2   400 0.000044           0.8941      0.877  1.473e+04       2065          0   0.001313     0.00\n",
      "NOTE:      3   400 0.000044           0.9028     0.8498   1.53e+04       2704          0   0.001313     0.00\n",
      "NOTE:      4   400 0.000044            0.814      0.855  1.346e+04       2283          0   0.001313     0.00\n",
      "NOTE:      5   400 0.000044           0.9548     0.8794  1.444e+04       1980          0   0.001313     0.00\n",
      "NOTE:      6   400 0.000044            0.832     0.8086  1.263e+04       2988          0   0.001313     0.00\n",
      "NOTE:      7   400 0.000044            1.071     0.8421  1.687e+04       3162          0   0.001313     0.00\n",
      "NOTE:      8   400 0.000044           0.9206      0.842   1.44e+04       2701          0   0.001313     0.00\n",
      "NOTE:      9   400 0.000044           0.6836      0.857       9868       1647          0   0.001313     0.00\n",
      "NOTE:     10   400 0.000044           0.8114     0.8867  1.332e+04       1701          0   0.001313     0.00\n",
      "NOTE:     11   400 0.000044           0.7332     0.8664   1.18e+04       1820          0   0.001313     0.00\n",
      "NOTE:     12   400 0.000044           0.8533     0.8381  1.261e+04       2436          0   0.001313     0.00\n",
      "NOTE:     13   400 0.000044           0.8755     0.8148  1.186e+04       2696          0   0.001313     0.00\n",
      "NOTE:     14   400 0.000044             1.05     0.8552  1.506e+04       2550          0   0.001313     0.00\n",
      "NOTE:     15   400 0.000044           0.9175     0.8247  1.138e+04       2419          0   0.001313     0.00\n",
      "NOTE:     16   400 0.000044           0.8417     0.8879   1.48e+04       1868          0   0.001313     0.00\n",
      "NOTE:     17   400 0.000044           0.9336      0.896   1.54e+04       1786          0   0.001313     0.00\n",
      "NOTE:     18   400 0.000044           0.9031     0.8325    1.3e+04       2616          0   0.001313     0.00\n",
      "NOTE:     19   400 0.000044           0.7595     0.8513  1.215e+04       2121          0   0.001313     0.00\n",
      "NOTE:     20   400 0.000044           0.7817     0.8601  1.225e+04       1993          0   0.001313     0.00\n",
      "NOTE:     21   400 0.000044           0.9706     0.8569  1.376e+04       2297          0   0.001313     0.00\n",
      "NOTE:     22   400 0.000044           0.8048     0.8779  1.408e+04       1959          0   0.001313     0.00\n",
      "NOTE:     23   400 0.000044           0.8175     0.9067  1.349e+04       1389          0   0.001313     0.00\n",
      "NOTE:     24   400 0.000044           0.9002     0.8385   1.38e+04       2657          0   0.001313     0.00\n",
      "NOTE:     25   400 0.000044           0.8534     0.8638  1.393e+04       2196          0   0.001313     0.00\n",
      "NOTE:     26   400 0.000044           0.9445       0.83  1.436e+04       2942          0   0.001313     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  163       44E-6          0.8799     0.8569  3.689e+05  6.163e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000035           0.7386     0.9019  1.281e+04       1394          0   0.001313     0.00\n",
      "NOTE:      1   400 0.000035           0.6981     0.8522  1.129e+04       1958          0   0.001313     0.00\n",
      "NOTE:      2   400 0.000035           0.9517     0.8817  1.436e+04       1927          0   0.001313     0.00\n",
      "NOTE:      3   400 0.000035           0.8869     0.8474  1.396e+04       2513          0   0.001313     0.00\n",
      "NOTE:      4   400 0.000035           0.7514     0.8816  1.238e+04       1662          0   0.001313     0.00\n",
      "NOTE:      5   400 0.000035           0.8678     0.8679  1.475e+04       2244          0   0.001313     0.00\n",
      "NOTE:      6   400 0.000035           0.8547     0.8521  1.275e+04       2212          0   0.001313     0.00\n",
      "NOTE:      7   400 0.000035           0.8764     0.8735  1.363e+04       1975          0   0.001313     0.00\n",
      "NOTE:      8   400 0.000035           0.9147     0.8295  1.346e+04       2766          0   0.001313     0.00\n",
      "NOTE:      9   400 0.000035           0.9647     0.8502  1.538e+04       2709          0   0.001313     0.00\n",
      "NOTE:     10   400 0.000035            0.923     0.8312  1.432e+04       2909          0   0.001313     0.00\n",
      "NOTE:     11   400 0.000035           0.7746     0.8321  1.187e+04       2395          0   0.001313     0.00\n",
      "NOTE:     12   400 0.000035           0.9116     0.8552  1.314e+04       2224          0   0.001313     0.00\n",
      "NOTE:     13   400 0.000035           0.8516     0.8607  1.283e+04       2076          0   0.001313     0.00\n",
      "NOTE:     14   400 0.000035           0.7759     0.8283  1.119e+04       2320          0   0.001313     0.00\n",
      "NOTE:     15   400 0.000035           0.8486     0.8813  1.375e+04       1852          0   0.001313     0.00\n",
      "NOTE:     16   400 0.000035            0.871     0.8369  1.247e+04       2430          0   0.001313     0.00\n",
      "NOTE:     17   400 0.000035           0.9793     0.8519  1.478e+04       2569          0   0.001313     0.00\n",
      "NOTE:     18   400 0.000035           0.7818     0.8587  1.251e+04       2059          0   0.001313     0.00\n",
      "NOTE:     19   400 0.000035           0.8886      0.846  1.383e+04       2517          0   0.001313     0.00\n",
      "NOTE:     20   400 0.000035            0.983     0.8361  1.507e+04       2955          0   0.001313     0.00\n",
      "NOTE:     21   400 0.000035           0.8979     0.8552  1.507e+04       2551          0   0.001313     0.00\n",
      "NOTE:     22   400 0.000035           0.8464     0.8437  1.316e+04       2437          0   0.001313     0.00\n",
      "NOTE:     23   400 0.000035           0.8784     0.8035  1.255e+04       3068          0   0.001313     0.00\n",
      "NOTE:     24   400 0.000035            1.124     0.8607  1.621e+04       2622          0   0.001313     0.00\n",
      "NOTE:     25   400 0.000035           0.9858     0.7854  1.284e+04       3508          0   0.001313     0.00\n",
      "NOTE:     26   400 0.000035           0.9462     0.8674  1.556e+04       2379          0   0.001313     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  164      352E-7          0.8805     0.8507  3.659e+05  6.423e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000035           0.9656     0.8621  1.533e+04       2452          0   0.001313     0.00\n",
      "NOTE:      1   400 0.000035           0.8391     0.8532  1.372e+04       2362          0   0.001313     0.00\n",
      "NOTE:      2   400 0.000035           0.7033     0.8408  1.143e+04       2165          0   0.001313     0.00\n",
      "NOTE:      3   400 0.000035           0.8469     0.8694  1.264e+04       1899          0   0.001313     0.00\n",
      "NOTE:      4   400 0.000035           0.8555     0.8442  1.326e+04       2446          0   0.001313     0.00\n",
      "NOTE:      5   400 0.000035            0.894     0.8658  1.298e+04       2012          0   0.001313     0.00\n",
      "NOTE:      6   400 0.000035           0.8787     0.8852  1.412e+04       1831          0   0.001313     0.00\n",
      "NOTE:      7   400 0.000035           0.7919     0.8387  1.117e+04       2148          0   0.001313     0.00\n",
      "NOTE:      8   400 0.000035           0.8724     0.8496  1.363e+04       2412          0   0.001313     0.00\n",
      "NOTE:      9   400 0.000035           0.8873     0.8684  1.397e+04       2118          0   0.001313     0.00\n",
      "NOTE:     10   400 0.000035           0.7668     0.8827  1.282e+04       1703          0   0.001313     0.00\n",
      "NOTE:     11   400 0.000035           0.8426     0.8802  1.387e+04       1888          0   0.001313     0.00\n",
      "NOTE:     12   400 0.000035           0.9169     0.8369  1.295e+04       2523          0   0.001313     0.00\n",
      "NOTE:     13   400 0.000035           0.9498     0.8284  1.422e+04       2946          0   0.001313     0.00\n",
      "NOTE:     14   400 0.000035             1.01     0.8975  1.578e+04       1802          0   0.001313     0.00\n",
      "NOTE:     15   400 0.000035           0.8182     0.8242  1.147e+04       2446          0   0.001313     0.00\n",
      "NOTE:     16   400 0.000035           0.8638     0.8853  1.395e+04       1807          0   0.001313     0.00\n",
      "NOTE:     17   400 0.000035           0.8186     0.8757  1.375e+04       1952          0   0.001313     0.00\n",
      "NOTE:     18   400 0.000035           0.9757     0.8655  1.502e+04       2333          0   0.001313     0.00\n",
      "NOTE:     19   400 0.000035           0.8388      0.839  1.211e+04       2324          0   0.001313     0.00\n",
      "NOTE:     20   400 0.000035            0.813     0.8392   1.23e+04       2357          0   0.001313     0.00\n",
      "NOTE:     21   400 0.000035           0.8924     0.8408  1.388e+04       2628          0   0.001313     0.00\n",
      "NOTE:     22   400 0.000035           0.8734     0.8245  1.275e+04       2713          0   0.001313     0.00\n",
      "NOTE:     23   400 0.000035           0.7675     0.8935  1.313e+04       1565          0   0.001313     0.00\n",
      "NOTE:     24   400 0.000035            0.978     0.9026  1.652e+04       1783          0   0.001313     0.00\n",
      "NOTE:     25   400 0.000035           0.9862     0.8118  1.405e+04       3258          0   0.001313     0.00\n",
      "NOTE:     26   400 0.000035           0.7223     0.8427   1.09e+04       2035          0   0.001313     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  165      352E-7          0.8655     0.8579  3.617e+05  5.991e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000035           0.8071     0.8583  1.332e+04       2199          0   0.001313     0.00\n",
      "NOTE:      1   400 0.000035           0.9491     0.8815   1.52e+04       2044          0   0.001313     0.00\n",
      "NOTE:      2   400 0.000035           0.9355     0.9026  1.536e+04       1658          0   0.001313     0.00\n",
      "NOTE:      3   400 0.000035           0.8308     0.8678   1.43e+04       2178          0   0.001313     0.00\n",
      "NOTE:      4   400 0.000035           0.7916     0.8579  1.224e+04       2027          0   0.001313     0.00\n",
      "NOTE:      5   400 0.000035           0.9244     0.8348  1.322e+04       2617          0   0.001313     0.00\n",
      "NOTE:      6   400 0.000035            1.069      0.856  1.624e+04       2733          0   0.001313     0.00\n",
      "NOTE:      7   400 0.000035           0.6666     0.8784       9597       1329          0   0.001313     0.00\n",
      "NOTE:      8   400 0.000035           0.9114      0.828  1.379e+04       2866          0   0.001313     0.00\n",
      "NOTE:      9   400 0.000035           0.9269     0.8215  1.285e+04       2792          0   0.001313     0.00\n",
      "NOTE:     10   400 0.000035           0.9372     0.8139  1.386e+04       3171          0   0.001313     0.00\n",
      "NOTE:     11   400 0.000035           0.8573     0.8472  1.393e+04       2513          0   0.001313     0.00\n",
      "NOTE:     12   400 0.000035           0.8949     0.8451  1.294e+04       2372          0   0.001313     0.00\n",
      "NOTE:     13   400 0.000035           0.8552     0.8799  1.335e+04       1821          0   0.001313     0.00\n",
      "NOTE:     14   400 0.000035            0.835      0.841  1.322e+04       2498          0   0.001313     0.00\n",
      "NOTE:     15   400 0.000035           0.9486     0.8652  1.399e+04       2180          0   0.001313     0.00\n",
      "NOTE:     16   400 0.000035           0.9095     0.8883  1.582e+04       1988          0   0.001313     0.00\n",
      "NOTE:     17   400 0.000035           0.8264     0.8104  1.082e+04       2530          0   0.001313     0.00\n",
      "NOTE:     18   400 0.000035           0.7882     0.8665  1.296e+04       1997          0   0.001313     0.00\n",
      "NOTE:     19   400 0.000035           0.9603     0.8573  1.566e+04       2607          0   0.001313     0.00\n",
      "NOTE:     20   400 0.000035            1.044     0.9012  1.805e+04       1978          0   0.001313     0.00\n",
      "NOTE:     21   400 0.000035           0.9372     0.8162  1.419e+04       3195          0   0.001313     0.00\n",
      "NOTE:     22   400 0.000035            0.989     0.8765  1.564e+04       2203          0   0.001313     0.00\n",
      "NOTE:     23   400 0.000035           0.9869     0.8443  1.567e+04       2891          0   0.001313     0.00\n",
      "NOTE:     24   400 0.000035           0.8247     0.8376  1.247e+04       2418          0   0.001313     0.00\n",
      "NOTE:     25   400 0.000035           0.8079     0.8463  1.168e+04       2120          0   0.001313     0.00\n",
      "NOTE:     26   400 0.000035           0.7899      0.834  1.193e+04       2374          0   0.001313     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  166      352E-7          0.8891     0.8547  3.723e+05   6.33e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000035           0.8639     0.8691  1.235e+04       1860          0   0.001313     0.00\n",
      "NOTE:      1   400 0.000035           0.9395     0.8487  1.439e+04       2565          0   0.001313     0.00\n",
      "NOTE:      2   400 0.000035            0.934     0.8327  1.386e+04       2786          0   0.001313     0.00\n",
      "NOTE:      3   400 0.000035           0.9517     0.8684  1.516e+04       2297          0   0.001313     0.00\n",
      "NOTE:      4   400 0.000035           0.9075     0.8555   1.39e+04       2347          0   0.001313     0.00\n",
      "NOTE:      5   400 0.000035           0.9214     0.8628  1.451e+04       2307          0   0.001313     0.00\n",
      "NOTE:      6   400 0.000035           0.9967     0.8497  1.462e+04       2586          0   0.001313     0.00\n",
      "NOTE:      7   400 0.000035           0.8828     0.8964  1.485e+04       1717          0   0.001313     0.00\n",
      "NOTE:      8   400 0.000035             0.88     0.8677  1.467e+04       2237          0   0.001313     0.00\n",
      "NOTE:      9   400 0.000035           0.8032     0.8762  1.309e+04       1850          0   0.001313     0.00\n",
      "NOTE:     10   400 0.000035           0.8658     0.8977  1.537e+04       1752          0   0.001313     0.00\n",
      "NOTE:     11   400 0.000035           0.8527     0.8673  1.213e+04       1856          0   0.001313     0.00\n",
      "NOTE:     12   400 0.000035            1.063     0.8555  1.665e+04       2811          0   0.001313     0.00\n",
      "NOTE:     13   400 0.000035           0.8145     0.8799  1.371e+04       1871          0   0.001313     0.00\n",
      "NOTE:     14   400 0.000035           0.8911     0.8826   1.49e+04       1982          0   0.001313     0.00\n",
      "NOTE:     15   400 0.000035           0.8437     0.8344   1.24e+04       2460          0   0.001313     0.00\n",
      "NOTE:     16   400 0.000035            1.067     0.8895   1.73e+04       2149          0   0.001313     0.00\n",
      "NOTE:     17   400 0.000035           0.8666     0.8704  1.407e+04       2095          0   0.001313     0.00\n",
      "NOTE:     18   400 0.000035           0.9277     0.7997  1.267e+04       3174          0   0.001313     0.00\n",
      "NOTE:     19   400 0.000035           0.9493     0.8566  1.357e+04       2271          0   0.001313     0.00\n",
      "NOTE:     20   400 0.000035           0.8389     0.8664  1.339e+04       2065          0   0.001313     0.00\n",
      "NOTE:     21   400 0.000035           0.9113      0.845  1.311e+04       2405          0   0.001313     0.00\n",
      "NOTE:     22   400 0.000035            0.722     0.9023  1.294e+04       1401          0   0.001313     0.00\n",
      "NOTE:     23   400 0.000035           0.8258     0.8799  1.402e+04       1914          0   0.001313     0.00\n",
      "NOTE:     24   400 0.000035            1.104     0.8142  1.494e+04       3408          0   0.001313     0.00\n",
      "NOTE:     25   400 0.000035           0.6659     0.8437       9882       1831          0   0.001313     0.00\n",
      "NOTE:     26   400 0.000035           0.8623     0.8773  1.431e+04       2000          0   0.001313     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  167      352E-7          0.8946     0.8626  3.768e+05      6e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000035           0.8202     0.8843   1.38e+04       1805          0   0.001313     0.00\n",
      "NOTE:      1   400 0.000035           0.9773     0.8593  1.598e+04       2617          0   0.001313     0.00\n",
      "NOTE:      2   400 0.000035           0.8367     0.8624  1.274e+04       2034          0   0.001313     0.00\n",
      "NOTE:      3   400 0.000035            1.052     0.8478  1.568e+04       2815          0   0.001313     0.00\n",
      "NOTE:      4   400 0.000035           0.8556     0.9029  1.422e+04       1530          0   0.001313     0.00\n",
      "NOTE:      5   400 0.000035           0.7933     0.8844  1.404e+04       1836          0   0.001313     0.00\n",
      "NOTE:      6   400 0.000035           0.8619     0.8501  1.285e+04       2267          0   0.001313     0.00\n",
      "NOTE:      7   400 0.000035           0.8791     0.9055   1.44e+04       1504          0   0.001313     0.00\n",
      "NOTE:      8   400 0.000035           0.8916     0.8923  1.423e+04       1717          0   0.001313     0.00\n",
      "NOTE:      9   400 0.000035           0.9712     0.8955  1.591e+04       1856          0   0.001313     0.00\n",
      "NOTE:     10   400 0.000035           0.8883      0.858  1.319e+04       2183          0   0.001313     0.00\n",
      "NOTE:     11   400 0.000035            0.872     0.8464  1.392e+04       2527          0   0.001314     0.00\n",
      "NOTE:     12   400 0.000035            0.828     0.8767  1.391e+04       1956          0   0.001314     0.00\n",
      "NOTE:     13   400 0.000035           0.9309     0.8408  1.497e+04       2835          0   0.001314     0.00\n",
      "NOTE:     14   400 0.000035           0.8857     0.8606  1.464e+04       2372          0   0.001314     0.00\n",
      "NOTE:     15   400 0.000035           0.9725     0.8503  1.365e+04       2404          0   0.001314     0.00\n",
      "NOTE:     16   400 0.000035           0.8255      0.841  1.431e+04       2705          0   0.001314     0.00\n",
      "NOTE:     17   400 0.000035           0.9779     0.8551  1.559e+04       2643          0   0.001314     0.00\n",
      "NOTE:     18   400 0.000035           0.8759     0.8582  1.301e+04       2150          0   0.001314     0.00\n",
      "NOTE:     19   400 0.000035           0.8394     0.8678  1.291e+04       1968          0   0.001314     0.00\n",
      "NOTE:     20   400 0.000035             0.84     0.8584  1.306e+04       2153          0   0.001314     0.00\n",
      "NOTE:     21   400 0.000035           0.9776     0.8784  1.578e+04       2184          0   0.001314     0.00\n",
      "NOTE:     22   400 0.000035           0.9526     0.8955  1.635e+04       1909          0   0.001314     0.00\n",
      "NOTE:     23   400 0.000035           0.9318     0.8429  1.407e+04       2621          0   0.001314     0.00\n",
      "NOTE:     24   400 0.000035           0.7791     0.8178   1.18e+04       2629          0   0.001314     0.00\n",
      "NOTE:     25   400 0.000035           0.9895     0.8466  1.407e+04       2549          0   0.001314     0.00\n",
      "NOTE:     26   400 0.000035           0.9432     0.8678    1.4e+04       2133          0   0.001314     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  168      352E-7          0.8981     0.8648  3.831e+05   5.99e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000035           0.9217      0.885  1.452e+04       1887          0   0.001314     0.00\n",
      "NOTE:      1   400 0.000035           0.8953     0.8401  1.311e+04       2495          0   0.001314     0.00\n",
      "NOTE:      2   400 0.000035           0.8304     0.8494  1.299e+04       2304          0   0.001314     0.00\n",
      "NOTE:      3   400 0.000035           0.9834     0.8566   1.49e+04       2495          0   0.001314     0.00\n",
      "NOTE:      4   400 0.000035           0.8334     0.8818  1.432e+04       1920          0   0.001314     0.00\n",
      "NOTE:      5   400 0.000035           0.9528     0.8652  1.594e+04       2484          0   0.001314     0.00\n",
      "NOTE:      6   400 0.000035           0.9294     0.8418  1.447e+04       2718          0   0.001314     0.00\n",
      "NOTE:      7   400 0.000035             0.91     0.8713  1.405e+04       2074          0   0.001314     0.00\n",
      "NOTE:      8   400 0.000035           0.7473     0.8261       9746       2052          0   0.001314     0.00\n",
      "NOTE:      9   400 0.000035           0.9456     0.8894  1.592e+04       1980          0   0.001314     0.00\n",
      "NOTE:     10   400 0.000035           0.7824     0.8978    1.4e+04       1593          0   0.001314     0.00\n",
      "NOTE:     11   400 0.000035            0.844      0.884  1.254e+04       1646          0   0.001314     0.00\n",
      "NOTE:     12   400 0.000035           0.8536     0.8785  1.418e+04       1961          0   0.001314     0.00\n",
      "NOTE:     13   400 0.000035            1.013     0.8649  1.537e+04       2401          0   0.001314     0.00\n",
      "NOTE:     14   400 0.000035           0.9842     0.8773  1.558e+04       2180          0   0.001314     0.00\n",
      "NOTE:     15   400 0.000035           0.9912     0.8562  1.465e+04       2461          0   0.001314     0.00\n",
      "NOTE:     16   400 0.000035           0.9586     0.8837  1.441e+04       1896          0   0.001314     0.00\n",
      "NOTE:     17   400 0.000035           0.7779     0.8976  1.299e+04       1482          0   0.001314     0.00\n",
      "NOTE:     18   400 0.000035           0.8029     0.8963  1.436e+04       1661          0   0.001314     0.00\n",
      "NOTE:     19   400 0.000035           0.8787     0.9055  1.509e+04       1575          0   0.001314     0.00\n",
      "NOTE:     20   400 0.000035           0.8793     0.8984  1.469e+04       1661          0   0.001314     0.00\n",
      "NOTE:     21   400 0.000035           0.8515     0.8975  1.461e+04       1668          0   0.001314     0.00\n",
      "NOTE:     22   400 0.000035           0.8484     0.8367  1.292e+04       2522          0   0.001314     0.00\n",
      "NOTE:     23   400 0.000035           0.7901     0.8888  1.345e+04       1683          0   0.001314     0.00\n",
      "NOTE:     24   400 0.000035           0.8443     0.8826   1.43e+04       1902          0   0.001314     0.00\n",
      "NOTE:     25   400 0.000035           0.9801     0.8809  1.521e+04       2056          0   0.001314     0.00\n",
      "NOTE:     26   400 0.000035           0.8551      0.817  1.213e+04       2718          0   0.001314     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  169      352E-7          0.8846     0.8727  3.804e+05  5.548e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000035           0.9765     0.8286  1.441e+04       2979          0   0.001314     0.00\n",
      "NOTE:      1   400 0.000035            0.761     0.8552  1.159e+04       1962          0   0.001314     0.00\n",
      "NOTE:      2   400 0.000035           0.7753       0.86  1.199e+04       1952          0   0.001314     0.00\n",
      "NOTE:      3   400 0.000035            1.034     0.8515  1.616e+04       2817          0   0.001314     0.00\n",
      "NOTE:      4   400 0.000035            0.937     0.8151  1.355e+04       3073          0   0.001314     0.00\n",
      "NOTE:      5   400 0.000035           0.7612     0.8963  1.391e+04       1610          0   0.001314     0.00\n",
      "NOTE:      6   400 0.000035           0.7126     0.9044  1.141e+04       1206          0   0.001314     0.00\n",
      "NOTE:      7   400 0.000035           0.7885     0.8607  1.207e+04       1953          0   0.001314     0.00\n",
      "NOTE:      8   400 0.000035           0.8152     0.8234  1.392e+04       2985          0   0.001314     0.00\n",
      "NOTE:      9   400 0.000035           0.8662     0.8839  1.421e+04       1867          0   0.001314     0.00\n",
      "NOTE:     10   400 0.000035            0.918     0.8408  1.364e+04       2582          0   0.001314     0.00\n",
      "NOTE:     11   400 0.000035            0.849     0.8714  1.341e+04       1979          0   0.001314     0.00\n",
      "NOTE:     12   400 0.000035           0.7494     0.8965  1.298e+04       1499          0   0.001314     0.00\n",
      "NOTE:     13   400 0.000035           0.8981     0.8864  1.524e+04       1953          0   0.001314     0.00\n",
      "NOTE:     14   400 0.000035            1.048     0.8472  1.551e+04       2796          0   0.001314     0.00\n",
      "NOTE:     15   400 0.000035           0.9404     0.8889  1.494e+04       1867          0   0.001314     0.00\n",
      "NOTE:     16   400 0.000035           0.7499     0.8151  1.143e+04       2594          0   0.001314     0.00\n",
      "NOTE:     17   400 0.000035           0.9713     0.8699  1.388e+04       2075          0   0.001314     0.00\n",
      "NOTE:     18   400 0.000035            0.878     0.8128   1.21e+04       2786          0   0.001314     0.00\n",
      "NOTE:     19   400 0.000035           0.8647     0.8766  1.379e+04       1941          0   0.001314     0.00\n",
      "NOTE:     20   400 0.000035           0.9228     0.8814  1.528e+04       2056          0   0.001314     0.00\n",
      "NOTE:     21   400 0.000035           0.8047     0.8845  1.319e+04       1722          0   0.001314     0.00\n",
      "NOTE:     22   400 0.000035           0.8977     0.8609  1.449e+04       2342          0   0.001314     0.00\n",
      "NOTE:     23   400 0.000035           0.8827      0.828  1.353e+04       2811          0   0.001314     0.00\n",
      "NOTE:     24   400 0.000035           0.8431     0.8511  1.322e+04       2312          0   0.001314     0.00\n",
      "NOTE:     25   400 0.000035           0.9517     0.8407  1.436e+04       2721          0   0.001314     0.00\n",
      "NOTE:     26   400 0.000035           0.9678     0.8866  1.561e+04       1997          0   0.001314     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  170      352E-7          0.8728     0.8595  3.698e+05  6.044e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000035           0.8154      0.878  1.325e+04       1841          0   0.001314     0.00\n",
      "NOTE:      1   400 0.000035             1.04     0.8548  1.722e+04       2924          0   0.001314     0.00\n",
      "NOTE:      2   400 0.000035            1.113     0.8509  1.708e+04       2993          0   0.001314     0.00\n",
      "NOTE:      3   400 0.000035           0.8126     0.8324  1.154e+04       2323          0   0.001314     0.00\n",
      "NOTE:      4   400 0.000035           0.9276     0.8152  1.429e+04       3240          0   0.001314     0.00\n",
      "NOTE:      5   400 0.000035           0.7889      0.825   1.04e+04       2206          0   0.001314     0.00\n",
      "NOTE:      6   400 0.000035           0.8975     0.8955  1.427e+04       1665          0   0.001314     0.00\n",
      "NOTE:      7   400 0.000035            1.015     0.8588   1.61e+04       2647          0   0.001314     0.00\n",
      "NOTE:      8   400 0.000035           0.9441     0.8587   1.45e+04       2387          0   0.001314     0.00\n",
      "NOTE:      9   400 0.000035            1.087     0.8499  1.752e+04       3094          0   0.001314     0.00\n",
      "NOTE:     10   400 0.000035           0.8439     0.8567  1.319e+04       2207          0   0.001314     0.00\n",
      "NOTE:     11   400 0.000035           0.8242     0.8106  1.189e+04       2778          0   0.001314     0.00\n",
      "NOTE:     12   400 0.000035            1.008     0.8585  1.545e+04       2546          0   0.001314     0.00\n",
      "NOTE:     13   400 0.000035           0.8324      0.859  1.159e+04       1903          0   0.001314     0.00\n",
      "NOTE:     14   400 0.000035           0.8741     0.8895  1.503e+04       1867          0   0.001314     0.00\n",
      "NOTE:     15   400 0.000035           0.9021     0.8593  1.318e+04       2157          0   0.001314     0.00\n",
      "NOTE:     16   400 0.000035           0.8825     0.7881    1.2e+04       3227          0   0.001314     0.00\n",
      "NOTE:     17   400 0.000035           0.7323     0.8601  1.188e+04       1933          0   0.001314     0.00\n",
      "NOTE:     18   400 0.000035           0.9678     0.8257  1.453e+04       3068          0   0.001314     0.00\n",
      "NOTE:     19   400 0.000035           0.9563     0.8445   1.43e+04       2633          0   0.001314     0.00\n",
      "NOTE:     20   400 0.000035           0.8232     0.8716  1.241e+04       1828          0   0.001314     0.00\n",
      "NOTE:     21   400 0.000035           0.8574     0.8189  1.304e+04       2883          0   0.001314     0.00\n",
      "NOTE:     22   400 0.000035           0.9601     0.8688  1.415e+04       2136          0   0.001314     0.00\n",
      "NOTE:     23   400 0.000035           0.8455     0.8947  1.499e+04       1764          0   0.001314     0.00\n",
      "NOTE:     24   400 0.000035           0.9965     0.8466  1.476e+04       2674          0   0.001314     0.00\n",
      "NOTE:     25   400 0.000035           0.8422     0.8352  1.362e+04       2688          0   0.001314     0.00\n",
      "NOTE:     26   400 0.000035            1.023     0.8544  1.475e+04       2513          0   0.001314     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  171      352E-7          0.9116     0.8507  3.769e+05  6.613e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000035           0.8291     0.8701  1.426e+04       2129          0   0.001314     0.00\n",
      "NOTE:      1   400 0.000035           0.8107     0.8868  1.427e+04       1821          0   0.001314     0.00\n",
      "NOTE:      2   400 0.000035           0.8433     0.8622  1.367e+04       2185          0   0.001314     0.00\n",
      "NOTE:      3   400 0.000035           0.7497     0.8501  1.238e+04       2183          0   0.001314     0.00\n",
      "NOTE:      4   400 0.000035           0.9393      0.877  1.543e+04       2163          0   0.001314     0.00\n",
      "NOTE:      5   400 0.000035            0.789     0.8856  1.231e+04       1591          0   0.001314     0.00\n",
      "NOTE:      6   400 0.000035           0.7418     0.8378  1.175e+04       2274          0   0.001314     0.00\n",
      "NOTE:      7   400 0.000035            0.961     0.8776  1.439e+04       2006          0   0.001314     0.00\n",
      "NOTE:      8   400 0.000035           0.9204     0.8538  1.509e+04       2583          0   0.001314     0.00\n",
      "NOTE:      9   400 0.000035           0.8748     0.8448   1.34e+04       2461          0   0.001314     0.00\n",
      "NOTE:     10   400 0.000035            1.087     0.8797  1.623e+04       2220          0   0.001314     0.00\n",
      "NOTE:     11   400 0.000035           0.8234     0.8853  1.535e+04       1989          0   0.001314     0.00\n",
      "NOTE:     12   400 0.000035           0.7936     0.8378  1.175e+04       2274          0   0.001314     0.00\n",
      "NOTE:     13   400 0.000035           0.9209     0.8573  1.322e+04       2199          0   0.001314     0.00\n",
      "NOTE:     14   400 0.000035           0.6727     0.8597  1.195e+04       1951          0   0.001314     0.00\n",
      "NOTE:     15   400 0.000035           0.7298     0.8807  1.169e+04       1584          0   0.001314     0.00\n",
      "NOTE:     16   400 0.000035           0.8944     0.8571  1.336e+04       2228          0   0.001314     0.00\n",
      "NOTE:     17   400 0.000035           0.8437     0.8241  1.229e+04       2623          0   0.001314     0.00\n",
      "NOTE:     18   400 0.000035            0.748     0.8955  1.189e+04       1387          0   0.001314     0.00\n",
      "NOTE:     19   400 0.000035            0.822      0.864  1.343e+04       2113          0   0.001314     0.00\n",
      "NOTE:     20   400 0.000035           0.8658      0.889  1.468e+04       1833          0   0.001314     0.00\n",
      "NOTE:     21   400 0.000035           0.8161     0.8674  1.313e+04       2006          0   0.001314     0.00\n",
      "NOTE:     22   400 0.000035            0.735     0.9227  1.343e+04       1125          0   0.001314     0.00\n",
      "NOTE:     23   400 0.000035           0.8652     0.8832  1.462e+04       1932          0   0.001314     0.00\n",
      "NOTE:     24   400 0.000035           0.8436     0.8661  1.277e+04       1973          0   0.001314     0.00\n",
      "NOTE:     25   400 0.000035           0.8618     0.8974  1.428e+04       1633          0   0.001314     0.00\n",
      "NOTE:     26   400 0.000035           0.9433      0.839  1.394e+04       2676          0   0.001314     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  172      352E-7          0.8417     0.8687  3.649e+05  5.514e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000028            0.875     0.8334  1.292e+04       2582          0   0.001314     0.00\n",
      "NOTE:      1   400 0.000028           0.9307     0.7869  1.282e+04       3472          0   0.001314     0.00\n",
      "NOTE:      2   400 0.000028           0.9686     0.8197  1.346e+04       2962          0   0.001314     0.00\n",
      "NOTE:      3   400 0.000028           0.9576     0.8785  1.543e+04       2134          0   0.001314     0.00\n",
      "NOTE:      4   400 0.000028           0.8437     0.8531  1.343e+04       2312          0   0.001314     0.00\n",
      "NOTE:      5   400 0.000028            1.056     0.8886  1.779e+04       2230          0   0.001314     0.00\n",
      "NOTE:      6   400 0.000028           0.9933     0.8375  1.464e+04       2841          0   0.001314     0.00\n",
      "NOTE:      7   400 0.000028           0.8465     0.8009  1.203e+04       2992          0   0.001314     0.00\n",
      "NOTE:      8   400 0.000028           0.9173     0.8586  1.494e+04       2461          0   0.001314     0.00\n",
      "NOTE:      9   400 0.000028           0.8139     0.8883  1.339e+04       1683          0   0.001314     0.00\n",
      "NOTE:     10   400 0.000028           0.8633     0.8763  1.341e+04       1894          0   0.001314     0.00\n",
      "NOTE:     11   400 0.000028           0.8208     0.8546   1.22e+04       2075          0   0.001314     0.00\n",
      "NOTE:     12   400 0.000028           0.7676     0.8943  1.277e+04       1508          0   0.001314     0.00\n",
      "NOTE:     13   400 0.000028           0.8561     0.8155  1.157e+04       2618          0   0.001314     0.00\n",
      "NOTE:     14   400 0.000028           0.7996     0.8771  1.345e+04       1884          0   0.001314     0.00\n",
      "NOTE:     15   400 0.000028           0.8646     0.8494  1.269e+04       2250          0   0.001314     0.00\n",
      "NOTE:     16   400 0.000028           0.8058     0.8986  1.374e+04       1550          0   0.001314     0.00\n",
      "NOTE:     17   400 0.000028            0.857     0.8497  1.207e+04       2135          0   0.001314     0.00\n",
      "NOTE:     18   400 0.000028           0.7877     0.8678  1.255e+04       1911          0   0.001314     0.00\n",
      "NOTE:     19   400 0.000028           0.7364     0.8661  1.211e+04       1872          0   0.001314     0.00\n",
      "NOTE:     20   400 0.000028           0.9325     0.8453  1.415e+04       2589          0   0.001314     0.00\n",
      "NOTE:     21   400 0.000028           0.7998     0.8831  1.347e+04       1782          0   0.001314     0.00\n",
      "NOTE:     22   400 0.000028           0.8144     0.8652  1.299e+04       2023          0   0.001314     0.00\n",
      "NOTE:     23   400 0.000028            0.834     0.8712  1.385e+04       2048          0   0.001314     0.00\n",
      "NOTE:     24   400 0.000028           0.7833       0.85  1.286e+04       2270          0   0.001314     0.00\n",
      "NOTE:     25   400 0.000028            0.894     0.8557  1.383e+04       2332          0   0.001314     0.00\n",
      "NOTE:     26   400 0.000028              0.8     0.8227  1.106e+04       2383          0   0.001314     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  173      281E-7            0.86     0.8554  3.596e+05  6.079e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000028           0.7557     0.8752  1.362e+04       1942          0   0.001314     0.00\n",
      "NOTE:      1   400 0.000028           0.8677     0.8606  1.288e+04       2086          0   0.001314     0.00\n",
      "NOTE:      2   400 0.000028           0.9087     0.8457  1.317e+04       2403          0   0.001314     0.00\n",
      "NOTE:      3   400 0.000028            0.792     0.8847  1.378e+04       1796          0   0.001314     0.00\n",
      "NOTE:      4   400 0.000028           0.7358      0.845  1.192e+04       2187          0   0.001314     0.00\n",
      "NOTE:      5   400 0.000028           0.9653     0.8391  1.448e+04       2775          0   0.001314     0.00\n",
      "NOTE:      6   400 0.000028            1.037     0.8656  1.523e+04       2365          0   0.001314     0.00\n",
      "NOTE:      7   400 0.000028           0.8613     0.8588  1.423e+04       2340          0   0.001314     0.00\n",
      "NOTE:      8   400 0.000028           0.9126     0.8751  1.392e+04       1987          0   0.001314     0.00\n",
      "NOTE:      9   400 0.000028           0.8247     0.8633  1.357e+04       2148          0   0.001314     0.00\n",
      "NOTE:     10   400 0.000028           0.9848      0.872  1.531e+04       2247          0   0.001314     0.00\n",
      "NOTE:     11   400 0.000028           0.9699     0.8327  1.467e+04       2949          0   0.001314     0.00\n",
      "NOTE:     12   400 0.000028           0.9241     0.8926  1.559e+04       1876          0   0.001314     0.00\n",
      "NOTE:     13   400 0.000028           0.8177     0.8701  1.193e+04       1781          0   0.001314     0.00\n",
      "NOTE:     14   400 0.000028            0.842     0.8989   1.51e+04       1697          0   0.001314     0.00\n",
      "NOTE:     15   400 0.000028           0.6964     0.8871  1.241e+04       1579          0   0.001314     0.00\n",
      "NOTE:     16   400 0.000028           0.9185     0.8796   1.59e+04       2177          0   0.001314     0.00\n",
      "NOTE:     17   400 0.000028           0.9676     0.8655  1.487e+04       2311          0   0.001314     0.00\n",
      "NOTE:     18   400 0.000028            1.059     0.8879  1.796e+04       2269          0   0.001314     0.00\n",
      "NOTE:     19   400 0.000028           0.8796     0.8683  1.396e+04       2117          0   0.001314     0.00\n",
      "NOTE:     20   400 0.000028           0.8362     0.8917  1.409e+04       1711          0   0.001314     0.00\n",
      "NOTE:     21   400 0.000028           0.8778     0.8727  1.337e+04       1950          0   0.001314     0.00\n",
      "NOTE:     22   400 0.000028           0.8334     0.9178  1.454e+04       1303          0   0.001314     0.00\n",
      "NOTE:     23   400 0.000028           0.9639     0.9212  1.597e+04       1366          0   0.001315     0.00\n",
      "NOTE:     24   400 0.000028           0.8201     0.8393   1.25e+04       2393          0   0.001315     0.00\n",
      "NOTE:     25   400 0.000028           0.7874     0.8746  1.273e+04       1825          0   0.001315     0.00\n",
      "NOTE:     26   400 0.000028           0.9818     0.8561  1.423e+04       2391          0   0.001315     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  174      281E-7          0.8822     0.8722  3.819e+05  5.597e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000028           0.7758     0.8544  1.194e+04       2036          0   0.001315     0.00\n",
      "NOTE:      1   400 0.000028            1.012     0.8674  1.552e+04       2373          0   0.001315     0.00\n",
      "NOTE:      2   400 0.000028           0.7831     0.8644  1.253e+04       1965          0   0.001315     0.00\n",
      "NOTE:      3   400 0.000028           0.9552     0.8596  1.424e+04       2326          0   0.001315     0.00\n",
      "NOTE:      4   400 0.000028           0.9409     0.8802  1.491e+04       2030          0   0.001315     0.00\n",
      "NOTE:      5   400 0.000028           0.8007     0.8396  1.186e+04       2265          0   0.001315     0.00\n",
      "NOTE:      6   400 0.000028           0.8163     0.8302   1.18e+04       2414          0   0.001315     0.00\n",
      "NOTE:      7   400 0.000028           0.8396     0.8831  1.364e+04       1805          0   0.001315     0.00\n",
      "NOTE:      8   400 0.000028           0.7678      0.839  1.058e+04       2030          0   0.001315     0.00\n",
      "NOTE:      9   400 0.000028            0.938     0.8624  1.426e+04       2275          0   0.001315     0.00\n",
      "NOTE:     10   400 0.000028            1.039     0.8368  1.535e+04       2994          0   0.001315     0.00\n",
      "NOTE:     11   400 0.000028           0.7526     0.8913  1.246e+04       1520          0   0.001315     0.00\n",
      "NOTE:     12   400 0.000028           0.8844     0.8954  1.407e+04       1644          0   0.001315     0.00\n",
      "NOTE:     13   400 0.000028           0.8146     0.8698  1.359e+04       2034          0   0.001315     0.00\n",
      "NOTE:     14   400 0.000028           0.9593     0.8928  1.562e+04       1876          0   0.001315     0.00\n",
      "NOTE:     15   400 0.000028            1.065      0.857  1.537e+04       2564          0   0.001315     0.00\n",
      "NOTE:     16   400 0.000028              0.9     0.8808  1.487e+04       2012          0   0.001315     0.00\n",
      "NOTE:     17   400 0.000028           0.9681     0.8469  1.447e+04       2617          0   0.001315     0.00\n",
      "NOTE:     18   400 0.000028            1.048     0.8195  1.515e+04       3336          0   0.001315     0.00\n",
      "NOTE:     19   400 0.000028           0.8917      0.837  1.272e+04       2477          0   0.001315     0.00\n",
      "NOTE:     20   400 0.000028            1.043     0.8292  1.512e+04       3114          0   0.001315     0.00\n",
      "NOTE:     21   400 0.000028           0.7897     0.8945  1.279e+04       1508          0   0.001315     0.00\n",
      "NOTE:     22   400 0.000028           0.8797     0.8785  1.499e+04       2073          0   0.001315     0.00\n",
      "NOTE:     23   400 0.000028           0.8637     0.8229  1.186e+04       2551          0   0.001315     0.00\n",
      "NOTE:     24   400 0.000028           0.8684     0.8353  1.238e+04       2440          0   0.001315     0.00\n",
      "NOTE:     25   400 0.000028           0.7981     0.8642  1.409e+04       2213          0   0.001315     0.00\n",
      "NOTE:     26   400 0.000028           0.7865     0.8703  1.285e+04       1915          0   0.001315     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  175      281E-7          0.8882     0.8593   3.69e+05  6.041e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000028           0.8385     0.8858  1.426e+04       1839          0   0.001315     0.00\n",
      "NOTE:      1   400 0.000028           0.9314     0.8768  1.539e+04       2163          0   0.001315     0.00\n",
      "NOTE:      2   400 0.000028           0.9058     0.8644  1.419e+04       2226          0   0.001315     0.00\n",
      "NOTE:      3   400 0.000028           0.8142     0.8766  1.354e+04       1906          0   0.001315     0.00\n",
      "NOTE:      4   400 0.000028             1.01     0.8661  1.466e+04       2267          0   0.001315     0.00\n",
      "NOTE:      5   400 0.000028             0.97     0.8665  1.511e+04       2327          0   0.001315     0.00\n",
      "NOTE:      6   400 0.000028            0.877     0.8869  1.543e+04       1968          0   0.001315     0.00\n",
      "NOTE:      7   400 0.000028           0.8781     0.8342   1.26e+04       2505          0   0.001315     0.00\n",
      "NOTE:      8   400 0.000028            0.812     0.8611  1.256e+04       2026          0   0.001315     0.00\n",
      "NOTE:      9   400 0.000028           0.8766     0.8587  1.365e+04       2246          0   0.001315     0.00\n",
      "NOTE:     10   400 0.000028           0.9378     0.8506  1.419e+04       2492          0   0.001315     0.00\n",
      "NOTE:     11   400 0.000028           0.8439     0.8619  1.401e+04       2244          0   0.001315     0.00\n",
      "NOTE:     12   400 0.000028           0.8303     0.8716  1.424e+04       2098          0   0.001315     0.00\n",
      "NOTE:     13   400 0.000028             0.78     0.9195  1.463e+04       1280          0   0.001315     0.00\n",
      "NOTE:     14   400 0.000028           0.8665     0.8785  1.386e+04       1917          0   0.001315     0.00\n",
      "NOTE:     15   400 0.000028           0.9216     0.8471  1.392e+04       2514          0   0.001315     0.00\n",
      "NOTE:     16   400 0.000028           0.8867     0.8491  1.435e+04       2549          0   0.001315     0.00\n",
      "NOTE:     17   400 0.000028           0.8326     0.9128  1.361e+04       1300          0   0.001315     0.00\n",
      "NOTE:     18   400 0.000028             0.84     0.8784   1.27e+04       1759          0   0.001315     0.00\n",
      "NOTE:     19   400 0.000028            1.004     0.8673  1.603e+04       2451          0   0.001315     0.00\n",
      "NOTE:     20   400 0.000028           0.7826     0.8685  1.315e+04       1991          0   0.001315     0.00\n",
      "NOTE:     21   400 0.000028           0.9049     0.8552  1.381e+04       2338          0   0.001315     0.00\n",
      "NOTE:     22   400 0.000028           0.7472     0.8882   1.22e+04       1536          0   0.001315     0.00\n",
      "NOTE:     23   400 0.000028           0.9491     0.8223  1.327e+04       2867          0   0.001315     0.00\n",
      "NOTE:     24   400 0.000028           0.8325     0.8055  1.174e+04       2834          0   0.001315     0.00\n",
      "NOTE:     25   400 0.000028           0.8266     0.8508  1.263e+04       2216          0   0.001315     0.00\n",
      "NOTE:     26   400 0.000028           0.8034     0.9017  1.401e+04       1527          0   0.001315     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  176      281E-7          0.8705     0.8669  3.737e+05  5.739e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000028           0.9253     0.8494  1.374e+04       2435          0   0.001315     0.00\n",
      "NOTE:      1   400 0.000028           0.9057     0.8498  1.528e+04       2700          0   0.001315     0.00\n",
      "NOTE:      2   400 0.000028           0.8208      0.856  1.364e+04       2294          0   0.001315     0.00\n",
      "NOTE:      3   400 0.000028           0.9025     0.8624  1.311e+04       2092          0   0.001315     0.00\n",
      "NOTE:      4   400 0.000028           0.9226     0.8702   1.36e+04       2027          0   0.001315     0.00\n",
      "NOTE:      5   400 0.000028             0.86     0.8859  1.375e+04       1771          0   0.001315     0.00\n",
      "NOTE:      6   400 0.000028           0.8787     0.8888  1.552e+04       1943          0   0.001315     0.00\n",
      "NOTE:      7   400 0.000028           0.8592     0.8708  1.307e+04       1939          0   0.001315     0.00\n",
      "NOTE:      8   400 0.000028           0.7616     0.8463  1.189e+04       2158          0   0.001315     0.00\n",
      "NOTE:      9   400 0.000028           0.7653     0.8481   1.18e+04       2113          0   0.001315     0.00\n",
      "NOTE:     10   400 0.000028           0.7818     0.8696  1.241e+04       1861          0   0.001315     0.00\n",
      "NOTE:     11   400 0.000028           0.9244     0.8155  1.302e+04       2945          0   0.001315     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     12   400 0.000028           0.9952     0.8507  1.468e+04       2577          0   0.001315     0.00\n",
      "NOTE:     13   400 0.000028           0.9119     0.8754  1.615e+04       2299          0   0.001315     0.00\n",
      "NOTE:     14   400 0.000028           0.8418     0.8356  1.327e+04       2612          0   0.001315     0.00\n",
      "NOTE:     15   400 0.000028           0.9571      0.867  1.433e+04       2197          0   0.001315     0.00\n",
      "NOTE:     16   400 0.000028           0.9229     0.8532  1.514e+04       2604          0   0.001315     0.00\n",
      "NOTE:     17   400 0.000028           0.8365     0.8874  1.491e+04       1892          0   0.001315     0.00\n",
      "NOTE:     18   400 0.000028           0.8174     0.8543  1.362e+04       2324          0   0.001315     0.00\n",
      "NOTE:     19   400 0.000028            1.044     0.8627  1.545e+04       2458          0   0.001315     0.00\n",
      "NOTE:     20   400 0.000028            0.761     0.8848  1.301e+04       1694          0   0.001315     0.00\n",
      "NOTE:     21   400 0.000028           0.8723     0.8931  1.452e+04       1738          0   0.001315     0.00\n",
      "NOTE:     22   400 0.000028           0.8675     0.8649  1.342e+04       2097          0   0.001315     0.00\n",
      "NOTE:     23   400 0.000028           0.8592     0.8822  1.493e+04       1994          0   0.001315     0.00\n",
      "NOTE:     24   400 0.000028            0.885     0.8492  1.368e+04       2428          0   0.001315     0.00\n",
      "NOTE:     25   400 0.000028           0.7841     0.8278   1.17e+04       2433          0   0.001315     0.00\n",
      "NOTE:     26   400 0.000028           0.9284     0.8768  1.614e+04       2268          0   0.001315     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  177      281E-7          0.8738     0.8625  3.758e+05  5.989e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000028           0.8757     0.8563  1.448e+04       2431          0   0.001315     0.00\n",
      "NOTE:      1   400 0.000028           0.7972     0.8408  1.226e+04       2320          0   0.001315     0.00\n",
      "NOTE:      2   400 0.000028           0.9687     0.8123  1.374e+04       3173          0   0.001315     0.00\n",
      "NOTE:      3   400 0.000028           0.7452     0.9172  1.318e+04       1189          0   0.001315     0.00\n",
      "NOTE:      4   400 0.000028           0.8198     0.8375  1.283e+04       2489          0   0.001315     0.00\n",
      "NOTE:      5   400 0.000028           0.7498     0.8767  1.226e+04       1725          0   0.001315     0.00\n",
      "NOTE:      6   400 0.000028           0.9408     0.8474  1.451e+04       2614          0   0.001315     0.00\n",
      "NOTE:      7   400 0.000028           0.9991     0.8361  1.437e+04       2817          0   0.001315     0.00\n",
      "NOTE:      8   400 0.000028           0.8478     0.8525  1.327e+04       2296          0   0.001315     0.00\n",
      "NOTE:      9   400 0.000028           0.7769     0.8713  1.276e+04       1885          0   0.001315     0.00\n",
      "NOTE:     10   400 0.000028           0.8348     0.8498  1.302e+04       2302          0   0.001315     0.00\n",
      "NOTE:     11   400 0.000028           0.9039     0.8882  1.445e+04       1819          0   0.001315     0.00\n",
      "NOTE:     12   400 0.000028           0.8367     0.8628   1.35e+04       2147          0   0.001315     0.00\n",
      "NOTE:     13   400 0.000028           0.8804      0.841  1.422e+04       2688          0   0.001315     0.00\n",
      "NOTE:     14   400 0.000028           0.9885     0.8507  1.406e+04       2466          0   0.001315     0.00\n",
      "NOTE:     15   400 0.000028           0.8778     0.8609  1.283e+04       2072          0   0.001315     0.00\n",
      "NOTE:     16   400 0.000028           0.8286     0.8775  1.379e+04       1924          0   0.001315     0.00\n",
      "NOTE:     17   400 0.000028            0.799     0.8568  1.265e+04       2114          0   0.001315     0.00\n",
      "NOTE:     18   400 0.000028            0.934     0.8182  1.337e+04       2971          0   0.001315     0.00\n",
      "NOTE:     19   400 0.000028           0.8564     0.8342   1.29e+04       2564          0   0.001315     0.00\n",
      "NOTE:     20   400 0.000028           0.8743     0.8374  1.326e+04       2575          0   0.001315     0.00\n",
      "NOTE:     21   400 0.000028           0.9018     0.8699  1.367e+04       2044          0   0.001315     0.00\n",
      "NOTE:     22   400 0.000028           0.8948     0.8517  1.329e+04       2314          0   0.001315     0.00\n",
      "NOTE:     23   400 0.000028           0.8277     0.8453  1.289e+04       2358          0   0.001315     0.00\n",
      "NOTE:     24   400 0.000028           0.9405     0.8591  1.414e+04       2320          0   0.001315     0.00\n",
      "NOTE:     25   400 0.000028            0.803     0.8963  1.372e+04       1588          0   0.001315     0.00\n",
      "NOTE:     26   400 0.000028            0.852     0.8225  1.194e+04       2577          0   0.001315     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  178      281E-7           0.865      0.854  3.614e+05  6.178e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000028           0.9137      0.897  1.546e+04       1776          0   0.001315     0.00\n",
      "NOTE:      1   400 0.000028            1.012     0.8963  1.684e+04       1949          0   0.001315     0.00\n",
      "NOTE:      2   400 0.000028             0.84     0.8613  1.277e+04       2057          0   0.001315     0.00\n",
      "NOTE:      3   400 0.000028            1.031      0.833  1.501e+04       3010          0   0.001315     0.00\n",
      "NOTE:      4   400 0.000028           0.8949     0.8783  1.491e+04       2067          0   0.001315     0.00\n",
      "NOTE:      5   400 0.000028           0.9125     0.8473  1.406e+04       2535          0   0.001315     0.00\n",
      "NOTE:      6   400 0.000028           0.7153     0.8565  1.065e+04       1784          0   0.001315     0.00\n",
      "NOTE:      7   400 0.000028           0.8026     0.8589  1.283e+04       2109          0   0.001315     0.00\n",
      "NOTE:      8   400 0.000028            0.911     0.8241  1.302e+04       2779          0   0.001315     0.00\n",
      "NOTE:      9   400 0.000028           0.8353     0.8853  1.383e+04       1792          0   0.001315     0.00\n",
      "NOTE:     10   400 0.000028           0.9648     0.8811   1.68e+04       2268          0   0.001315     0.00\n",
      "NOTE:     11   400 0.000028           0.8293     0.8467  1.319e+04       2387          0   0.001315     0.00\n",
      "NOTE:     12   400 0.000028            1.002     0.8581  1.469e+04       2429          0   0.001315     0.00\n",
      "NOTE:     13   400 0.000028           0.8709     0.8007  1.134e+04       2822          0   0.001315     0.00\n",
      "NOTE:     14   400 0.000028           0.8491     0.8858  1.363e+04       1758          0   0.001315     0.00\n",
      "NOTE:     15   400 0.000028           0.9174     0.8613  1.482e+04       2385          0   0.001315     0.00\n",
      "NOTE:     16   400 0.000028            1.015     0.8708  1.572e+04       2333          0   0.001315     0.00\n",
      "NOTE:     17   400 0.000028            0.716     0.8797  1.212e+04       1658          0   0.001315     0.00\n",
      "NOTE:     18   400 0.000028           0.8648     0.8338  1.292e+04       2574          0   0.001315     0.00\n",
      "NOTE:     19   400 0.000028             0.86     0.8796  1.441e+04       1973          0   0.001315     0.00\n",
      "NOTE:     20   400 0.000028           0.7666     0.8896  1.253e+04       1555          0   0.001315     0.00\n",
      "NOTE:     21   400 0.000028           0.7476     0.8637  1.138e+04       1796          0   0.001315     0.00\n",
      "NOTE:     22   400 0.000028           0.8296     0.8637  1.343e+04       2119          0   0.001315     0.00\n",
      "NOTE:     23   400 0.000028           0.9897     0.8556  1.507e+04       2543          0   0.001315     0.00\n",
      "NOTE:     24   400 0.000028           0.8925      0.844   1.34e+04       2478          0   0.001315     0.00\n",
      "NOTE:     25   400 0.000028            1.054     0.8001  1.492e+04       3728          0   0.001315     0.00\n",
      "NOTE:     26   400 0.000028           0.8159     0.9092  1.432e+04       1430          0   0.001315     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  179      281E-7          0.8835     0.8616  3.741e+05  6.009e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000028           0.7034     0.8915  1.335e+04       1626          0   0.001315     0.00\n",
      "NOTE:      1   400 0.000028           0.9757     0.8867  1.533e+04       1959          0   0.001315     0.00\n",
      "NOTE:      2   400 0.000028           0.8713     0.8652  1.351e+04       2105          0   0.001315     0.00\n",
      "NOTE:      3   400 0.000028           0.9221      0.876  1.525e+04       2159          0   0.001315     0.00\n",
      "NOTE:      4   400 0.000028            1.024     0.8651  1.523e+04       2375          0   0.001315     0.00\n",
      "NOTE:      5   400 0.000028           0.9263     0.8875  1.492e+04       1890          0   0.001315     0.00\n",
      "NOTE:      6   400 0.000028           0.8371     0.8867  1.434e+04       1832          0   0.001315     0.00\n",
      "NOTE:      7   400 0.000028           0.7902     0.8647  1.332e+04       2084          0   0.001315     0.00\n",
      "NOTE:      8   400 0.000028           0.8567      0.829  1.342e+04       2768          0   0.001315     0.00\n",
      "NOTE:      9   400 0.000028           0.7828     0.8722  1.232e+04       1805          0   0.001315     0.00\n",
      "NOTE:     10   400 0.000028           0.8475     0.8067  1.254e+04       3005          0   0.001315     0.00\n",
      "NOTE:     11   400 0.000028           0.8187     0.8566   1.27e+04       2126          0   0.001315     0.00\n",
      "NOTE:     12   400 0.000028           0.9864      0.884  1.646e+04       2160          0   0.001315     0.00\n",
      "NOTE:     13   400 0.000028           0.8524     0.8496  1.269e+04       2247          0   0.001315     0.00\n",
      "NOTE:     14   400 0.000028           0.8093     0.8256  1.265e+04       2672          0   0.001315     0.00\n",
      "NOTE:     15   400 0.000028            0.871     0.8533  1.322e+04       2272          0   0.001315     0.00\n",
      "NOTE:     16   400 0.000028           0.9066     0.8834  1.534e+04       2025          0   0.001315     0.00\n",
      "NOTE:     17   400 0.000028            0.894     0.8804  1.364e+04       1852          0   0.001315     0.00\n",
      "NOTE:     18   400 0.000028           0.7616     0.9121  1.406e+04       1355          0   0.001315     0.00\n",
      "NOTE:     19   400 0.000028           0.8277     0.8669  1.299e+04       1995          0   0.001315     0.00\n",
      "NOTE:     20   400 0.000028            0.796     0.8218  1.163e+04       2522          0   0.001315     0.00\n",
      "NOTE:     21   400 0.000028           0.9585     0.8564  1.496e+04       2507          0   0.001315     0.00\n",
      "NOTE:     22   400 0.000028           0.8014     0.8698  1.226e+04       1836          0   0.001315     0.00\n",
      "NOTE:     23   400 0.000028           0.9231     0.8408  1.284e+04       2430          0   0.001315     0.00\n",
      "NOTE:     24   400 0.000028           0.8651     0.8867  1.494e+04       1908          0   0.001315     0.00\n",
      "NOTE:     25   400 0.000028           0.7789     0.8788  1.287e+04       1776          0   0.001315     0.00\n",
      "NOTE:     26   400 0.000028           0.8858     0.8943  1.497e+04       1769          0   0.001315     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  180      281E-7           0.862     0.8669  3.717e+05  5.706e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000028           0.8417      0.793       9995       2609          0   0.001315     0.00\n",
      "NOTE:      1   400 0.000028           0.8064      0.858  1.221e+04       2021          0   0.001315     0.00\n",
      "NOTE:      2   400 0.000028           0.9409      0.892  1.494e+04       1809          0   0.001315     0.00\n",
      "NOTE:      3   400 0.000028           0.7694     0.8481   1.24e+04       2221          0   0.001315     0.00\n",
      "NOTE:      4   400 0.000028           0.8487     0.8527  1.344e+04       2321          0   0.001315     0.00\n",
      "NOTE:      5   400 0.000028           0.9012     0.8532  1.468e+04       2526          0   0.001315     0.00\n",
      "NOTE:      6   400 0.000028             0.94     0.8928  1.522e+04       1827          0   0.001315     0.00\n",
      "NOTE:      7   400 0.000028           0.8651     0.8152  1.269e+04       2877          0   0.001315     0.00\n",
      "NOTE:      8   400 0.000028           0.9304     0.8514  1.557e+04       2718          0   0.001315     0.00\n",
      "NOTE:      9   400 0.000028            1.052     0.8724  1.755e+04       2568          0   0.001315     0.00\n",
      "NOTE:     10   400 0.000028           0.8488     0.8734  1.326e+04       1921          0   0.001315     0.00\n",
      "NOTE:     11   400 0.000028           0.9166      0.887  1.697e+04       2162          0   0.001315     0.00\n",
      "NOTE:     12   400 0.000028           0.9722     0.8412  1.334e+04       2518          0   0.001315     0.00\n",
      "NOTE:     13   400 0.000028            1.039     0.8743  1.742e+04       2504          0   0.001315     0.00\n",
      "NOTE:     14   400 0.000028           0.8069     0.8834  1.321e+04       1744          0   0.001315     0.00\n",
      "NOTE:     15   400 0.000028           0.9734     0.8682  1.582e+04       2401          0   0.001315     0.00\n",
      "NOTE:     16   400 0.000028             0.86     0.8339   1.21e+04       2411          0   0.001315     0.00\n",
      "NOTE:     17   400 0.000028            0.769     0.8564  1.213e+04       2033          0   0.001315     0.00\n",
      "NOTE:     18   400 0.000028            1.052     0.8724  1.714e+04       2508          0   0.001315     0.00\n",
      "NOTE:     19   400 0.000028           0.6691     0.8928  1.162e+04       1395          0   0.001315     0.00\n",
      "NOTE:     20   400 0.000028           0.8396     0.8384  1.296e+04       2497          0   0.001315     0.00\n",
      "NOTE:     21   400 0.000028           0.8916     0.8707  1.519e+04       2256          0   0.001315     0.00\n",
      "NOTE:     22   400 0.000028            0.725     0.8688  1.164e+04       1757          0   0.001315     0.00\n",
      "NOTE:     23   400 0.000028           0.8944     0.8053  1.283e+04       3101          0   0.001315     0.00\n",
      "NOTE:     24   400 0.000028            1.028     0.8467  1.678e+04       3039          0   0.001315     0.00\n",
      "NOTE:     25   400 0.000028           0.8601      0.885  1.389e+04       1804          0   0.001315     0.00\n",
      "NOTE:     26   400 0.000028            0.816     0.8427  1.223e+04       2283          0   0.001315     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  181      281E-7          0.8836     0.8592  3.772e+05  6.183e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000023           0.9406     0.8639  1.505e+04       2371          0   0.001315     0.00\n",
      "NOTE:      1   400 0.000023            1.104     0.8585  1.729e+04       2849          0   0.001315     0.00\n",
      "NOTE:      2   400 0.000023            0.824     0.8195  1.261e+04       2776          0   0.001315     0.00\n",
      "NOTE:      3   400 0.000023           0.9207     0.8512  1.387e+04       2425          0   0.001315     0.00\n",
      "NOTE:      4   400 0.000023           0.9249      0.854  1.331e+04       2276          0   0.001315     0.00\n",
      "NOTE:      5   400 0.000023           0.8682     0.8603   1.35e+04       2192          0   0.001315     0.00\n",
      "NOTE:      6   400 0.000023           0.8133     0.8523  1.283e+04       2224          0   0.001315     0.00\n",
      "NOTE:      7   400 0.000023           0.8618     0.8718  1.357e+04       1995          0   0.001315     0.00\n",
      "NOTE:      8   400 0.000023           0.8493     0.8769  1.411e+04       1981          0   0.001315     0.00\n",
      "NOTE:      9   400 0.000023           0.9088     0.8815  1.532e+04       2058          0   0.001315     0.00\n",
      "NOTE:     10   400 0.000023           0.8491     0.8326  1.234e+04       2481          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000023           0.9364     0.8703  1.376e+04       2051          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000023           0.9402     0.8554  1.436e+04       2427          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000023           0.8659     0.8184  1.267e+04       2812          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000023           0.9083     0.8927   1.41e+04       1695          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000023           0.7316     0.8335  1.067e+04       2131          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000023           0.7908     0.7935  1.116e+04       2904          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000023           0.8078     0.8729  1.288e+04       1876          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000023           0.8663     0.8444  1.342e+04       2474          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000023           0.7096     0.8875   1.22e+04       1546          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000023           0.9367     0.8816  1.517e+04       2037          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000023            1.067     0.8555  1.606e+04       2714          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000023           0.8945     0.8649  1.441e+04       2251          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000023           0.8538     0.8683  1.273e+04       1931          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000023           0.7768     0.8417  1.183e+04       2225          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000023           0.7497     0.8412  1.221e+04       2306          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000023           0.8165     0.8526  1.316e+04       2275          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  182      225E-7           0.871     0.8561  3.646e+05  6.128e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000023           0.7755     0.8919  1.343e+04       1628          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000023           0.9505     0.8584  1.478e+04       2438          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000023           0.8331     0.8825  1.559e+04       2076          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000023           0.8594     0.8765  1.408e+04       1983          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000023            0.876     0.8721  1.427e+04       2093          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000023           0.7433     0.9024  1.199e+04       1297          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000023            0.917     0.8982  1.463e+04       1659          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000023           0.8816     0.8846  1.416e+04       1846          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000023           0.8492     0.8692  1.434e+04       2158          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000023           0.9097     0.8285  1.388e+04       2874          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000023           0.8934     0.8431  1.395e+04       2597          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000023           0.8932     0.8441  1.259e+04       2325          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000023           0.8197     0.8604  1.331e+04       2159          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000023           0.9526     0.8774  1.607e+04       2245          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000023           0.7783     0.8883  1.315e+04       1653          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000023           0.7954     0.7937  1.101e+04       2861          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000023            1.067     0.7902  1.522e+04       4041          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000023           0.9077     0.8695  1.495e+04       2244          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000023           0.8662     0.8959  1.614e+04       1875          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000023           0.7768     0.8439  1.172e+04       2167          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000023           0.8955      0.858  1.348e+04       2232          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000023           0.7859      0.844  1.055e+04       1950          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000023             1.03     0.8666  1.615e+04       2485          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000023           0.8982      0.847  1.468e+04       2653          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000023            1.013     0.8666  1.561e+04       2403          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000023           0.7964     0.8443  1.224e+04       2257          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000023           0.9037     0.8216  1.279e+04       2777          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  183      225E-7          0.8766     0.8601  3.747e+05  6.098e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000023            0.894      0.885  1.567e+04       2035          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000023           0.9075     0.8937  1.538e+04       1830          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000023           0.8128     0.8767  1.292e+04       1817          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000023           0.8202     0.8459  1.321e+04       2406          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000023           0.8665     0.8832  1.362e+04       1801          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000023           0.8651     0.8633  1.298e+04       2055          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000023           0.8829     0.8521  1.306e+04       2268          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000023           0.8885     0.8125  1.197e+04       2763          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000023           0.8331     0.7671  1.117e+04       3390          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000023            1.014     0.9041  1.811e+04       1922          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000023           0.8486     0.8354  1.263e+04       2489          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000023           0.8914     0.9022  1.449e+04       1571          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000023           0.8505     0.8564  1.272e+04       2133          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000023           0.9494      0.839  1.386e+04       2660          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000023           0.7963     0.8792  1.353e+04       1859          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000023           0.8279     0.8566  1.238e+04       2072          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000023           0.8728     0.8504  1.361e+04       2395          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000023           0.6254     0.8505       9270       1630          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000023           0.9028      0.823  1.379e+04       2966          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000023            0.768     0.8201  1.173e+04       2572          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000023           0.9391     0.8459   1.45e+04       2641          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000023           0.8351     0.8611  1.293e+04       2086          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000023           0.9504     0.8315   1.48e+04       2999          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000023            0.794     0.8955  1.427e+04       1666          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000023            0.961     0.8815  1.642e+04       2207          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000023           0.8122     0.8351  1.229e+04       2427          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000023           0.8412     0.8555  1.308e+04       2209          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  184      225E-7          0.8611     0.8569  3.644e+05  6.087e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000023           0.8847     0.9014   1.36e+04       1488          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000023           0.9066     0.8512  1.323e+04       2313          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000023           0.9096     0.8459  1.413e+04       2573          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000023           0.8267     0.8649  1.293e+04       2020          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000023           0.9588     0.8525  1.433e+04       2480          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000023           0.7875     0.8685  1.266e+04       1916          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000023           0.9028     0.8591  1.378e+04       2261          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000023           0.8833     0.8895  1.497e+04       1859          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000023           0.8326     0.8754  1.422e+04       2024          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000023            0.856     0.8749  1.419e+04       2030          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000023           0.8542     0.8367  1.249e+04       2437          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000023           0.9065     0.8745  1.492e+04       2140          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000023           0.8486     0.8645  1.299e+04       2036          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000023            0.959     0.8569  1.432e+04       2391          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000023           0.7448     0.8946  1.285e+04       1514          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000023           0.8485     0.8816  1.438e+04       1932          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000023            0.918     0.8497  1.373e+04       2428          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000023           0.8123     0.8521  1.254e+04       2177          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000023           0.9141     0.8813  1.488e+04       2003          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000023           0.9493     0.8795  1.568e+04       2148          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000023           0.9256     0.8366  1.302e+04       2542          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000023           0.8051     0.8782  1.284e+04       1781          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000023           0.8444     0.8831  1.388e+04       1836          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000023           0.8947     0.8583  1.272e+04       2100          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000023            1.017      0.841  1.525e+04       2883          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000023           0.8192     0.8959  1.432e+04       1664          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000023           0.8483     0.8927  1.341e+04       1612          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  185      225E-7          0.8762      0.868  3.722e+05  5.659e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000023           0.8261     0.8622  1.303e+04       2083          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000023            1.076     0.8331  1.689e+04       3382          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000023           0.9733     0.8581  1.468e+04       2428          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000023            1.011       0.88  1.547e+04       2109          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000023           0.7491     0.8928    1.2e+04       1440          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000023           0.8018     0.8702   1.21e+04       1805          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000023           0.8925     0.8562  1.411e+04       2370          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000023           0.7974     0.8689  1.246e+04       1880          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000023           0.7861     0.8777  1.258e+04       1753          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000023           0.7904     0.8279  1.097e+04       2279          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000023           0.9036     0.8649  1.505e+04       2351          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000023            1.009     0.8736  1.548e+04       2240          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000023           0.7837     0.8638   1.24e+04       1954          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000023           0.9487     0.8709  1.603e+04       2375          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000023           0.8235      0.853  1.289e+04       2221          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000023            1.058     0.8756  1.565e+04       2223          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000023           0.9317     0.8319  1.453e+04       2937          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000023           0.8702     0.8685  1.509e+04       2285          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000023           0.7049     0.8602  1.005e+04       1633          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000023           0.8458     0.8803  1.505e+04       2046          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000023           0.9294     0.8259  1.418e+04       2989          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000023           0.7977     0.8624  1.276e+04       2035          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000023           0.8791     0.8745   1.37e+04       1967          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000023           0.9087     0.8691  1.603e+04       2416          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000023           0.8347     0.8767  1.348e+04       1896          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000023                1     0.8685  1.511e+04       2288          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000023           0.8709     0.8545  1.341e+04       2284          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  186      225E-7          0.8816     0.8628  3.752e+05  5.967e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000023           0.8496     0.8656  1.431e+04       2222          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000023           0.8766     0.8595  1.317e+04       2152          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000023           0.8694     0.8679  1.354e+04       2061          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000023            0.854     0.8458  1.355e+04       2469          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000023           0.9601     0.8893  1.582e+04       1969          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000023            1.007     0.8068  1.482e+04       3549          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000023            1.014     0.8264  1.547e+04       3248          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000023           0.8477     0.8777  1.349e+04       1880          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000023            1.015     0.8441  1.496e+04       2763          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000023           0.8743     0.8787  1.403e+04       1938          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000023           0.7625     0.8281  1.167e+04       2423          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000023           0.8189     0.8386  1.217e+04       2342          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000023           0.9509     0.8541  1.362e+04       2326          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000023           0.7887     0.8213  1.124e+04       2445          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000023           0.9676     0.8362  1.509e+04       2957          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000023            0.937     0.8317  1.334e+04       2700          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000023           0.8069     0.8285  1.157e+04       2396          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000023           0.9012      0.877  1.471e+04       2063          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000023           0.8951      0.831   1.31e+04       2664          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000023           0.8926     0.8466  1.251e+04       2266          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000023           0.8413     0.8449  1.274e+04       2338          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000023           0.7165      0.846  1.089e+04       1982          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000023           0.9424     0.8661  1.483e+04       2293          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000023           0.9825     0.8767  1.451e+04       2041          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000023           0.8091     0.8822  1.303e+04       1739          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000023           0.9982     0.8216  1.486e+04       3227          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000023           0.8007     0.8447   1.12e+04       2059          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  187      225E-7          0.8881     0.8495  3.642e+05  6.451e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000023           0.9458     0.8031  1.299e+04       3184          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000023           0.9649     0.8756  1.534e+04       2180          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000023           0.8548       0.85   1.24e+04       2189          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000023             0.87     0.8934  1.368e+04       1632          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000023           0.7755     0.8702  1.308e+04       1952          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000023           0.8428     0.8637  1.364e+04       2153          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000023           0.9484     0.8412  1.455e+04       2747          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000023           0.8986     0.8688  1.463e+04       2209          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000023           0.8981     0.7998  1.161e+04       2907          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000023           0.8333     0.8751   1.38e+04       1970          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000023           0.9117     0.8929  1.455e+04       1745          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000023           0.8676     0.8768  1.432e+04       2012          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000023           0.8677     0.8113  1.275e+04       2966          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000023           0.8303     0.8641   1.26e+04       1981          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000023           0.9781     0.8682  1.553e+04       2356          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000023           0.9661     0.8897  1.351e+04       1676          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000023           0.7818     0.9068  1.334e+04       1371          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000023           0.8481     0.8469    1.3e+04       2350          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000023           0.8405     0.8477  1.321e+04       2373          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000023           0.8423     0.8789  1.428e+04       1968          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000023           0.9422     0.9018  1.602e+04       1745          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000023           0.8576      0.895  1.537e+04       1803          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000023           0.9554     0.8293  1.416e+04       2914          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000023           0.8261     0.8756  1.221e+04       1734          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000023           0.9029     0.8214  1.257e+04       2734          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000023           0.8557     0.8615  1.279e+04       2057          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000023           0.8814      0.876  1.446e+04       2047          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  188      225E-7           0.881     0.8627  3.704e+05  5.896e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000023            0.764     0.8047  1.127e+04       2736          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000023           0.8799     0.8688  1.346e+04       2032          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000023            1.006      0.869  1.598e+04       2410          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000023           0.8699     0.8569  1.317e+04       2200          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000023           0.9374      0.844  1.379e+04       2548          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000023           0.9908     0.8085  1.368e+04       3240          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000023           0.9203     0.8496   1.42e+04       2514          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000023           0.7847     0.8955  1.114e+04       1299          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000023           0.8833     0.8497  1.352e+04       2392          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000023           0.9308     0.8459   1.45e+04       2641          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000023           0.9772     0.8666   1.57e+04       2418          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000023           0.9642     0.8562  1.516e+04       2546          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000023           0.8977      0.894  1.455e+04       1725          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000023           0.8268     0.8836  1.422e+04       1873          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000023           0.9492     0.8481  1.426e+04       2554          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000023           0.8193     0.8645  1.297e+04       2033          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000023           0.8438      0.877  1.438e+04       2017          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000023           0.9012     0.8791  1.501e+04       2064          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000023           0.8745     0.8613  1.304e+04       2100          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000023           0.8949     0.8554  1.419e+04       2400          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000023           0.8228     0.8343  1.267e+04       2515          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000023            0.915     0.8607   1.49e+04       2412          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000023           0.9164     0.8599  1.397e+04       2276          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000023            1.052      0.843  1.556e+04       2898          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000023           0.8876     0.8733  1.319e+04       1914          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000023           0.8883     0.8983  1.555e+04       1761          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000023           0.8664     0.8155  1.216e+04       2751          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  189      225E-7          0.8987      0.858  3.762e+05  6.227e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000023           0.8889     0.8603  1.362e+04       2211          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000023           0.8879     0.8819  1.463e+04       1960          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000023           0.7527     0.8274  1.061e+04       2213          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000023           0.9385     0.8862  1.529e+04       1963          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000023           0.9683     0.8696    1.5e+04       2250          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000023            1.043      0.855  1.671e+04       2832          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000023           0.7629     0.8654  1.175e+04       1828          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000023           0.7245     0.8814  1.199e+04       1613          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000023           0.7756     0.8709   1.19e+04       1765          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000023           0.7952     0.8548  1.159e+04       1968          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000023           0.8676     0.8826  1.514e+04       2013          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000023           0.7959     0.8653  1.297e+04       2018          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000023            1.015     0.8846  1.656e+04       2160          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000023            1.022     0.8408  1.509e+04       2857          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000023           0.8344     0.8534  1.329e+04       2282          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000023           0.7266     0.8497  1.129e+04       1997          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000023           0.8317     0.8776  1.291e+04       1801          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000023           0.8272     0.8501  1.327e+04       2339          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000023           0.7233     0.8711  1.116e+04       1652          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000023           0.7821     0.8222  1.052e+04       2275          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000023           0.9425      0.878  1.642e+04       2280          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000023            0.826     0.8423  1.331e+04       2491          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000023            1.043     0.8204  1.453e+04       3181          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000023           0.8166     0.8697   1.31e+04       1962          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000023           0.9636     0.8603  1.417e+04       2300          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000023           0.9532      0.845  1.465e+04       2687          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000023           0.6588     0.8621  1.043e+04       1668          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  190      225E-7          0.8581     0.8607  3.619e+05  5.857e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000018           0.8732     0.8438  1.239e+04       2294          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000018            0.924     0.8673  1.483e+04       2269          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000018           0.7765     0.8665  1.215e+04       1872          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000018           0.8306      0.837  1.198e+04       2332          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000018           0.9242     0.8107  1.395e+04       3256          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000018           0.8673     0.8666  1.418e+04       2184          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000018           0.8027     0.8631  1.225e+04       1944          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000018            1.021     0.8092  1.467e+04       3458          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000018           0.7188     0.8873  1.313e+04       1667          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000018           0.9774     0.8291  1.527e+04       3148          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000018           0.7691     0.8623  1.184e+04       1891          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000018           0.8331     0.8211  1.258e+04       2741          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000018           0.9158     0.8278  1.289e+04       2680          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000018           0.7893     0.8537  1.073e+04       1839          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000018           0.7713     0.8591  1.178e+04       1932          0   0.001316     0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE:     15   400 0.000018           0.9096     0.8682  1.493e+04       2266          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000018            0.919     0.8783  1.385e+04       1919          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000018           0.9269     0.8141   1.25e+04       2854          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000018           0.9429     0.8617  1.465e+04       2351          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000018           0.9094     0.8687  1.501e+04       2269          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000018           0.9821     0.8696    1.5e+04       2250          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000018           0.8764     0.8618  1.318e+04       2113          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000018           0.8679     0.8407   1.32e+04       2500          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000018           0.8617      0.853  1.193e+04       2057          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000018           0.9493     0.8591  1.428e+04       2343          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000018           0.8808     0.8984  1.585e+04       1793          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000018           0.8645     0.8683  1.431e+04       2171          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  191       18E-6          0.8772     0.8534  3.633e+05  6.239e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000018            0.934      0.849  1.389e+04       2472          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000018           0.8994     0.8753  1.477e+04       2103          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000018           0.6777      0.875  1.088e+04       1553          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000018            0.976     0.8516   1.52e+04       2649          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000018           0.8286     0.8821  1.382e+04       1847          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000018            1.104     0.8578   1.68e+04       2785          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000018           0.9851     0.8494   1.55e+04       2748          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000018            0.837     0.8772   1.32e+04       1848          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000018           0.8558     0.8369  1.236e+04       2408          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000018            0.885     0.7982  1.122e+04       2835          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000018           0.7666     0.8064  1.014e+04       2435          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000018            1.041     0.8618  1.686e+04       2702          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000018           0.8726     0.9012  1.525e+04       1672          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000018            1.051     0.8861  1.603e+04       2060          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000018           0.7976     0.8628  1.292e+04       2054          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000018           0.8715     0.8804  1.312e+04       1782          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000018           0.9026     0.8709  1.484e+04       2201          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000018           0.7452     0.8911  1.277e+04       1561          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000018           0.8711     0.8356  1.277e+04       2513          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000018           0.8395     0.8452  1.386e+04       2540          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000018           0.8992     0.8687  1.487e+04       2248          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000018           0.8414     0.8655  1.284e+04       1995          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000018           0.8623     0.8453  1.365e+04       2498          0   0.001316     0.00\n",
      "NOTE:     23   400 0.000018            0.861     0.8393  1.301e+04       2492          0   0.001316     0.00\n",
      "NOTE:     24   400 0.000018           0.8385     0.8376  1.237e+04       2397          0   0.001316     0.00\n",
      "NOTE:     25   400 0.000018           0.8557     0.8646  1.397e+04       2188          0   0.001316     0.00\n",
      "NOTE:     26   400 0.000018           0.9591     0.8755  1.461e+04       2077          0   0.001316     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  192       18E-6          0.8836     0.8596  3.715e+05  6.066e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000018           0.9749     0.8216  1.361e+04       2956          0   0.001316     0.00\n",
      "NOTE:      1   400 0.000018           0.7642     0.8646  1.287e+04       2015          0   0.001316     0.00\n",
      "NOTE:      2   400 0.000018           0.8528     0.8543  1.313e+04       2239          0   0.001316     0.00\n",
      "NOTE:      3   400 0.000018           0.8764       0.87  1.422e+04       2125          0   0.001316     0.00\n",
      "NOTE:      4   400 0.000018           0.8658     0.8765  1.328e+04       1870          0   0.001316     0.00\n",
      "NOTE:      5   400 0.000018           0.7912      0.821   1.13e+04       2462          0   0.001316     0.00\n",
      "NOTE:      6   400 0.000018           0.7696     0.8826  1.434e+04       1908          0   0.001316     0.00\n",
      "NOTE:      7   400 0.000018           0.7833     0.8305  1.113e+04       2270          0   0.001316     0.00\n",
      "NOTE:      8   400 0.000018           0.8583     0.8716  1.325e+04       1952          0   0.001316     0.00\n",
      "NOTE:      9   400 0.000018           0.9003       0.87  1.427e+04       2133          0   0.001316     0.00\n",
      "NOTE:     10   400 0.000018            0.937     0.8269  1.379e+04       2887          0   0.001316     0.00\n",
      "NOTE:     11   400 0.000018           0.8678     0.8189  1.341e+04       2967          0   0.001316     0.00\n",
      "NOTE:     12   400 0.000018           0.9342     0.8503  1.533e+04       2698          0   0.001316     0.00\n",
      "NOTE:     13   400 0.000018           0.8915     0.8888  1.512e+04       1891          0   0.001316     0.00\n",
      "NOTE:     14   400 0.000018           0.8886     0.8933  1.488e+04       1778          0   0.001316     0.00\n",
      "NOTE:     15   400 0.000018           0.8299     0.8362  1.277e+04       2502          0   0.001316     0.00\n",
      "NOTE:     16   400 0.000018           0.9665     0.8873  1.654e+04       2100          0   0.001316     0.00\n",
      "NOTE:     17   400 0.000018           0.8832     0.9172  1.582e+04       1427          0   0.001316     0.00\n",
      "NOTE:     18   400 0.000018           0.7747     0.8666  1.226e+04       1887          0   0.001316     0.00\n",
      "NOTE:     19   400 0.000018           0.9523     0.8002  1.247e+04       3112          0   0.001316     0.00\n",
      "NOTE:     20   400 0.000018           0.8435     0.8721  1.379e+04       2023          0   0.001316     0.00\n",
      "NOTE:     21   400 0.000018           0.9153     0.8487  1.429e+04       2547          0   0.001316     0.00\n",
      "NOTE:     22   400 0.000018           0.9152     0.8572  1.242e+04       2068          0   0.001317     0.00\n",
      "NOTE:     23   400 0.000018           0.7649     0.8472   1.13e+04       2039          0   0.001317     0.00\n",
      "NOTE:     24   400 0.000018           0.7988     0.8794  1.316e+04       1805          0   0.001317     0.00\n",
      "NOTE:     25   400 0.000018            1.082     0.8727  1.739e+04       2535          0   0.001317     0.00\n",
      "NOTE:     26   400 0.000018           0.9238     0.8559   1.35e+04       2274          0   0.001317     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  193       18E-6          0.8743     0.8594  3.696e+05  6.047e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000018           0.9259      0.862  1.434e+04       2295          0   0.001317     0.00\n",
      "NOTE:      1   400 0.000018           0.8655     0.8919  1.422e+04       1723          0   0.001317     0.00\n",
      "NOTE:      2   400 0.000018           0.7704     0.8985   1.32e+04       1492          0   0.001317     0.00\n",
      "NOTE:      3   400 0.000018           0.8048     0.8441  1.303e+04       2407          0   0.001317     0.00\n",
      "NOTE:      4   400 0.000018           0.8559     0.8566  1.323e+04       2216          0   0.001317     0.00\n",
      "NOTE:      5   400 0.000018           0.9783     0.8467  1.397e+04       2529          0   0.001317     0.00\n",
      "NOTE:      6   400 0.000018           0.8194     0.8973  1.469e+04       1680          0   0.001317     0.00\n",
      "NOTE:      7   400 0.000018           0.7974     0.9086  1.435e+04       1444          0   0.001317     0.00\n",
      "NOTE:      8   400 0.000018            0.895     0.8619  1.415e+04       2267          0   0.001317     0.00\n",
      "NOTE:      9   400 0.000018            1.072     0.8404  1.609e+04       3055          0   0.001317     0.00\n",
      "NOTE:     10   400 0.000018           0.9133      0.851  1.423e+04       2492          0   0.001317     0.00\n",
      "NOTE:     11   400 0.000018           0.9802     0.8295  1.404e+04       2886          0   0.001317     0.00\n",
      "NOTE:     12   400 0.000018           0.8342     0.8527  1.282e+04       2214          0   0.001317     0.00\n",
      "NOTE:     13   400 0.000018             0.94     0.8934  1.445e+04       1725          0   0.001317     0.00\n",
      "NOTE:     14   400 0.000018           0.8165     0.8901  1.448e+04       1788          0   0.001317     0.00\n",
      "NOTE:     15   400 0.000018           0.7711     0.8718  1.303e+04       1916          0   0.001317     0.00\n",
      "NOTE:     16   400 0.000018            0.965     0.8552  1.412e+04       2390          0   0.001317     0.00\n",
      "NOTE:     17   400 0.000018           0.8861     0.8408  1.355e+04       2566          0   0.001317     0.00\n",
      "NOTE:     18   400 0.000018           0.9493     0.8903  1.558e+04       1919          0   0.001317     0.00\n",
      "NOTE:     19   400 0.000018            1.006     0.8516  1.463e+04       2549          0   0.001317     0.00\n",
      "NOTE:     20   400 0.000018           0.9452     0.8662  1.512e+04       2335          0   0.001317     0.00\n",
      "NOTE:     21   400 0.000018             0.81     0.8465  1.228e+04       2227          0   0.001317     0.00\n",
      "NOTE:     22   400 0.000018           0.8534     0.9076  1.534e+04       1562          0   0.001317     0.00\n",
      "NOTE:     23   400 0.000018            0.938     0.7902  1.297e+04       3443          0   0.001317     0.00\n",
      "NOTE:     24   400 0.000018           0.8751     0.8957  1.539e+04       1792          0   0.001317     0.00\n",
      "NOTE:     25   400 0.000018           0.7141     0.8955   1.21e+04       1412          0   0.001317     0.00\n",
      "NOTE:     26   400 0.000018           0.8709      0.863  1.406e+04       2232          0   0.001317     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  194       18E-6          0.8835     0.8663  3.795e+05  5.856e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000018           0.7087     0.8637  1.214e+04       1915          0   0.001317     0.00\n",
      "NOTE:      1   400 0.000018           0.8665     0.8423  1.366e+04       2558          0   0.001317     0.00\n",
      "NOTE:      2   400 0.000018           0.9598     0.9011  1.663e+04       1826          0   0.001317     0.00\n",
      "NOTE:      3   400 0.000018           0.9394     0.8567  1.395e+04       2332          0   0.001317     0.00\n",
      "NOTE:      4   400 0.000018           0.8934     0.8601  1.394e+04       2268          0   0.001317     0.00\n",
      "NOTE:      5   400 0.000018           0.8752     0.8567  1.501e+04       2511          0   0.001317     0.00\n",
      "NOTE:      6   400 0.000018           0.9558     0.8398  1.438e+04       2743          0   0.001317     0.00\n",
      "NOTE:      7   400 0.000018           0.7636     0.8946  1.256e+04       1480          0   0.001317     0.00\n",
      "NOTE:      8   400 0.000018           0.8589     0.8496  1.224e+04       2166          0   0.001317     0.00\n",
      "NOTE:      9   400 0.000018           0.9016       0.86  1.393e+04       2267          0   0.001317     0.00\n",
      "NOTE:     10   400 0.000018           0.9752     0.8548  1.529e+04       2597          0   0.001317     0.00\n",
      "NOTE:     11   400 0.000018           0.9069     0.8851  1.472e+04       1911          0   0.001317     0.00\n",
      "NOTE:     12   400 0.000018           0.7872     0.8683  1.217e+04       1847          0   0.001317     0.00\n",
      "NOTE:     13   400 0.000018           0.9631     0.8221   1.44e+04       3116          0   0.001317     0.00\n",
      "NOTE:     14   400 0.000018           0.8382     0.8561   1.33e+04       2235          0   0.001317     0.00\n",
      "NOTE:     15   400 0.000018           0.8824     0.8973   1.44e+04       1648          0   0.001317     0.00\n",
      "NOTE:     16   400 0.000018           0.7712     0.8459  1.168e+04       2127          0   0.001317     0.00\n",
      "NOTE:     17   400 0.000018           0.6852     0.9044  1.187e+04       1255          0   0.001317     0.00\n",
      "NOTE:     18   400 0.000018           0.7964     0.8889  1.407e+04       1759          0   0.001317     0.00\n",
      "NOTE:     19   400 0.000018           0.9067      0.898  1.601e+04       1818          0   0.001317     0.00\n",
      "NOTE:     20   400 0.000018           0.6744     0.8746  1.125e+04       1614          0   0.001317     0.00\n",
      "NOTE:     21   400 0.000018           0.8065     0.8842  1.327e+04       1738          0   0.001317     0.00\n",
      "NOTE:     22   400 0.000018           0.9427     0.8376  1.361e+04       2640          0   0.001317     0.00\n",
      "NOTE:     23   400 0.000018           0.8859      0.857  1.379e+04       2300          0   0.001317     0.00\n",
      "NOTE:     24   400 0.000018           0.7314       0.87  1.187e+04       1773          0   0.001317     0.00\n",
      "NOTE:     25   400 0.000018           0.6909     0.8973  1.116e+04       1278          0   0.001317     0.00\n",
      "NOTE:     26   400 0.000018           0.7871     0.8954  1.408e+04       1645          0   0.001317     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  195       18E-6          0.8427     0.8684  3.654e+05  5.537e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000018           0.9044     0.8665  1.393e+04       2147          0   0.001317     0.00\n",
      "NOTE:      1   400 0.000018           0.9321     0.8602  1.484e+04       2411          0   0.001317     0.00\n",
      "NOTE:      2   400 0.000018           0.7544     0.8783  1.116e+04       1547          0   0.001317     0.00\n",
      "NOTE:      3   400 0.000018           0.9258     0.8673  1.524e+04       2331          0   0.001317     0.00\n",
      "NOTE:      4   400 0.000018           0.7893     0.8399  1.209e+04       2305          0   0.001317     0.00\n",
      "NOTE:      5   400 0.000018           0.8499     0.8601  1.353e+04       2200          0   0.001317     0.00\n",
      "NOTE:      6   400 0.000018            1.018     0.8691  1.698e+04       2558          0   0.001317     0.00\n",
      "NOTE:      7   400 0.000018           0.9004     0.8413  1.373e+04       2590          0   0.001317     0.00\n",
      "NOTE:      8   400 0.000018           0.7785     0.8517  1.269e+04       2209          0   0.001317     0.00\n",
      "NOTE:      9   400 0.000018           0.8856     0.9019  1.544e+04       1679          0   0.001317     0.00\n",
      "NOTE:     10   400 0.000018           0.7593     0.8203   1.11e+04       2432          0   0.001317     0.00\n",
      "NOTE:     11   400 0.000018           0.9148      0.842  1.371e+04       2572          0   0.001317     0.00\n",
      "NOTE:     12   400 0.000018           0.9113     0.8626  1.451e+04       2311          0   0.001317     0.00\n",
      "NOTE:     13   400 0.000018           0.8557     0.8921  1.342e+04       1623          0   0.001317     0.00\n",
      "NOTE:     14   400 0.000018            0.764     0.8684   1.18e+04       1788          0   0.001317     0.00\n",
      "NOTE:     15   400 0.000018           0.9166     0.8323   1.32e+04       2659          0   0.001317     0.00\n",
      "NOTE:     16   400 0.000018           0.9881     0.8604  1.562e+04       2533          0   0.001317     0.00\n",
      "NOTE:     17   400 0.000018            1.196     0.8586  1.841e+04       3031          0   0.001317     0.00\n",
      "NOTE:     18   400 0.000018           0.7057     0.8242       9845       2100          0   0.001317     0.00\n",
      "NOTE:     19   400 0.000018           0.9417     0.8512  1.488e+04       2600          0   0.001317     0.00\n",
      "NOTE:     20   400 0.000018            1.038     0.8411  1.363e+04       2575          0   0.001317     0.00\n",
      "NOTE:     21   400 0.000018           0.8829      0.881  1.398e+04       1888          0   0.001317     0.00\n",
      "NOTE:     22   400 0.000018            1.016     0.8799  1.713e+04       2338          0   0.001317     0.00\n",
      "NOTE:     23   400 0.000018           0.9274     0.8618  1.366e+04       2191          0   0.001317     0.00\n",
      "NOTE:     24   400 0.000018            1.088     0.8989  1.775e+04       1996          0   0.001317     0.00\n",
      "NOTE:     25   400 0.000018           0.9029     0.8816  1.569e+04       2107          0   0.001317     0.00\n",
      "NOTE:     26   400 0.000018           0.8093     0.8585  1.259e+04       2075          0   0.001317     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  196       18E-6          0.9021     0.8622  3.805e+05   6.08e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000018           0.8747     0.9139  1.574e+04       1484          0   0.001317     0.00\n",
      "NOTE:      1   400 0.000018           0.9857     0.8913  1.678e+04       2046          0   0.001317     0.00\n",
      "NOTE:      2   400 0.000018            0.802     0.8317  1.122e+04       2271          0   0.001317     0.00\n",
      "NOTE:      3   400 0.000018           0.8809     0.8665  1.393e+04       2147          0   0.001317     0.00\n",
      "NOTE:      4   400 0.000018           0.9777     0.7927  1.363e+04       3564          0   0.001317     0.00\n",
      "NOTE:      5   400 0.000018            1.065     0.8486  1.591e+04       2840          0   0.001317     0.00\n",
      "NOTE:      6   400 0.000018           0.9509     0.8498  1.421e+04       2511          0   0.001317     0.00\n",
      "NOTE:      7   400 0.000018           0.8161     0.8594  1.382e+04       2261          0   0.001317     0.00\n",
      "NOTE:      8   400 0.000018           0.9504     0.8632  1.522e+04       2412          0   0.001317     0.00\n",
      "NOTE:      9   400 0.000018           0.8871     0.8832  1.445e+04       1911          0   0.001317     0.00\n",
      "NOTE:     10   400 0.000018           0.8644     0.8923  1.467e+04       1772          0   0.001317     0.00\n",
      "NOTE:     11   400 0.000018            1.007     0.8406  1.537e+04       2914          0   0.001317     0.00\n",
      "NOTE:     12   400 0.000018           0.9194      0.864  1.391e+04       2190          0   0.001317     0.00\n",
      "NOTE:     13   400 0.000018           0.9123     0.8744  1.396e+04       2004          0   0.001317     0.00\n",
      "NOTE:     14   400 0.000018           0.8601     0.8408  1.395e+04       2641          0   0.001317     0.00\n",
      "NOTE:     15   400 0.000018           0.8735     0.8964  1.463e+04       1690          0   0.001317     0.00\n",
      "NOTE:     16   400 0.000018           0.7969     0.8905  1.323e+04       1627          0   0.001317     0.00\n",
      "NOTE:     17   400 0.000018           0.7552     0.8251  1.024e+04       2171          0   0.001317     0.00\n",
      "NOTE:     18   400 0.000018            0.783     0.8835  1.312e+04       1729          0   0.001317     0.00\n",
      "NOTE:     19   400 0.000018           0.8516     0.8796  1.421e+04       1945          0   0.001317     0.00\n",
      "NOTE:     20   400 0.000018           0.9126     0.8629  1.479e+04       2350          0   0.001317     0.00\n",
      "NOTE:     21   400 0.000018           0.8332      0.893  1.411e+04       1690          0   0.001317     0.00\n",
      "NOTE:     22   400 0.000018           0.8386      0.886  1.446e+04       1861          0   0.001317     0.00\n",
      "NOTE:     23   400 0.000018           0.8753     0.8413  1.377e+04       2596          0   0.001317     0.00\n",
      "NOTE:     24   400 0.000018             0.89     0.8515  1.322e+04       2304          0   0.001317     0.00\n",
      "NOTE:     25   400 0.000018           0.9628     0.8229  1.388e+04       2988          0   0.001317     0.00\n",
      "NOTE:     26   400 0.000018           0.8169     0.8227  1.117e+04       2407          0   0.001317     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  197       18E-6          0.8868     0.8622  3.776e+05  6.033e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000018           0.7599     0.8888  1.255e+04       1570          0   0.001317     0.00\n",
      "NOTE:      1   400 0.000018           0.9576     0.8605  1.337e+04       2167          0   0.001317     0.00\n",
      "NOTE:      2   400 0.000018           0.9614     0.8543  1.434e+04       2446          0   0.001317     0.00\n",
      "NOTE:      3   400 0.000018           0.8405     0.8778    1.4e+04       1949          0   0.001317     0.00\n",
      "NOTE:      4   400 0.000018           0.8367     0.8658  1.323e+04       2051          0   0.001317     0.00\n",
      "NOTE:      5   400 0.000018           0.9395     0.8557  1.482e+04       2500          0   0.001317     0.00\n",
      "NOTE:      6   400 0.000018           0.8138     0.8879  1.344e+04       1696          0   0.001317     0.00\n",
      "NOTE:      7   400 0.000018           0.7616     0.8661  1.344e+04       2078          0   0.001317     0.00\n",
      "NOTE:      8   400 0.000018           0.9815     0.8135  1.481e+04       3395          0   0.001317     0.00\n",
      "NOTE:      9   400 0.000018            0.935     0.8761  1.474e+04       2085          0   0.001317     0.00\n",
      "NOTE:     10   400 0.000018            0.783     0.8605  1.272e+04       2062          0   0.001317     0.00\n",
      "NOTE:     11   400 0.000018           0.7665     0.8215  1.098e+04       2386          0   0.001317     0.00\n",
      "NOTE:     12   400 0.000018           0.9045     0.8809  1.387e+04       1874          0   0.001317     0.00\n",
      "NOTE:     13   400 0.000018            0.941     0.8681  1.548e+04       2352          0   0.001317     0.00\n",
      "NOTE:     14   400 0.000018            0.779      0.884   1.38e+04       1810          0   0.001317     0.00\n",
      "NOTE:     15   400 0.000018            0.778     0.9136  1.381e+04       1306          0   0.001317     0.00\n",
      "NOTE:     16   400 0.000018            1.068     0.8798  1.705e+04       2328          0   0.001317     0.00\n",
      "NOTE:     17   400 0.000018           0.7526     0.8685   1.21e+04       1833          0   0.001317     0.00\n",
      "NOTE:     18   400 0.000018           0.8474     0.8992  1.538e+04       1725          0   0.001317     0.00\n",
      "NOTE:     19   400 0.000018           0.8818     0.9065  1.557e+04       1606          0   0.001317     0.00\n",
      "NOTE:     20   400 0.000018           0.8103      0.881  1.274e+04       1721          0   0.001317     0.00\n",
      "NOTE:     21   400 0.000018           0.9208     0.8938  1.419e+04       1686          0   0.001317     0.00\n",
      "NOTE:     22   400 0.000018           0.8802     0.8584   1.35e+04       2228          0   0.001317     0.00\n",
      "NOTE:     23   400 0.000018           0.7559     0.8732  1.165e+04       1692          0   0.001317     0.00\n",
      "NOTE:     24   400 0.000018            0.886     0.8263  1.268e+04       2666          0   0.001317     0.00\n",
      "NOTE:     25   400 0.000018           0.8025      0.882  1.284e+04       1718          0   0.001317     0.00\n",
      "NOTE:     26   400 0.000018           0.9726     0.8828  1.594e+04       2117          0   0.001317     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  198       18E-6          0.8636     0.8714   3.73e+05  5.505e+04          0     0.01\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied     L2Norm   Time(s) (Training)\n",
      "NOTE:      0   400 0.000018           0.8322     0.8649  1.365e+04       2131          0   0.001317     0.00\n",
      "NOTE:      1   400 0.000018           0.8175     0.8884   1.37e+04       1720          0   0.001317     0.00\n",
      "NOTE:      2   400 0.000018           0.8923     0.8515  1.353e+04       2361          0   0.001317     0.00\n",
      "NOTE:      3   400 0.000018           0.8711     0.8602  1.363e+04       2215          0   0.001317     0.00\n",
      "NOTE:      4   400 0.000018           0.9973     0.8594  1.509e+04       2469          0   0.001317     0.00\n",
      "NOTE:      5   400 0.000018           0.7445     0.8655  1.172e+04       1821          0   0.001317     0.00\n",
      "NOTE:      6   400 0.000018            1.025     0.8736  1.635e+04       2366          0   0.001317     0.00\n",
      "NOTE:      7   400 0.000018            1.006     0.8193  1.438e+04       3171          0   0.001317     0.00\n",
      "NOTE:      8   400 0.000018           0.9129     0.8715  1.472e+04       2170          0   0.001317     0.00\n",
      "NOTE:      9   400 0.000018           0.9692     0.8736  1.408e+04       2038          0   0.001317     0.00\n",
      "NOTE:     10   400 0.000018           0.7304     0.8835  1.219e+04       1607          0   0.001317     0.00\n",
      "NOTE:     11   400 0.000018           0.8684     0.8475  1.331e+04       2396          0   0.001317     0.00\n",
      "NOTE:     12   400 0.000018            1.043     0.8522  1.443e+04       2501          0   0.001317     0.00\n",
      "NOTE:     13   400 0.000018           0.9694     0.8381  1.478e+04       2855          0   0.001317     0.00\n",
      "NOTE:     14   400 0.000018           0.7873     0.8059  1.028e+04       2476          0   0.001317     0.00\n",
      "NOTE:     15   400 0.000018           0.8319      0.896  1.349e+04       1566          0   0.001317     0.00\n",
      "NOTE:     16   400 0.000018           0.8823     0.8742   1.54e+04       2216          0   0.001317     0.00\n",
      "NOTE:     17   400 0.000018           0.8766     0.8262  1.222e+04       2570          0   0.001317     0.00\n",
      "NOTE:     18   400 0.000018           0.8842     0.8949  1.509e+04       1772          0   0.001317     0.00\n",
      "NOTE:     19   400 0.000018            0.659     0.9134  1.208e+04       1145          0   0.001317     0.00\n",
      "NOTE:     20   400 0.000018           0.7671     0.8674  1.283e+04       1961          0   0.001317     0.00\n",
      "NOTE:     21   400 0.000018           0.9664      0.839  1.459e+04       2798          0   0.001317     0.00\n",
      "NOTE:     22   400 0.000018           0.8494     0.8248   1.11e+04       2358          0   0.001317     0.00\n",
      "NOTE:     23   400 0.000018           0.9185     0.8817  1.453e+04       1950          0   0.001317     0.00\n",
      "NOTE:     24   400 0.000018           0.7935     0.8645  1.166e+04       1828          0   0.001317     0.00\n",
      "NOTE:     25   400 0.000018           0.9042     0.8572   1.41e+04       2350          0   0.001317     0.00\n",
      "NOTE:     26   400 0.000018           0.9393     0.8913  1.679e+04       2047          0   0.001317     0.00\n",
      "NOTE:  Epoch Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s)\n",
      "NOTE:  199       18E-6          0.8792     0.8627  3.697e+05  5.886e+04          0     0.01\n",
      "NOTE:  The optimization reached the maximum number of epochs.\n",
      "NOTE:  The total time is       2.40 (s).\n"
     ]
    }
   ],
   "source": [
    "optimizer = Optimizer(algorithm=solver, mini_batch_size=200, log_level=4, max_epochs=200, reg_l2=0.001)\n",
    "# start to train\n",
    "print(coxModel.model_name)\n",
    "s.droptable(coxModel.model_weights.name)\n",
    "coxTrain_res = coxModel.fit(\n",
    "                      data=trainTbl,\n",
    "                      data_specs=[\n",
    "                      dict(layer='input1', type='numnom', data=(inputVars), nominals=nominals),\n",
    "                      dict(layer='survival1', type='numnom', data='y')],\n",
    "                      n_threads= 2, \n",
    "                      record_seed=13309, \n",
    "                      optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(coxTrain_res[\"OptIterHistory\"][[\"Loss\"]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate  Model Performance <a name=\"evaluation\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Overall model predictive performance in terms of C-Index on test data\n",
    "\n",
    "- C-index (Concordance index) is a commonly used metric to evaluate and compare survival model's discriminative power.\n",
    "- It is calculated as the fraction of pairs of observations whose observed and predicted outcomes agree among all possible pairs. That is, the longer the observed survival time is, the higher the predicted survival probability is.\n",
    "- The higher the C-index is, the better the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C-Index for Cox survival model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Due to data distribution, miniBatchSize has been limited to 66.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       0.01 (s).\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s) (Validation)\n",
      "NOTE:      0  4500        0           0.6322     0.8544  1.765e+06  3.007e+05          0     0.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ScoreInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Number of Observations Read</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Number of Observations Used</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Loss Error</td>\n",
       "      <td>0.63222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; CumHazard</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Time\">Time</th>\n",
       "      <th title=\"Cumulative hazard\">CumHaz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.001916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.076830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.175121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.381753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.492437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CASUSER(guilin)</td>\n",
       "      <td>Valid_Res_t1FoGP</td>\n",
       "      <td>4500</td>\n",
       "      <td>13</td>\n",
       "      <td>CASTable('Valid_Res_t1FoGP', caslib='CASUSER(g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.0387s</span> &#183; <span class=\"cas-user\">user 0.0744s</span> &#183; <span class=\"cas-sys\">sys 0.0387s</span> &#183; <span class=\"cas-memory\">mem 156MB</span></small></p>"
      ],
      "text/plain": [
       "[ScoreInfo]\n",
       "\n",
       "                          Descr         Value\n",
       " 0  Number of Observations Read          4500\n",
       " 1  Number of Observations Used          4500\n",
       " 2                   Loss Error       0.63222\n",
       "\n",
       "[CumHazard]\n",
       "\n",
       "    Time    CumHaz\n",
       " 0   2.0  0.001916\n",
       " 1   3.0  0.076830\n",
       " 2   4.0  0.175121\n",
       " 3   5.0  0.381753\n",
       " 4   6.0  0.492437\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib              Name  Rows  Columns  \\\n",
       " 0  CASUSER(guilin)  Valid_Res_t1FoGP  4500       13   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Valid_Res_t1FoGP', caslib='CASUSER(g...  \n",
       "\n",
       "+ Elapsed: 0.0387s, user: 0.0744s, sys: 0.0387s, mem: 156mb"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify log_level=4 to compute C-Index for Cox model\n",
    "coxModel.predict(testTbl,buffer_size=10000,log_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C-Index for deep survival model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Due to data distribution, miniBatchSize has been limited to 66.\n",
      "NOTE:  Loading weights cost       0.00 (s).\n",
      "NOTE:  Initializing each layer cost       0.01 (s).\n",
      "NOTE:  Batch nUsed Learning Rate        Loss    C-Index    Correct  Incorrect       Tied   Time(s) (Validation)\n",
      "NOTE:      0  4500        0           0.4849     0.9283  1.917e+06   1.48e+05          0     0.00\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"cas-results-key\"><b>&#167; ScoreInfo</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Descr\">Descr</th>\n",
       "      <th title=\"Value\">Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Number of Observations Read</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Number of Observations Used</td>\n",
       "      <td>4500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Loss Error</td>\n",
       "      <td>0.484949</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; CumHazard</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"Time\">Time</th>\n",
       "      <th title=\"Cumulative hazard\">CumHaz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.019206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.073149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.210213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.308992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-results-key\"><hr/><b>&#167; OutputCasTables</b></div>\n",
       "<div class=\"cas-results-body\">\n",
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"CAS Library\">casLib</th>\n",
       "      <th title=\"Name\">Name</th>\n",
       "      <th title=\"Number of Rows\">Rows</th>\n",
       "      <th title=\"Number of Columns\">Columns</th>\n",
       "      <th title=\"Table\">casTable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CASUSER(guilin)</td>\n",
       "      <td>Valid_Res_OJxyPm</td>\n",
       "      <td>4500</td>\n",
       "      <td>13</td>\n",
       "      <td>CASTable('Valid_Res_OJxyPm', caslib='CASUSER(g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "</div>\n",
       "<div class=\"cas-output-area\"></div>\n",
       "<p class=\"cas-results-performance\"><small><span class=\"cas-elapsed\">elapsed 0.0363s</span> &#183; <span class=\"cas-user\">user 0.0579s</span> &#183; <span class=\"cas-sys\">sys 0.0575s</span> &#183; <span class=\"cas-memory\">mem 156MB</span></small></p>"
      ],
      "text/plain": [
       "[ScoreInfo]\n",
       "\n",
       "                          Descr         Value\n",
       " 0  Number of Observations Read          4500\n",
       " 1  Number of Observations Used          4500\n",
       " 2                   Loss Error      0.484949\n",
       "\n",
       "[CumHazard]\n",
       "\n",
       "    Time    CumHaz\n",
       " 0   2.0  0.000541\n",
       " 1   3.0  0.019206\n",
       " 2   4.0  0.073149\n",
       " 3   5.0  0.210213\n",
       " 4   6.0  0.308992\n",
       "\n",
       "[OutputCasTables]\n",
       "\n",
       "             casLib              Name  Rows  Columns  \\\n",
       " 0  CASUSER(guilin)  Valid_Res_OJxyPm  4500       13   \n",
       " \n",
       "                                             casTable  \n",
       " 0  CASTable('Valid_Res_OJxyPm', caslib='CASUSER(g...  \n",
       "\n",
       "+ Elapsed: 0.0363s, user: 0.0579s, sys: 0.0575s, mem: 156mb"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# specify log_level=4 to compute C-Index for deep survival model\n",
    "deepSurvModel.predict(testTbl,buffer_size=10000,log_level=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Comparison of model predictive performance on test data in terms of C-Index</center>\n",
    "| Model | C-Index |\n",
    "| --- | --- | \n",
    "| Cox Model | 0.86 |\n",
    "| Deep Survival Model | 0.93 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Predict the employees who are likely quit and when they will quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ScoreInfo]\n",
      "\n",
      "                          Descr         Value\n",
      " 0  Number of Observations Read         10499\n",
      " 1  Number of Observations Used         10499\n",
      " 2                   Loss Error       0.15264\n",
      "\n",
      "[CumHazard]\n",
      "\n",
      "    Time    CumHaz\n",
      " 0   2.0  0.000605\n",
      " 1   3.0  0.019045\n",
      " 2   4.0  0.078120\n",
      " 3   5.0  0.216710\n",
      " 4   6.0  0.317541\n",
      "\n",
      "[OutputCasTables]\n",
      "\n",
      "             casLib              Name   Rows  Columns  \\\n",
      " 0  CASUSER(guilin)  Valid_Res_OJxyPm  10499       12   \n",
      " \n",
      "                                             casTable  \n",
      " 0  CASTable('Valid_Res_OJxyPm', caslib='CASUSER(g...  \n",
      "\n",
      "+ Elapsed: 0.0434s, user: 0.106s, sys: 0.059s, mem: 156mb\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><caption>Selected Rows from Table VALID_RES_OJXYPM</caption>\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"satisfaction_level\">satisfaction_level</th>\n",
       "      <th title=\"last_evaluation\">last_evaluation</th>\n",
       "      <th title=\"number_projects\">number_projects</th>\n",
       "      <th title=\"average_montly_hours\">average_montly_hours</th>\n",
       "      <th title=\"time_spend_company\">time_spend_company</th>\n",
       "      <th title=\"work_accident\">work_accident</th>\n",
       "      <th title=\"left\">left</th>\n",
       "      <th title=\"promotion_last_5years\">promotion_last_5years</th>\n",
       "      <th title=\"department\">department</th>\n",
       "      <th title=\"salary\">salary</th>\n",
       "      <th title=\"y\">y</th>\n",
       "      <th title=\"id\">id</th>\n",
       "      <th title=\"_DL_Pred_\">_DL_Pred_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.74</td>\n",
       "      <td>6.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>management</td>\n",
       "      <td>high</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.131263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.83</td>\n",
       "      <td>6.0</td>\n",
       "      <td>282.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technical</td>\n",
       "      <td>medium</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.894433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.46</td>\n",
       "      <td>2.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>57.076889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>34.535267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.87</td>\n",
       "      <td>2.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>IT</td>\n",
       "      <td>low</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.733552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Selected Rows from Table VALID_RES_OJXYPM\n",
       "\n",
       "   satisfaction_level  last_evaluation  number_projects  average_montly_hours  \\\n",
       "0                0.15             0.74              6.0                 144.0   \n",
       "1                0.09             0.83              6.0                 282.0   \n",
       "2                0.45             0.46              2.0                 130.0   \n",
       "3                0.46             0.55              2.0                 145.0   \n",
       "4                0.45             0.87              2.0                 268.0   \n",
       "\n",
       "   time_spend_company  work_accident  left  promotion_last_5years  department  \\\n",
       "0                 7.0            0.0   0.0                    1.0  management   \n",
       "1                 4.0            0.0   1.0                    0.0   technical   \n",
       "2                 3.0            0.0   1.0                    0.0   technical   \n",
       "3                 3.0            0.0   1.0                    0.0   technical   \n",
       "4                 4.0            1.0   0.0                    0.0          IT   \n",
       "\n",
       "   salary    y   id  _DL_Pred_  \n",
       "0    high -7.0  0.0   0.131263  \n",
       "1  medium  4.0  1.0   7.894433  \n",
       "2     low  3.0  2.0  57.076889  \n",
       "3     low  3.0  3.0  34.535267  \n",
       "4     low -4.0  4.0   0.733552  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: Obtain cumulative hazard estimates\n",
    "surv_model = deepSurvModel\n",
    "score_trainTbl = surv_model.predict(trainTbl)\n",
    "print(score_trainTbl)\n",
    "# step 2: predict hazard rate\n",
    "surv_model.predict(testTbl)\n",
    "predicted_hazard  = surv_model.valid_res_tbl\n",
    "predicted_hazard.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot predicted hazard scores on test data set\n",
    "- Employees can be classified into two groups in terms of predicted risk scores: \n",
    "- High risk group with score>10 and low risk group with score<10. \n",
    "- Employees in the high risk group are more likely quit than those in the low risk group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3df5QcZZ3v8ffHBAISJOGHc0OSJVEiyg9FnAtR3N0JICToMbhHzgZzISBu8Cwo3stRgq6LAjkXzyWgsAhGkyVoYMwimFxuFLOBloPH8COKCSEgI0QYEhIgITL8WoPf+0c9LcXQPd3zI90zqc/rnD5T9dTzVH3rmepvVz9d3aWIwMzMiuFtzQ7AzMwax0nfzKxAnPTNzArESd/MrECc9M3MCsRJ38ysQJz0hwBJ6yS1NTuOnU3SZZKek/RMk7Yfkg5O09dL+nof19Ml6V0DHFubpM6BXOdAkDQh9dvwZsdi9XHSbzJJGySd0K3sTEn3lOcj4rCIKNVYz5B+8kkaD1wAHBoR/63Z8UTE5yPi0lr1JJUkfa5b25ER8fjOi86s75z0DQBlmnk8HAQ8HxFbBmJlkoYNxHrsDYPthGIQHLNDkjtsCMi/G5B0tKQHJP1J0mZJV6Zqd6e/L6ThhQ9LGiZpXhoyeULSefl3A+ksda6kXwEvA++SdJak9ZJelPS4pHNycbRJ6pT0FUlbJG2SdIqkkyX9XtJWSV/tYT/2kXSjpGcl/VHSv0h6W9q3FcCBKfYbKrQtb/uraX82SJqZW36DpOskLZf0EjBF0ghJV0h6MvXV9ZL2zLX5ctqHjZI+2217N0i6LDc/XdKDqd//IGmqpLnA3wL/luL+t1Q3JB0sabKkZ/IvQJI+JWlNmn6bpDlpfc9LWiJp3xrHwgW5vj8rV/5xSb9N8T0l6Ru5ZeX4yo8d5eW57b8o6WFJn8q1O1PSryRdJWkr8I10TF2R/gePAx+vEe+Fkp5O639U0vGpfFj6X5a3vVrZuz0kfUTS/ZK2p78fya2v0jG7j6QFqU+eVjZMOCzVP1jSL9O6npP0457iLYSI8KOJD2ADcEK3sjOBeyrVAX4NnJ6mRwKT0/QEIIDhuXafBx4GxgGjgf/M1wFKwJPAYcBwYDeyJ/G7AQF/T/bEOirVbwN2AP+a6v4T8CxwE7B3Ws+rwLuq7OuNwNJUdwLwe+Ds3Lo7e+in8ravBEak2F4CDknLbwC2A8eSnczsAXwbWAbsm7b5f4H/nepPBTYDhwN7pX0I4ODc+i5L00endX8srXss8N5cH36uW6z59fwB+Fhu2X8Ac9L0l4BV6f8zAvgecHON/b8k9f3J6X8zOrf8iBTf+9O+nVJhPUem/9kH0/ypwIGp3T+mPh2TOw53AF9Ix8eeZMfUI8D41K930e24y23rEOAp4MDcMfruNP1lYG2qI+ADwH5pnduA09M2T0vz+/VwzP409d1ewDuB+4BzUv2bga/ljomPNvs53+xH0wMo+oMsoXcBL+QeL1M96d8NfBPYv9t6JnR/8gF3lg/+NH8Cb036l9SI76fA+Wm6DXgFGJbm907rOyZXf3WVZDMMeI1szL5cdg5Qyq27nqS/V65sCfD1NH0DcGNumcgS2LtzZR8GnkjTC4HLc8veQ/Wk/z3gqipxleg56V8GLMz110vAQWl+PXB8rt0Y4M9UTqDlvs//f7eQXvQr1P9295iBA9KxNKOHfn4QmJ6mzwSe7Lb8TuDzufkTux93uWUHpxhPAHbrtuzR8na6lZ8O3Net7NfAmZWOWaAlHVd75spOA+5K0zcC84FxA/m8HcoPD+8MDqdExKjyA/jnHuqeTZagHklvfT/RQ90Dyc60yp6qUOdNZZKmSVqVhmpeIDuj3D9X5fmIeD1Nv5L+bs4tf4XsHUh3+wO7A3/Mlf2R7Ky5Xtsi4qVu7Q/Mzef35QDg7cBqSS+kffl5Koe39k0+ru7Gk52x98VNwD9IGgH8A/CbiChv6yDgtlx864HXyRJZJc9HxI7c/MukvpZ0jKS70tDZdrIz8r/+3yTtBtwC3BQR7bnyM9KwVTmGw3nz/7v7MVN3v0VEB9m7mW8AWyS1Syr/v6r16YEV1tn9OMlv/yCys/1NuX34HtkZP8BXyE4A7lN2FdybhvGKyEl/iImIxyLiNLKD+lvALZL2Ijvb6m4T2dBB2fhKqyxPpMT0E+AKoCW9AC0ne9L013NkZ7EH5cr+Bni6F+sYnfY1335jbj7fB8+RvQAdlntB3Sciyi9Im3hzf/xND9t9imzIq5Ief6Y2Ih4mS1rTgM+QvQjk1zst/4IfEXtERG/6pOwmsqGs8RGxD3A9b/6/XQO8CPxLuUDSQcD3gfPIhk9GAQ91a9d9/3rTb0TETRHxUbL/e5Ads1C9Tzfy5mOkvI18n+RjeorsTH//XB++IyIOS9t/JiL+KSIOJHtn+V2ly3KLykl/iJH0PyQdEBF/IRsKguzs8FngL0D++vAlwPmSxkoaBVxYY/W7k40tPwvskDSN7O17v6V3B0uAuZL2TgnnfwE/6uWqvilpd0l/C3yCbIy80vb+QpbQrpL0ToDUDyelKkuAMyUdKuntwMU9bHMBcJak49OHr2MlvTct28yb+7ySm4AvAn/XLd7ryfrjoBTfAZKm11hXNXsDWyPiVUlHk73AkNZ7DtlnIJ9J/VJWPll4NtU7i+xMvydLgC9KGidpNDCnWkVJh0g6Lp1MvEr2Ilx+l/gD4FJJk5R5v6T9yE4y3iPpM5KGS/pH4FDg9krbiIhNwC+AeZLekf4/75b09ymGUyWVT3y2pf19vdK6isJJf+iZCqyT1AV8h2x89tWIeBmYC/wqvc2dTJb0fgGsAX5L9oTaQZWDPiJeJEtOS8ieIJ8hO3scKF8gG9N+HLiHLBku7EX7Z1JcG4HFZGPLj/RQ/0KgA1gl6U9kH2QfAhARPyMb974z1bmz2koi4j7gLOAqsg90f8kbZ6PfAT4taZukq6us4mayMfk7I+K5XPl3yPr3F5JeJPtQ95ge9qcn/wxcktbzr2T/w7LTyF6YNuqNK3i+mt6FzCMbM99M9kHwr2ps5/vAHcDvgN8At/ZQdwRwOdm7rmfI3p2Wr+66MsX4C+BPZC+se0bE82Qv5hcAz5MNz3yiW791dwbZCcvDZMfHLWSfjwD8d+De9HxZRvb51BM19nGXpvRhhxVAOnO/PiK6v30e9JR9I/lHETGuVl0zq85n+rswSXsqu4Z+uKSxZEMYtzU7LjNrHif9XZvILu/cRja8s57srb+ZFZSHd8zMCsRn+mZmBTKofkCpu/333z8mTJjQ5/YvvfQSe+21V+2Kg8xQjRsce7M49sYbzHGvXr36uYg4oNKyQZ30J0yYwAMPPNDn9qVSiba2toELqEGGatzg2JvFsTfeYI5bUtVvSnt4x8ysQJz0zcwKxEnfzKxAnPTNzArESd/MrECc9M3MCqTupJ/uaflbSben+YmS7pX0mKQfS9o9lY9I8x1p+YTcOi5K5Y/mfuLWzMwapDdn+ueT/XZL2bfIbsc2iey3Xc5O5WeT3eHoYLKfov0WgKRDgRlk97acSnYzg2GYmVnD1JX0000IPk524wMkCTiO7HerARYBp6Tp6WmetPz4VH860B4Rr6Xfs+4gu+G0mZk1SL3fyP022c0M9k7z+wEv5O7X2ckb97AcS7qHZUTsSPfr3C+Vr8qtM9/mryTNBmYDtLS0UCqV6t2Xt9iydTvXLF7a5/Z9dcTYffrVvqurq1/73UyOvTkce+MN1bhrJv104+0tEbE63cgCKt8zNWos66nNGwUR88nuXk9ra2v052vO1yxeyry1jf+liQ0z2/rVfjB/vbsWx94cjr3xhmrc9WTEY4FPSjoZ2AN4B9mZ/yhJw9PZ/jjeuEF1J9mNkzslDQf2AbbmysvybczMrAFqjulHxEURMS4iJpB9EHtnRMwE7gI+narNAsrjKMvSPGn5nZH9aP8yYEa6umciMAm4b8D2xMzMaurP2MeFQLuky8juyrQglS8Afiipg+wMfwZARKyTtITs5sU7gHMjotB3pTcza7ReJf2IKAGlNP04Fa6+iYhXgVOrtJ8LzO1tkGZmNjD8jVwzswJx0jczKxAnfTOzAnHSNzMrECd9M7MCcdI3MysQJ30zswJx0jczKxAnfTOzAnHSNzMrECd9M7MCcdI3MysQJ30zswJx0jczKxAnfTOzAnHSNzMrECd9M7MCqZn0Je0h6T5Jv5O0TtI3U/kNkp6Q9GB6HJnKJelqSR2S1kg6KreuWZIeS49Z1bZpZmY7Rz23S3wNOC4iuiTtBtwj6Wdp2Zcj4pZu9aeR3fR8EnAMcB1wjKR9gYuBViCA1ZKWRcS2gdgRMzOrreaZfmS60uxu6RE9NJkO3JjarQJGSRoDnASsiIitKdGvAKb2L3wzM+sNRfSUv1MlaRiwGjgYuDYiLpR0A/BhsncCK4E5EfGapNuByyPintR2JXAh0AbsERGXpfKvA69ExBXdtjUbmA3Q0tLyofb29j7v3Jat29n8Sp+b99kRY/fpV/uuri5Gjhw5QNE0lmNvDsfeeIM57ilTpqyOiNZKy+oZ3iEiXgeOlDQKuE3S4cBFwDPA7sB8ssR+CaBKq+ihvPu25qf10draGm1tbfWEWNE1i5cyb21duzigNsxs61f7UqlEf/a7mRx7czj2xhuqcffq6p2IeAEoAVMjYlMawnkN+Hfg6FStExifazYO2NhDuZmZNUg9V+8ckM7wkbQncALwSBqnR5KAU4CHUpNlwBnpKp7JwPaI2ATcAZwoabSk0cCJqczMzBqknrGPMcCiNK7/NmBJRNwu6U5JB5AN2zwIfD7VXw6cDHQALwNnAUTEVkmXAvenepdExNaB2xUzM6ulZtKPiDXAByuUH1elfgDnVlm2EFjYyxjNzGyA+Bu5ZmYF4qRvZlYgTvpmZgXipG9mViBO+mZmBeKkb2ZWIE76ZmYF4qRvZlYgTvpmZgXipG9mViBO+mZmBeKkb2ZWIE76ZmYF4qRvZlYgTvpmZgXipG9mViBO+mZmBVLPPXL3kHSfpN9JWifpm6l8oqR7JT0m6ceSdk/lI9J8R1o+Ibeui1L5o5JO2lk7ZWZmldVzpv8acFxEfAA4Epiabnj+LeCqiJgEbAPOTvXPBrZFxMHAVakekg4FZgCHAVOB76b77pqZWYPUTPqR6Uqzu6VHAMcBt6TyRcApaXp6mictP16SUnl7RLwWEU+Q3Tj96AHZCzMzq0vNG6MDpDPy1cDBwLXAH4AXImJHqtIJjE3TY4GnACJih6TtwH6pfFVutfk2+W3NBmYDtLS0UCqVerdHOS17wgVH7KhdcYD1J2aArq6ufq+jWRx7czj2xhuqcdeV9CPideBISaOA24D3VaqW/qrKsmrl3bc1H5gP0NraGm1tbfWEWNE1i5cyb21duzigNsxs61f7UqlEf/a7mRx7czj2xhuqcffq6p2IeAEoAZOBUZLKGXUcsDFNdwLjAdLyfYCt+fIKbczMrAHquXrngHSGj6Q9gROA9cBdwKdTtVnA0jS9LM2Tlt8ZEZHKZ6SreyYCk4D7BmpHzMystnrGPsYAi9K4/tuAJRFxu6SHgXZJlwG/BRak+guAH0rqIDvDnwEQEeskLQEeBnYA56ZhIzMza5CaST8i1gAfrFD+OBWuvomIV4FTq6xrLjC392GamdlA8DdyzcwKxEnfzKxAnPTNzArESd/MrECc9M3MCsRJ38ysQJz0zcwKxEnfzKxAnPTNzArESd/MrECc9M3MCsRJ38ysQJz0zcwKxEnfzKxAnPTNzArESd/MrEDquV3ieEl3SVovaZ2k81P5NyQ9LenB9Dg51+YiSR2SHpV0Uq58airrkDRn5+ySmZlVU8/tEncAF0TEbyTtDayWtCItuyoirshXlnQo2S0SDwMOBP5T0nvS4muBj5HdJP1+Scsi4uGB2BEzM6utntslbgI2pekXJa0HxvbQZDrQHhGvAU+ke+WWb6vYkW6ziKT2VNdJ38ysQXo1pi9pAtn9cu9NRedJWiNpoaTRqWws8FSuWWcqq1ZuZmYNooior6I0EvglMDcibpXUAjwHBHApMCYiPivpWuDXEfGj1G4BsJzsBeakiPhcKj8dODoivtBtO7OB2QAtLS0fam9v7/PObdm6nc2v9Ll5nx0xdp9+te/q6mLkyJEDFE1jOfbmcOyNN5jjnjJlyuqIaK20rJ4xfSTtBvwEWBwRtwJExObc8u8Dt6fZTmB8rvk4YGOarlb+VxExH5gP0NraGm1tbfWEWNE1i5cyb21duzigNsxs61f7UqlEf/a7mRx7czj2xhuqcddz9Y6ABcD6iLgyVz4mV+1TwENpehkwQ9IISROBScB9wP3AJEkTJe1O9mHvsoHZDTMzq0c9p8HHAqcDayU9mMq+Cpwm6Uiy4Z0NwDkAEbFO0hKyD2h3AOdGxOsAks4D7gCGAQsjYt0A7ouZmdVQz9U79wCqsGh5D23mAnMrlC/vqZ2Zme1c/kaumVmBOOmbmRWIk76ZWYE46ZuZFYiTvplZgTjpm5kViJO+mVmBOOmbmRWIk76ZWYE46ZuZFYiTvplZgTjpm5kViJO+mVmBOOmbmRWIk76ZWYE46ZuZFYiTvplZgdRzj9zxku6StF7SOknnp/J9Ja2Q9Fj6OzqVS9LVkjokrZF0VG5ds1L9xyTN2nm7ZWZmldRzpr8DuCAi3gdMBs6VdCgwB1gZEZOAlWkeYBrZzdAnAbOB6yB7kQAuBo4BjgYuLr9QmJlZY9RM+hGxKSJ+k6ZfBNYDY4HpwKJUbRFwSpqeDtwYmVXAKEljgJOAFRGxNSK2ASuAqQO6N2Zm1iNFRP2VpQnA3cDhwJMRMSq3bFtEjJZ0O3B5uqE6klYCFwJtwB4RcVkq/zrwSkRc0W0bs8neIdDS0vKh9vb2Pu/clq3b2fxKn5v32RFj9+lX+66uLkaOHDlA0TSWY28Ox954gznuKVOmrI6I1krLhte7EkkjgZ8AX4qIP0mqWrVCWfRQ/uaCiPnAfIDW1tZoa2urN8S3uGbxUuatrXsXB8yGmW39al8qlejPfjeTY28Ox954QzXuuq7ekbQbWcJfHBG3puLNadiG9HdLKu8ExueajwM29lBuZmYNUs/VOwIWAOsj4srcomVA+QqcWcDSXPkZ6SqeycD2iNgE3AGcKGl0+gD3xFRmZmYNUs/Yx7HA6cBaSQ+msq8ClwNLJJ0NPAmcmpYtB04GOoCXgbMAImKrpEuB+1O9SyJi64DshZmZ1aVm0k8fyFYbwD++Qv0Azq2yroXAwt4EaGZmA8ffyDUzKxAnfTOzAnHSNzMrECd9M7MCcdI3MysQJ30zswJx0jczKxAnfTOzAnHSNzMrECd9M7MCcdI3MysQJ30zswJx0jczKxAnfTOzAnHSNzMrECd9M7MCcdI3MyuQeu6Ru1DSFkkP5cq+IelpSQ+mx8m5ZRdJ6pD0qKSTcuVTU1mHpDkDvytmZlZLPWf6NwBTK5RfFRFHpsdyAEmHAjOAw1Kb70oaJmkYcC0wDTgUOC3VNTOzBqrnHrl3S5pQ5/qmA+0R8RrwhKQO4Oi0rCMiHgeQ1J7qPtzriM3MrM9qJv0enCfpDOAB4IKI2AaMBVbl6nSmMoCnupUfU2mlkmYDswFaWloolUp9DrBlT7jgiB19bt9X/YkZoKurq9/raBbH3hyOvfGGatx9TfrXAZcCkf7OAz4LqELdoPIwUlRacUTMB+YDtLa2RltbWx9DhGsWL2Xe2v68rvXNhplt/WpfKpXoz343k2NvDsfeeEM17j5lxIjYXJ6W9H3g9jTbCYzPVR0HbEzT1crNzKxB+nTJpqQxudlPAeUre5YBMySNkDQRmATcB9wPTJI0UdLuZB/2Lut72GZm1hc1z/Ql3Qy0AftL6gQuBtokHUk2RLMBOAcgItZJWkL2Ae0O4NyIeD2t5zzgDmAYsDAi1g343piZWY/quXrntArFC3qoPxeYW6F8ObC8V9GZmdmA8jdyzcwKxEnfzKxAnPTNzArESd/MrECc9M3MCsRJ38ysQJz0zcwKxEnfzKxAnPTNzArESd/MrECc9M3MCsRJ38ysQJz0zcwKxEnfzKxAnPTNzArESd/MrECc9M3MCqRm0pe0UNIWSQ/lyvaVtELSY+nv6FQuSVdL6pC0RtJRuTazUv3HJM3aObtjZmY9qedM/wZgareyOcDKiJgErEzzANPIboY+CZgNXAfZiwTZvXWPAY4GLi6/UJiZWePUTPoRcTewtVvxdGBRml4EnJIrvzEyq4BRksYAJwErImJrRGwDVvDWFxIzM9vJat4YvYqWiNgEEBGbJL0zlY8FnsrV60xl1crfQtJssncJtLS0UCqV+hgitOwJFxyxo8/t+6o/MQN0dXX1ex3N4tibw7E33lCNu69JvxpVKIseyt9aGDEfmA/Q2toabW1tfQ7mmsVLmbd2oHextg0z2/rVvlQq0Z/9bibH3hyOvfGGatx9vXpncxq2If3dkso7gfG5euOAjT2Um5lZA/U16S8DylfgzAKW5srPSFfxTAa2p2GgO4ATJY1OH+CemMrMzKyBao59SLoZaAP2l9RJdhXO5cASSWcDTwKnpurLgZOBDuBl4CyAiNgq6VLg/lTvkojo/uGwmZntZDWTfkScVmXR8RXqBnBulfUsBBb2KjozMxtQ/kaumVmBOOmbmRWIk76ZWYE46ZuZFYiTvplZgTjpm5kViJO+mVmBOOmbmRWIk76ZWYE46ZuZFYiTvplZgTjpm5kViJO+mVmBOOmbmRWIk76ZWYE46ZuZFYiTvplZgfQr6UvaIGmtpAclPZDK9pW0QtJj6e/oVC5JV0vqkLRG0lEDsQNmZla/gTjTnxIRR0ZEa5qfA6yMiEnAyjQPMA2YlB6zgesGYNtmZtYLO2N4ZzqwKE0vAk7Jld8YmVXAKEljdsL2zcysCmX3Mu9jY+kJYBsQwPciYr6kFyJiVK7OtogYLel24PKIuCeVrwQujIgHuq1zNtk7AVpaWj7U3t7e5/i2bN3O5lf63LzPjhi7T7/ad3V1MXLkyAGKprEce3M49sYbzHFPmTJldW705U2G93Pdx0bERknvBFZIeqSHuqpQ9pZXnIiYD8wHaG1tjba2tj4Hd83ipcxb299d7L0NM9v61b5UKtGf/W4mx94cjr3xhmrc/RreiYiN6e8W4DbgaGBzedgm/d2SqncC43PNxwEb+7N9MzPrnT4nfUl7Sdq7PA2cCDwELANmpWqzgKVpehlwRrqKZzKwPSI29TlyMzPrtf6MfbQAt0kqr+emiPi5pPuBJZLOBp4ETk31lwMnAx3Ay8BZ/di2mZn1QZ+TfkQ8DnygQvnzwPEVygM4t6/bMzOz/vM3cs3MCsRJ38ysQJz0zcwKxEnfzKxAnPTNzArESd/MrECc9M3MCsRJ38ysQJz0zcwKxEnfzKxAnPTNzArESd/MrECc9M3MCqTxt5WynWrCnP/XlO1uuPzjTdmumfWOk/5O0N/Ee8EROzizScm7r8r73OjY/WJj1jse3jEzKxCf6duQNpDDWb19l+J3GTYUNfxMX9JUSY9K6pA0p9HbNzMrsoae6UsaBlwLfAzoBO6XtCwiHm5kHGYDwR+a21DU6OGdo4GOdH9dJLUD0wEnfbM6VXqxadQH6EV8wan24r6z+3xn9bWy+5U3hqRPA1Mj4nNp/nTgmIg4L1dnNjA7zR4CPNqPTe4PPNeP9s0yVOMGx94sjr3xBnPcB0XEAZUWNPpMXxXK3vSqExHzgfkDsjHpgYhoHYh1NdJQjRsce7M49sYbqnE3+oPcTmB8bn4csLHBMZiZFVajk/79wCRJEyXtDswAljU4BjOzwmro8E5E7JB0HnAHMAxYGBHrduImB2SYqAmGatzg2JvFsTfekIy7oR/kmplZc/lnGMzMCsRJ38ysQHbJpD+UfupB0nhJd0laL2mdpPNT+b6SVkh6LP0d3exYK5E0TNJvJd2e5idKujfF/eP0gf2gI2mUpFskPZL6/sNDqM//ZzpWHpJ0s6Q9Bmu/S1ooaYukh3JlFftZmavT83aNpKOaF3nV2P9POmbWSLpN0qjcsotS7I9KOqk5Ude2yyX93E89TAMOBU6TdGhzo+rRDuCCiHgfMBk4N8U7B1gZEZOAlWl+MDofWJ+b/xZwVYp7G3B2U6Kq7TvAzyPivcAHyPZh0Pe5pLHAF4HWiDic7IKIGQzefr8BmNqtrFo/TwMmpcds4LoGxVjNDbw19hXA4RHxfuD3wEUA6Tk7AzgstfluykWDzi6X9Mn91ENE/BdQ/qmHQSkiNkXEb9L0i2TJZyxZzItStUXAKc2JsDpJ44CPAz9I8wKOA25JVQZr3O8A/g5YABAR/xURLzAE+jwZDuwpaTjwdmATg7TfI+JuYGu34mr9PB24MTKrgFGSxjQm0reqFHtE/CIidqTZVWTfNYIs9vaIeC0ingA6yHLRoLMrJv2xwFO5+c5UNuhJmgB8ELgXaImITZC9MADvbF5kVX0b+ArwlzS/H/BC7kkxWPv+XcCzwL+noakfSNqLIdDnEfE0cAXwJFmy3w6sZmj0e1m1fh5qz93PAj9L00Mm9l0x6df8qYfBSNJI4CfAlyLiT82OpxZJnwC2RMTqfHGFqoOx74cDRwHXRcQHgZcYhEM5laTx7+nAROBAYC+yYZHuBmO/1zJUjh8kfY1saHZxuahCtUEZ+66Y9IfcTz1I2o0s4S+OiFtT8ebyW9v0d0uz4qviWOCTkjaQDaEdR3bmPyoNO8Dg7ftOoDMi7k3zt5C9CAz2Pgc4AXgiIp6NiD8DtwIfYWj0e1m1fh4Sz11Js4BPADPjjS86DYnYYddM+kPqpx7SOPgCYH1EXJlbtAyYlaZnAUsbHVtPIuKiiBgXERPI+vjOiJgJ3AV8OlUbdHEDRMQzwFOSDklFx5P9vPeg7vPkSWCypLenY6cc+6Dv95xq/bwMOCNdxTMZ2F4eBhosJE0FLgQ+GREv5xYtA2ZIGiFpItmH0fc1I8aaImKXewAnk32y/gfga82Op0asHyV7G7gGeDA9TiYbH18JPJb+7tvsWHvYhzbg9jT9LrKDvQP4D2BEsyuOOUAAAACCSURBVOOrEvORwAOp338KjB4qfQ58E3gEeAj4ITBisPY7cDPZZw9/JjsbPrtaP5MNkVybnrdrya5QGmyxd5CN3Zefq9fn6n8txf4oMK3ZfV/t4Z9hMDMrkF1xeMfMzKpw0jczKxAnfTOzAnHSNzMrECd9M7MCcdI3MysQJ30zswL5/wDWR89hBcUZAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_hazard[\"_DL_Pred_\"].hist(bins=10)\n",
    "plt.title(\"Histgram of predictive hazard scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict survival function for given inputs\n",
    "- Step 1. We score with the training data set to obtain the baseline cumulative hazards, and get the hazard prediction for these individuals\n",
    "-  Step 2. By combining the baseline cumulative hazard function and the hazard prediction, we can obtain survival function for these individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dl_survival_function(df_baselineCumHazard, df_hazard, id=None):\n",
    "    # id: column for observation identity\n",
    "    # import pandas as pd\n",
    "    if not isinstance(df_baselineCumHazard, SASDataFrame) and not isinstance(df_hazard, pd.DataFrame):\n",
    "        #print(\"Error:input types must be data frame\")\n",
    "        raise TypeError(\"inputs must be data frame!\")\n",
    "\n",
    "    survival = pd.DataFrame()\n",
    "    survival[\"Time\"] = df_baselineCumHazard[\"Time\"]\n",
    "    s = np.outer(-1*df_baselineCumHazard[\"CumHaz\"],df_hazard[\"_DL_Pred_\"])\n",
    "    s = np.exp(s)\n",
    "    # create column names with prefex id\n",
    "    if isinstance(id, str):   \n",
    "        idColumns = [id+'='+str(v) for v in df_hazard[id]]\n",
    "        s = pd.DataFrame(s,columns=idColumns)\n",
    "    else:\n",
    "        s = pd.DataFrame(s)             \n",
    "    survival = pd.concat([survival, s], axis=1)\n",
    "    return survival          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For illustrative purpose, we compute the predicted survival function of two  employees: one is chosen from the high risk group and the other is chosen from the low risk group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An employee's id from the low risk group in the test data set: 6\n",
      "An employee's id from the high risk group in the test data set: 3\n"
     ]
    }
   ],
   "source": [
    "# step 1:\n",
    "df_hazard = predicted_hazard.to_frame()\n",
    "df_baselineCum_hazard= pd.DataFrame(score_trainTbl[\"CumHazard\"])\n",
    "\n",
    "# step 2:\n",
    "dl_survival = dl_survival_function(df_baselineCum_hazard, df_hazard, id=None)\n",
    "# id for risk groups based on deep learning model\n",
    "# \n",
    "risk_cutoff = 10\n",
    "riskGroup = {\"low risk\":(df_hazard[df_hazard._DL_Pred_<risk_cutoff].id).reset_index(drop=True),\n",
    "             \"high risk\": df_hazard[df_hazard._DL_Pred_>risk_cutoff].id.reset_index(drop=True)}\n",
    "low = riskGroup[\"low risk\"]\n",
    "df_low = (df_hazard.loc[df_hazard[\"id\"].isin(low)])\n",
    "# employees who left the company\n",
    "df_low = df_low[df_low.left==1]\n",
    "\n",
    "high = riskGroup[\"high risk\"]\n",
    "df_high = (df_hazard.loc[df_hazard[\"id\"].isin(high)])\n",
    "df_high = df_high[df_high.left==1] ## employees who left the company\n",
    "id = [df_low.id.iloc[1],df_high.id.iloc[1]] ##\n",
    "print(\"An employee's id from the low risk group in the test data set: {}\".format(int(id[0])))\n",
    "print(\"An employee's id from the high risk group in the test data set: {}\".format(int(id[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot and compare predicted survival functions between individuals from different risk groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_survival(survival, hazard_df, modelName=None, id=None):\n",
    "    \n",
    "    colors=['red','blue']\n",
    "    if id is None:\n",
    "        id=[0,1]  \n",
    "        \n",
    "    eventTimes = list(df_hazard[hazard_df.id==id[0]].time_spend_company) +  list(hazard_df[hazard_df.id==id[1]].time_spend_company)## series\n",
    "\n",
    "    print(\"Time spent in company for two employees:{} and {}\".format(int(eventTimes[0]),int(eventTimes[1])))\n",
    "    predHazard = list(hazard_df[hazard_df.id==id[0]]._DL_Pred_) +  list(hazard_df[hazard_df.id==id[1]]._DL_Pred_)## series\n",
    "    \n",
    "    print(\"Predicted hazards for two employees:\",predHazard)    \n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    plt.plot(survival.Time, survival[id[0]],color=colors[0],label='low risk')\n",
    "    plt.plot(survival.Time, survival[id[1]],color=colors[1],label='high risk')\n",
    "    # Actual time\n",
    "    plt.axvline(x=eventTimes[0],  color=colors[0], ls ='--')\n",
    "    plt.axvline(x=eventTimes[1],  color=colors[1], ls ='--')\n",
    "    i=0\n",
    "    ax.annotate('T={:.1f}'.format(eventTimes[0]), xy=(eventTimes[0], 0.5*(1.+0.2*i)),\n",
    "            xytext=(eventTimes[0], 0.5*(1.+0.2*i)), fontsize=12)\n",
    "    i=1\n",
    "    ax.annotate('T={:.1f}'.format(eventTimes[1]), xy=(eventTimes[1], 0.5*(1.+0.2*i)),\n",
    "            xytext=(eventTimes[1], 0.5*(1.+0.2*i)), fontsize=12)\n",
    "\n",
    "    title = \"Comparing survival functions from {} groups\".format(\"low risk and high risk\")\n",
    "    if isinstance(modelName, str):\n",
    "        title = modelName + \": \" + title\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.title(title, fontsize=15)\n",
    "    plt.ylabel(\"Survival probability\")\n",
    "    plt.xlabel(\"Time spent in company\")    \n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent in company for two employees:5 and 3\n",
      "Predicted hazards for two employees: [1.276875615119934, 34.5352668762207]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_survival(dl_survival, df_hazard, modelName=\"Deep Learning Model\",id=id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As we expect, the predicted survival curve for the employee with low risk is above the one with hight risk, since the employee from the high risk group has higher hazard rate than the employee from teh low risk group.\n",
    "- From the survial function of the employee from the high risk group, we can tell that he spent at this company more than 3 years with the probability less than .5. From the test data set, we can tell that the employee did leave the company at the 3rd year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Understand and identify risk factors that contribute to employee's attrition\n",
    "- Find how an employee's satisfaction_level impacts his/her time spent at the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: Cloud Analytic Services made the uploaded file available as table INPUT in caslib CASUSER(guilin).\n",
      "NOTE: The table INPUT has been created in caslib CASUSER(guilin) from binary data uploaded to Cloud Analytic Services.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CASTable('INPUT', caslib='CASUSER(guilin)')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare input data to predict survival probability curves\n",
    "employee1 =testTbl[testTbl.id==df_high.id.iloc[1]]\n",
    "input_df = employee1.to_frame()\n",
    "\n",
    "input_df[\"department\"]=[\"technical  \"] ## For varchar, RawLength need match\n",
    "input_df[\"salary\"]=[\"low   \"] ## For varchar, RawLength need match\n",
    "input_df = pd.concat([input_df]*3, ignore_index=True)\n",
    "input_df[\"satisfaction_level\"] = [0.3,0.5,0.7]\n",
    "input_df[\"id\"] = [i  for i in  range(3)]\n",
    "inputTbl = s.CASTable(\"input\",replace=True)\n",
    "CAS.upload_frame(s, input_df,  casout=inputTbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><caption>Selected Rows from Table TEST</caption>\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"satisfaction_level\">satisfaction_level</th>\n",
       "      <th title=\"last_evaluation\">last_evaluation</th>\n",
       "      <th title=\"number_projects\">number_projects</th>\n",
       "      <th title=\"average_montly_hours\">average_montly_hours</th>\n",
       "      <th title=\"time_spend_company\">time_spend_company</th>\n",
       "      <th title=\"work_accident\">work_accident</th>\n",
       "      <th title=\"left\">left</th>\n",
       "      <th title=\"promotion_last_5years\">promotion_last_5years</th>\n",
       "      <th title=\"department\">department</th>\n",
       "      <th title=\"salary\">salary</th>\n",
       "      <th title=\"y\">y</th>\n",
       "      <th title=\"id\">id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Selected Rows from Table TEST\n",
       "\n",
       "   satisfaction_level  last_evaluation  number_projects  average_montly_hours  \\\n",
       "0                0.46             0.55              2.0                 145.0   \n",
       "\n",
       "   time_spend_company  work_accident  left  promotion_last_5years department  \\\n",
       "0                 3.0            0.0   1.0                    0.0  technical   \n",
       "\n",
       "  salary    y   id  \n",
       "0    low  3.0  3.0  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "employee1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment code below to verify the input data\n",
    "# inputTbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ScoreInfo]\n",
      "\n",
      "                          Descr         Value\n",
      " 0  Number of Observations Read         10499\n",
      " 1  Number of Observations Used         10499\n",
      " 2                   Loss Error       0.15264\n",
      "\n",
      "[CumHazard]\n",
      "\n",
      "    Time    CumHaz\n",
      " 0   2.0  0.000605\n",
      " 1   3.0  0.019045\n",
      " 2   4.0  0.078120\n",
      " 3   5.0  0.216710\n",
      " 4   6.0  0.317541\n",
      "\n",
      "[OutputCasTables]\n",
      "\n",
      "             casLib              Name   Rows  Columns  \\\n",
      " 0  CASUSER(guilin)  Valid_Res_OJxyPm  10499       12   \n",
      " \n",
      "                                             casTable  \n",
      " 0  CASTable('Valid_Res_OJxyPm', caslib='CASUSER(g...  \n",
      "\n",
      "+ Elapsed: 0.046s, user: 0.131s, sys: 0.0485s, mem: 156mb\n",
      "NOTE: Due to data distribution, miniBatchSize has been limited to 1.\n",
      "\n",
      " Predicted hazard scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\"><caption>Selected Rows from Table VALID_RES_OJXYPM</caption>\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th title=\"\"></th>\n",
       "      <th title=\"satisfaction_level\">satisfaction_level</th>\n",
       "      <th title=\"last_evaluation\">last_evaluation</th>\n",
       "      <th title=\"number_projects\">number_projects</th>\n",
       "      <th title=\"average_montly_hours\">average_montly_hours</th>\n",
       "      <th title=\"time_spend_company\">time_spend_company</th>\n",
       "      <th title=\"work_accident\">work_accident</th>\n",
       "      <th title=\"left\">left</th>\n",
       "      <th title=\"promotion_last_5years\">promotion_last_5years</th>\n",
       "      <th title=\"department\">department</th>\n",
       "      <th title=\"salary\">salary</th>\n",
       "      <th title=\"y\">y</th>\n",
       "      <th title=\"id\">id</th>\n",
       "      <th title=\"_DL_Pred_\">_DL_Pred_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>58.874691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>30.223619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.55</td>\n",
       "      <td>2.0</td>\n",
       "      <td>145.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technical</td>\n",
       "      <td>low</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.510350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Selected Rows from Table VALID_RES_OJXYPM\n",
       "\n",
       "   satisfaction_level  last_evaluation  number_projects  average_montly_hours  \\\n",
       "0                 0.3             0.55              2.0                 145.0   \n",
       "1                 0.5             0.55              2.0                 145.0   \n",
       "2                 0.7             0.55              2.0                 145.0   \n",
       "\n",
       "   time_spend_company  work_accident  left  promotion_last_5years department  \\\n",
       "0                 3.0            0.0   1.0                    0.0  technical   \n",
       "1                 3.0            0.0   1.0                    0.0  technical   \n",
       "2                 3.0            0.0   1.0                    0.0  technical   \n",
       "\n",
       "  salary    y   id  _DL_Pred_  \n",
       "0    low  3.0  0.0  58.874691  \n",
       "1    low  3.0  1.0  30.223619  \n",
       "2    low  3.0  2.0  10.510350  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# step 1: Obtain cumulative hazard estimates\n",
    "surv_model = deepSurvModel\n",
    "#surv_model = CoxModel\n",
    "score_trainTbl = surv_model.predict(trainTbl)\n",
    "\n",
    "print(score_trainTbl)\n",
    "### step 2: predict hazard rate\n",
    "surv_model.predict(inputTbl)\n",
    "predicted_hazard  = surv_model.valid_res_tbl\n",
    "print(\"\\n Predicted hazard scores:\")\n",
    "predicted_hazard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazard_df = predicted_hazard.to_frame()\n",
    "baselineCumHazard_df= pd.DataFrame(score_trainTbl[\"CumHazard\"])\n",
    "dl_survival = dl_survival_function(baselineCumHazard_df,hazard_df, id=\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot predicted survival function on various satisfaction levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = dl_survival[list(dl_survival.columns)].plot(x=\"Time\", y=list(dl_survival.columns)[1])\n",
    "dl_survival[list(dl_survival.columns)].plot(x=\"Time\", y=dl_survival.columns[2],  ax=ax) ## suppress warning from plot\n",
    "dl_survival[list(dl_survival.columns)].plot(x=\"Time\", y=dl_survival.columns[3],  ax=ax)\n",
    "plt.gca().legend(('satisfaction level:0.3','satisfaction level:0.5','satisfaction level:0.7'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The survivial functions  for various satisfaction levels in the plot show that the employee's satisfaction level has positive relationship with the employee' tenure time in this company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Summary <a name=\"summary\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survival analysis provides a valid and powerful way to analyze survival data with censoring. In the employee attrition analysis, the deep survival model can automatically learn a complex and nonlinear function to predict an emloyee's turnover risk, and perform better than the popular Cox proportional hazards model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1 DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network\n",
    "- https://square.github.io/pysurvival/index.html PySurvival package for survival anlayis\n",
    "- https://github.com/square/pysurvival/blob/master/pysurvival/datasets/employee_attrition.csv Employee Attrition Data Set from PySurvival\n",
    "- https://square.github.io/pysurvival/tutorials/employee_retention.html Knowing when your employees will quit\n",
    "- http://medianetlab.ee.ucla.edu/papers/AAAI_2018_DeepHit DeepHit: A Deep Learning Approach to Survival Analysis with Competing Risks\n",
    "- https://www.jstor.org/stable/pdf/2985181.pdf Cox Proportional Hazards Model\n",
    "- https://support.sas.com/documentation/onlinedoc/stat/141/phreg.pdf The PHREG Procedure from SAS\n",
    "- https://rpubs.com/dmorgan26/telecomschurn  Telecoms Churn - Predicting Customer Turnover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook DLPy_DeepSurv_Employee_Churn_Analysis.ipynb to script\n",
      "[NbConvertApp] Writing 20148 bytes to DLPy_DeepSurv_Employee_Churn_Analysis.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script DLPy_DeepSurv_Employee_Churn_Analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the session\n",
    "s.endsession()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
