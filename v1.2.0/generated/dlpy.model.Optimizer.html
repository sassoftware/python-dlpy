

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>dlpy.model.Optimizer &mdash; DLPy 1.2.0 documentation</title>
  

  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
        <link rel="index" title="Index"
              href="../genindex.html"/>
        <link rel="search" title="Search" href="../search.html"/>
    <link rel="top" title="DLPy 1.2.0 documentation" href="../index.html"/>
        <link rel="up" title="API Reference" href="../api.html"/>
        <link rel="next" title="dlpy.lr_scheduler.FCMPLR" href="dlpy.lr_scheduler.FCMPLR.html"/>
        <link rel="prev" title="dlpy.model.VanillaSolver" href="dlpy.model.VanillaSolver.html"/> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> DLPy
          

          
          </a>

          
            
            
              <div class="version">
                1.2.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
                <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../whatsnew.html">What’s New</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting-started.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API Reference</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../api.html#imagetable">ImageTable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#audiotable">AudioTable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#timeseries">Timeseries</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#layers">Layers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#model">Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#solvers">Solvers</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../api.html#optimizer">Optimizer</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">dlpy.model.Optimizer</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#learning-rate-scheduler">Learning Rate Scheduler</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#metrics">Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#feature-maps">Feature Maps</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#sequential-model">Sequential Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#residual-networks">Residual Networks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#pre-built-models-for-computer-vision-tasks">Pre-Built Models for Computer Vision Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#image-captioning">Image captioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#pre-built-models-for-nlp-tasks">Pre-Built Models for NLP Tasks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#speech">Speech</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#speech-utilities">Speech Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="../api.html#splitting-utilities">Splitting Utilities</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../index.html">DLPy</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          

 



<div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../api.html">API Reference</a> &raquo;</li>
      
    <li>dlpy.model.Optimizer</li>
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../_sources/generated/dlpy.model.Optimizer.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="dlpy-model-optimizer">
<h1>dlpy.model.Optimizer<a class="headerlink" href="#dlpy-model-optimizer" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="dlpy.model.Optimizer">
<em class="property">class </em><code class="descclassname">dlpy.model.</code><code class="descname">Optimizer</code><span class="sig-paren">(</span><em>algorithm=&lt;dlpy.model.VanillaSolver object&gt;</em>, <em>mini_batch_size=1</em>, <em>seed=0</em>, <em>max_epochs=1</em>, <em>reg_l1=0</em>, <em>reg_l2=0</em>, <em>dropout=0</em>, <em>dropout_input=0</em>, <em>dropout_type='standard'</em>, <em>stagnation=0</em>, <em>threshold=1e-08</em>, <em>f_conv=0</em>, <em>snapshot_freq=0</em>, <em>log_level=0</em>, <em>bn_src_layer_warnings=True</em>, <em>freeze_layers_to=None</em>, <em>flush_weights=False</em>, <em>total_mini_batch_size=None</em>, <em>mini_batch_buf_size=None</em>, <em>freeze_layers=None</em>, <em>freeze_batch_norm_stats=None</em><span class="sig-paren">)</span><a class="headerlink" href="#dlpy.model.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <strong>dlpy.utils.DLPyDict</strong></p>
<p>Optimizer object</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><dl class="first docutils">
<dt><strong>algorithm</strong> <span class="classifier-delimiter">:</span> <span class="classifier">Algorithm</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the deep learning algorithm.</p>
</dd>
<dt><strong>mini_batch_size</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the number of observations per thread in a mini-batch.
You can use this parameter to control the number of observations
that the action uses on each worker for each thread to compute
the gradient prior to updating the weights. Larger values use more
memory. When synchronous SGD is used (the default), the total
mini-batch size is equal to miniBatchSize * number of threads *
number of workers. When asynchronous SGD is used (by specifying
the elasticSyncFreq parameter), each worker trains its own local
model. In this case, the total mini-batch size for each worker is
miniBatchSize * number of threads.</p>
</dd>
<dt><strong>seed</strong> <span class="classifier-delimiter">:</span> <span class="classifier">double</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the random number seed for the random number generator
in SGD. The default value, 0, and negative values indicate to use
random number streams based on the computer clock. Specify a value
that is greater than 0 for a reproducible random number sequence.</p>
</dd>
<dt><strong>max_epochs</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the maximum number of epochs. For SGD with a single-machine
server or a session that uses one worker on a distributed server,
one epoch is reached when the action passes through the data one time.
For a session that uses more than one worker, one epoch is reached
when all the workers exchange the weights with the controller one time.
The syncFreq parameter specifies the number of times each worker
passes through the data before exchanging weights with the controller.
For L-BFGS with full batch, each L-BFGS iteration might process more
than one epoch, and final number of epochs might exceed the maximum
number of epochs.</p>
</dd>
<dt><strong>reg_l1</strong> <span class="classifier-delimiter">:</span> <span class="classifier">double</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the weight for the L1 regularization term. By default,
L1 regularization is not performed and a value of 0 also disables the
regularization. Begin with small values such as 1e-6. L1 regularization
can be combined with L2 regularization.</p>
</dd>
<dt><strong>reg_l2</strong> <span class="classifier-delimiter">:</span> <span class="classifier">double</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the weight for the L2 regularization term. By default,
L2 regularization is not performed and a value of 0 also disables the
regularization. Begin with small
values such as 1e-3. L1 regularization can be combined with
L2 regularization.</p>
</dd>
<dt><strong>dropout</strong> <span class="classifier-delimiter">:</span> <span class="classifier">double</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the probability that the output of a neuron in a fully
connected layer will be set to zero during training. The specified
probability is recalculated each time an observation is processed.</p>
</dd>
<dt><strong>dropout_input</strong> <span class="classifier-delimiter">:</span> <span class="classifier">double</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the probability that an input variable will be set to zero
during training. The specified probability is recalculated each time
an observation is processed.</p>
</dd>
<dt><strong>dropout_type</strong> <span class="classifier-delimiter">:</span> <span class="classifier">string</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies what type of dropout to use.
<br/><strong class="param-meta">Valid Values:</strong> STANDARD, INVERTED
<br/><strong class="param-meta">Default:</strong> STANDARD</p>
</dd>
<dt><strong>stagnation</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the number of successive iterations without improvement
before stopping the optimization early. When the validTable parameter
is not specified, the loss error is monitored for stagnation. When
the validTable parameter is specified, the validation scores are
monitored for stagnation.</p>
</dd>
<dt><strong>threshold</strong> <span class="classifier-delimiter">:</span> <span class="classifier">double</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the threshold that is used to determine whether the loss
error or validation score is improving or is stagnating. When
abs(current_score - previous_score) &lt;= abs(current_score)*threshold,
the current iteration does not improve the optimization and the
stagnation counter is incremented. Otherwise, the stagnation counter
is set to zero.</p>
</dd>
<dt><strong>f_conv</strong> <span class="classifier-delimiter">:</span> <span class="classifier">double</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the relative function convergence criterion. If the relative
loss error abs(previous_loss - current_loss) / abs(previous_loss) does
not result in a change in the objective function, then the optimization
is stopped. By default, the relative function convergence is not checked.</p>
</dd>
<dt><strong>snapshot_freq</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies the frequency for generating snapshots of the neural weights
and storing the weights in a weight table during the training process.
When asynchronous SGD is used, the action synchronizes all the weights
before writing out the weights.</p>
</dd>
<dt><strong>log_level</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies how progress messages are sent to the client. The default
value, 0, indicates that no messages are sent. Specify 1 to receive
start and end messages. Specify 2 to include the iteration history.</p>
</dd>
<dt><strong>bn_src_layer_warnings</strong> <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Turns warning on or off, if batch normalization source layer has
an atypical type, activation, or include_bias setting. <br/><strong class="param-meta">Default:</strong> False</p>
</dd>
<dt><strong>total_mini_batch_size</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">specifies the number of observations in a mini-batch. You can use
this parameter to control the number of observations that the action
uses to compute the gradient prior to updating the weights. Larger
values use more memory. If the specified size cannot be evenly divided
by the number of threads (if using asynchronous SGD), or the number of
threads * number of workers (if using synchronous SGD), then the action
will terminate with an error unless the round parameter was specified
to be TRUE, in which case, the total mini-batch size will be rounded
up so that it will be evenly divided.</p>
</dd>
<dt><strong>flush_weights</strong> <span class="classifier-delimiter">:</span> <span class="classifier">bool</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">Specifies whether flush the weight table to the disk.
<br/><strong class="param-meta">Default:</strong> False</p>
</dd>
<dt><strong>mini_batch_buf_size</strong> <span class="classifier-delimiter">:</span> <span class="classifier">int</span><span class="opt-sep">, </span><span class="optional">optional</span></dt>
<dd><p class="first last">specifies the size of a buffer that is used to save input data and
intermediate calculations. By default, each layer allocates an input
buffer that is equal to the number of input channels multiplied by
the input feature map size multiplied by the bufferSize value. You
can reduce memory usage by specifying a value that is smaller than
the bufferSize. The only disadvantage to specifying a small value is
that run time can increase because multiple smaller matrices must be
multiplied instead of a single large matrix multiply.</p>
</dd>
<dt><strong>freeze_layers_to</strong> <span class="classifier-delimiter">:</span> <span class="classifier">string</span></dt>
<dd><p class="first last">Specifies a layer name to freeze this layer and all the layers before
this layer.</p>
</dd>
<dt><strong>freeze_batch_norm_stats</strong> <span class="classifier-delimiter">:</span> <span class="classifier">Boolean</span></dt>
<dd><p class="first last">When set to True, freezes the statistics of all batch normalization layers.</p>
</dd>
<dt><strong>freeze_layers</strong> <span class="classifier-delimiter">:</span> <span class="classifier">list of string</span></dt>
<dd><p class="first last">Specifies a list of layer names whose trainable parameters will be frozen.</p>
</dd>
</dl>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><dl class="first last docutils">
<dt><strong><a href="dlpy.model.Optimizer.html">Optimizer</a></strong></dt>
<dd></dd>
</dl>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="dlpy.model.Optimizer.__init__">
<code class="descname">__init__</code><span class="sig-paren">(</span><em>algorithm=&lt;dlpy.model.VanillaSolver object&gt;</em>, <em>mini_batch_size=1</em>, <em>seed=0</em>, <em>max_epochs=1</em>, <em>reg_l1=0</em>, <em>reg_l2=0</em>, <em>dropout=0</em>, <em>dropout_input=0</em>, <em>dropout_type='standard'</em>, <em>stagnation=0</em>, <em>threshold=1e-08</em>, <em>f_conv=0</em>, <em>snapshot_freq=0</em>, <em>log_level=0</em>, <em>bn_src_layer_warnings=True</em>, <em>freeze_layers_to=None</em>, <em>flush_weights=False</em>, <em>total_mini_batch_size=None</em>, <em>mini_batch_buf_size=None</em>, <em>freeze_layers=None</em>, <em>freeze_batch_norm_stats=None</em><span class="sig-paren">)</span><a class="headerlink" href="#dlpy.model.Optimizer.__init__" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize self.  See help(type(self)) for accurate signature.</p>
</dd></dl>

<p class="rubric">Methods</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%" />
<col width="90%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#dlpy.model.Optimizer.__init__" title="dlpy.model.Optimizer.__init__"><strong>__init__</strong></a>([algorithm,&nbsp;mini_batch_size,&nbsp;seed,&nbsp;…])</td>
<td>Initialize self.</td>
</tr>
<tr class="row-even"><td><strong>add_optimizer_mode</strong>([solver_mode_type,&nbsp;…])</td>
<td>Sets the mode of the solver.</td>
</tr>
<tr class="row-odd"><td><strong>clear</strong>()</td>
<td></td>
</tr>
<tr class="row-even"><td><strong>get</strong>(k[,d])</td>
<td></td>
</tr>
<tr class="row-odd"><td><strong>items</strong>()</td>
<td></td>
</tr>
<tr class="row-even"><td><strong>keys</strong>()</td>
<td></td>
</tr>
<tr class="row-odd"><td><strong>pop</strong>(k[,d])</td>
<td>If key is not found, d is returned if given, otherwise KeyError is raised.</td>
</tr>
<tr class="row-even"><td><strong>popitem</strong>()</td>
<td>as a 2-tuple; but raise KeyError if D is empty.</td>
</tr>
<tr class="row-odd"><td><strong>setdefault</strong>(k[,d])</td>
<td></td>
</tr>
<tr class="row-even"><td><strong>update</strong>([E,&nbsp;]**F)</td>
<td>If E present and has a .keys() method, does:     for k in E: D[k] = E[k] If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v In either case, this is followed by: for k, v in F.items(): D[k] = v</td>
</tr>
<tr class="row-odd"><td><strong>values</strong>()</td>
<td></td>
</tr>
</tbody>
</table>
</dd></dl>

</div>


           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dlpy.lr_scheduler.FCMPLR.html" class="btn btn-neutral float-right" title="dlpy.lr_scheduler.FCMPLR" accesskey="n">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="dlpy.model.VanillaSolver.html" class="btn btn-neutral" title="dlpy.model.VanillaSolver" accesskey="p"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018 SAS Institute Inc. All Rights Reserved..

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'1.2.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>